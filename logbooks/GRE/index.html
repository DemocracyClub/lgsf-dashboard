<!doctype html>
<html>
  <head>
    <title>LGSF Dashboard</title>
    <link rel="stylesheet"  href="/lgsf-dashboard/assets/css/styles.css">
    <link rel="shortcut icon" href="https://democracyclub.org.uk/static/images/logo_icon.png">
  </head>
  <body>
    <div class="ds-page">
      <a class="ds-skip-link" href="#main">skip to content</a>

      <header class="ds-header">
        <a class="ds-skip-link" href="#main">skip to content</a>
        <a class="ds-logo" href="/lgsf-dashboard/">
          <img src="https://DemocracyClub.github.io/design-system/images/logo_icon.svg" alt="" />
          <span>LGSF Logbooks</span>
        </a>

      </header>


      <main id="main" tabindex="-1" class="ds-stack">

        
<style>

    pre {
        height:400px;
        overflow-x: scroll;
        width:100%
    }
</style>




  


  <h2 id="2024-04-27-16-54">2024-04-27</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>151 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-27 16:54:44.068063</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-27 16:57:15.573463</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>0</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd></dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[16:54:44] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:257
           Getting all files in Councillors...                       base.py:209
[16:54:45] ...found 1 files in Councillors                           base.py:225
           Deleting batch no. 1 consisting of 1 files                base.py:236
           ...data deleted.                                          base.py:264
           Scraping from                                              base.py:49
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
[16:54:47] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/336/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:54:50] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/354/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:54:53] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/337/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:54:56] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/75/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:55:00] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/345/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:02] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/348/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:04] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/314/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:06] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/76/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:55:08] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/315/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:11] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/355/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:13] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/96/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:55:15] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/102/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:20] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/346/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:22] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/334/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:24] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/107/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:26] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/110/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:29] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/79/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:55:33] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/81/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:55:36] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/349/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:37] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/86/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:55:40] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/92/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:55:42] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/105/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:46] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/341/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:48] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/116/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:50] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/123/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:52] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/338/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:55] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/122/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:55:59] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/125/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:03] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/121/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:05] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/120/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:07] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/316/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:11] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/112/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:16] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/342/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:18] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/109/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:21] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/357/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:24] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/103/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:26] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/340/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:28] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/356/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:30] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/350/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:32] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/352/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:34] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/351/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:37] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/344/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:40] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/99/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:56:44] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/317/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:47] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/97/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:56:49] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/95/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:56:52] Committing batch 1 consisting of 92 files                 base.py:297
[16:56:53] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/347/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:55] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/333/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:56:57] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/332/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:57:00] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/353/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:57:02] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/82/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:57:04] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/339/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:57:06] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/80/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:57:10] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/77/ScreenMode/Alphabetic           
           al/Default.aspx                                                      
[16:57:12] Scraping from                                              base.py:49
           http://committees.royalgreenwich.gov.uk/Councillors/tabid/           
           63/ctl/ViewCMIS_Person/mid/383/id/343/ScreenMode/Alphabeti           
           cal/Default.aspx                                                     
[16:57:14] Committing batch 2 consisting of 18 files                 base.py:297
[16:57:15] Finished attempting to scrape: GRE                        base.py:345
</pre>
  

  


  <h2 id="2024-04-27-09-06">2024-04-27</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-27 09:06:11.448876</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-27 09:06:13.467626</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/httpx/_transports/default.py", line 69, in map_httpcore_exceptions
    yield
  File "/opt/python/httpx/_transports/default.py", line 233, in handle_request
    resp = self._pool.handle_request(req)
  File "/opt/python/httpcore/_sync/connection_pool.py", line 216, in handle_request
    raise exc from None
  File "/opt/python/httpcore/_sync/connection_pool.py", line 196, in handle_request
    response = connection.handle_request(
  File "/opt/python/httpcore/_sync/connection.py", line 99, in handle_request
    raise exc
  File "/opt/python/httpcore/_sync/connection.py", line 76, in handle_request
    stream = self._connect(request)
  File "/opt/python/httpcore/_sync/connection.py", line 154, in _connect
    stream = stream.start_tls(**kwargs)
  File "/opt/python/httpcore/_backends/sync.py", line 152, in start_tls
    with map_exceptions(exc_map):
  File "/var/lang/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/opt/python/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 55, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 272, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 55, in get
    response = self.http_client.get(url, headers=headers, timeout=30)
  File "/opt/python/httpx/_client.py", line 1054, in get
    return self.request(
  File "/opt/python/httpx/_client.py", line 827, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
  File "/opt/python/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
  File "/opt/python/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
  File "/opt/python/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "/opt/python/httpx/_client.py", line 1015, in _send_single_request
    response = transport.handle_request(request)
  File "/opt/python/httpx/_transports/default.py", line 232, in handle_request
    with map_httpcore_exceptions():
  File "/var/lang/lib/python3.10/contextlib.py", line 153, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/opt/python/httpx/_transports/default.py", line 86, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:06:11] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:257
[09:06:12] Getting all files in Councillors...                       base.py:209
           ...found 1 files in Councillors                           base.py:225
           Deleting batch no. 1 consisting of 1 files                base.py:236
[09:06:13] ...data deleted.                                          base.py:264
           Scraping from                                              base.py:49
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)                                                        
           Finished attempting to scrape: GRE                        base.py:345
</pre>
  

  


  <h2 id="2024-04-26-16-26">2024-04-26</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-26 16:26:58.217066</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-26 16:27:00.405960</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 55, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 274, in get_councillors
    req = self.get(
TypeError: ScraperBase.get() got an unexpected keyword argument 'verify'
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[16:26:58] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:257
[16:26:59] Getting all files in Councillors...                       base.py:209
           ...found 1 files in Councillors                           base.py:225
           Deleting batch no. 1 consisting of 1 files                base.py:236
[16:27:00] ...data deleted.                                          base.py:264
           ScraperBase.get() got an unexpected keyword argument   handlers.py:36
           'verify'                                                             
           Finished attempting to scrape: GRE                        base.py:345
</pre>
  

  


  <h2 id="2024-04-26-10-51">2024-04-26</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-26 10:51:17.288990</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-26 10:51:19.359298</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 55, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 274, in get_councillors
    req = self.get(
TypeError: ScraperBase.get() got an unexpected keyword argument 'verify'
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[10:51:17] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:257
[10:51:18] Getting all files in Councillors...                       base.py:209
           ...found 1 files in Councillors                           base.py:225
           Deleting batch no. 1 consisting of 1 files                base.py:236
[10:51:19] ...data deleted.                                          base.py:264
           ScraperBase.get() got an unexpected keyword argument   handlers.py:36
           'verify'                                                             
           Finished attempting to scrape: GRE                        base.py:345
</pre>
  

  


  <h2 id="2024-04-26-09-10">2024-04-26</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-26 09:10:53.542865</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-26 09:10:55.548371</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:10:53] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[09:10:54] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[09:10:55] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-25-09-41">2024-04-25</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>1 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-25 09:41:46.464860</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-25 09:41:48.284862</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:41:46] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[09:41:47] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
           ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
[09:41:48] HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-24-09-06">2024-04-24</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-24 09:06:32.854436</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-24 09:06:35.051618</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:06:32] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
[09:06:33] Deleting existing data...                                 base.py:251
           Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[09:06:34] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
[09:06:35] Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-23-08-40">2024-04-23</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-23 08:40:00.328985</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-23 08:40:02.331261</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[08:40:00] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[08:40:01] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
           ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
[08:40:02] HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-22-08-33">2024-04-22</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-22 08:33:37.269551</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-22 08:33:39.309355</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[08:33:37] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[08:33:38] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
           ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
[08:33:39] HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-21-10-00">2024-04-21</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-21 10:00:34.059532</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-21 10:00:36.228049</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[10:00:34] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[10:00:35] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
           ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
[10:00:36] HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-20-10-04">2024-04-20</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-20 10:04:51.582790</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-20 10:04:53.827799</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[10:04:51] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[10:04:52] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[10:04:53] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-19-09-31">2024-04-19</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-19 09:31:14.783336</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-19 09:31:16.828913</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:31:14] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
[09:31:15] Deleting existing data...                                 base.py:251
           Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[09:31:16] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-18-09-14">2024-04-18</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-18 09:14:13.830732</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-18 09:14:16.041398</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:14:13] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
[09:14:14] Deleting existing data...                                 base.py:251
           Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[09:14:15] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
[09:14:16] Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-17-09-52">2024-04-17</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-17 09:52:53.821186</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-17 09:52:55.868923</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:52:53] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
[09:52:54] Deleting existing data...                                 base.py:251
           Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[09:52:55] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-16-10-09">2024-04-16</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-16 10:09:31.389107</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-16 10:09:33.465040</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[10:09:31] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[10:09:32] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[10:09:33] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-15-10-27">2024-04-15</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-15 10:27:47.941832</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-15 10:27:50.140699</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[10:27:47] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
[10:27:48] Deleting existing data...                                 base.py:251
           Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[10:27:49] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
[10:27:50] Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-14-10-24">2024-04-14</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-14 10:24:14.423102</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-14 10:24:16.670741</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[10:24:14] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[10:24:15] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[10:24:16] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-13-10-29">2024-04-13</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>2 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-13 10:29:07.176720</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-13 10:29:09.283409</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[10:29:07] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[10:29:08] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
           ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
[10:29:09] HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-12-08-57">2024-04-12</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>1 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-12 08:57:20.808018</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-12 08:57:22.780197</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[08:57:20] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
[08:57:21] Deleting existing data...                                 base.py:251
           Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[08:57:22] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  

  


  <h2 id="2024-04-11-09-41">2024-04-11</h2>
  <div class="ds-card">
  <div class="ds-card-body">
    <dl class="ds-descriptions">
      <div>
        <dt>Duration</dt>
        <dd>3 seconds</dd>
      </div>
      <div>
        <dt>Start</dt>
        <dd>2024-04-11 09:41:46.339560</dd>
      </div>
      <div>
        <dt>End</dt>
        <dd>2024-04-11 09:41:49.949635</dd>
      </div>
      <div>
        <dt>Status code</dt>
        <dd>1</dd>
      </div>
      <div>
        <dt>Error</dt>
        <dd>Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 468, in _make_request
    self._validate_conn(conn)
  File "/opt/python/urllib3/connectionpool.py", line 1097, in _validate_conn
    conn.connect()
  File "/opt/python/urllib3/connection.py", line 642, in connect
    sock_and_verified = _ssl_wrap_socket_and_match_hostname(
  File "/opt/python/urllib3/connection.py", line 783, in _ssl_wrap_socket_and_match_hostname
    ssl_sock = ssl_wrap_socket(
  File "/opt/python/urllib3/util/ssl_.py", line 471, in ssl_wrap_socket
    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)
  File "/opt/python/urllib3/util/ssl_.py", line 515, in _ssl_wrap_socket_impl
    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)
  File "/var/lang/lib/python3.10/ssl.py", line 513, in wrap_socket
    return self.sslsocket_class._create(
  File "/var/lang/lib/python3.10/ssl.py", line 1104, in _create
    self.do_handshake()
  File "/var/lang/lib/python3.10/ssl.py", line 1375, in do_handshake
    self._sslobj.do_handshake()
ssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/opt/python/urllib3/connectionpool.py", line 791, in urlopen
    response = self._make_request(
  File "/opt/python/urllib3/connectionpool.py", line 492, in _make_request
    raise new_e
urllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/opt/python/requests/adapters.py", line 486, in send
    resp = conn.urlopen(
  File "/opt/python/urllib3/connectionpool.py", line 845, in urlopen
    retries = retries.increment(
  File "/opt/python/urllib3/util/retry.py", line 515, in increment
    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]
urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/var/task/lgsf/aws_lambda/handlers.py", line 32, in scraper_worker_handler
    scraper.run(run_log)
  File "/var/task/lgsf/councillors/scrapers.py", line 56, in run
    for councillor_html in self.get_councillors():
  File "/var/task/lgsf/councillors/scrapers.py", line 277, in get_councillors
    req = self.get(
  File "/var/task/lgsf/scrapers/base.py", line 47, in get
    response = self.requests_session.get(
  File "/opt/python/requests/sessions.py", line 602, in get
    return self.request("GET", url, **kwargs)
  File "/opt/python/requests/sessions.py", line 589, in request
    resp = self.send(prep, **send_kwargs)
  File "/opt/python/requests/sessions.py", line 703, in send
    r = adapter.send(request, **kwargs)
  File "/opt/python/requests/adapters.py", line 517, in send
    raise SSLError(e, request=request)
requests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))
</dd>
      </div>
    </dl>
  </div>
  </div>

  <h3>Run log</h3>
  <pre>[09:41:46] Fetching Scraper for: GRE                              handlers.py:23
           Begin attempting to scrape: GRE                        handlers.py:27
           Deleting existing data...                                 base.py:251
[09:41:48] Getting all files in Councillors...                       base.py:203
           ...found 1 files in Councillors                           base.py:219
           Deleting batch no. 1 consisting of 1 files                base.py:230
[09:41:49] ...data deleted.                                          base.py:258
           Scraping from                                              base.py:41
           https://committees.royalgreenwich.gov.uk/Councillors/tabid           
           /63/ScreenMode/Alphabetical/Default.aspx                             
           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36
           v.uk', port=443): Max retries exceeded with url:                     
           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               
           aspx (Caused by SSLError(SSLCertVerificationError(1,                 
           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 
           failed: unable to get local issuer certificate                       
           (_ssl.c:1007)')))                                                    
           Finished attempting to scrape: GRE                        base.py:339
</pre>
  


      </main>
      <footer class="ds-footer">
        <div class="ds-block-centered ds-text-centered ds-stack">
          <div class="ds-cluster-center">
            <ul>
              <li><a href="/lgsf-dashboard/api/failing.json">Failing JSON</a></li>
            </ul>
          </div>
          <div class="ds-copyright">
            <a href="https://democracyclub.org.uk/">
              <img src="https://DemocracyClub.github.io/design-system/images/logo_white_text.svg" alt="Democracy Club Home" />
            </a>
            <p>Copyright © 2021 Democracy Club</p>
            <p>Community Interest Company</p>
            <p>Company No: <a href="https://beta.companieshouse.gov.uk/company/09461226">09461226</a></p>
          </div>
        </div>
      </footer>
    </div>
  </body>
