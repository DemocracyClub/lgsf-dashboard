[{"council_id":"ABC","missing":false,"latest_run":{"status_code":1,"log_text":"[16:32:50] Fetching Scraper for: ABC                              handlers.py:23\n           Begin attempting to scrape: ABC                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:32:51] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:32:52] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: ABC                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:32:50.231714","end":"2024-04-26 16:32:52.433225","duration":2}},{"council_id":"AGB","missing":false,"latest_run":{"status_code":1,"log_text":"[16:16:41] Fetching Scraper for: AGB                              handlers.py:23\n           Begin attempting to scrape: AGB                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[16:16:42] ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:16:43] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: AGB                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in get_councillors\n    soup = self.get_page(url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:16:41.061121","end":"2024-04-26 16:16:43.194933","duration":2}},{"council_id":"AND","missing":false,"latest_run":{"status_code":1,"log_text":"[15:47:19] Fetching Scraper for: AND                              handlers.py:23\n           Begin attempting to scrape: AND                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:47:20] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:47:21] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: AND                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:47:19.321367","end":"2024-04-26 15:47:21.627736","duration":2}},{"council_id":"ANN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:55:00] Fetching Scraper for: ANN                              handlers.py:23\n           Begin attempting to scrape: ANN                        handlers.py:27\n[15:55:01] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:55:02] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: ANN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:55:00.756797","end":"2024-04-26 15:55:02.941195","duration":2}},{"council_id":"ANS","missing":false,"latest_run":{"status_code":1,"log_text":"[16:02:12] Fetching Scraper for: ANS                              handlers.py:23\n           Begin attempting to scrape: ANS                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:02:13] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:02:14] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: ANS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:02:12.142731","end":"2024-04-26 16:02:14.250006","duration":2}},{"council_id":"BIR","missing":false,"latest_run":{"status_code":1,"log_text":"[16:21:41] Fetching Scraper for: BIR                              handlers.py:23\n           Begin attempting to scrape: BIR                        handlers.py:27\n[16:21:42] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:21:43] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: BIR                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:21:41.755155","end":"2024-04-26 16:21:43.694897","duration":1}},{"council_id":"BOT","missing":false,"latest_run":{"status_code":1,"log_text":"[16:04:28] Fetching Scraper for: BOT                              handlers.py:23\n           Begin attempting to scrape: BOT                        handlers.py:27\n[16:04:29] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[16:04:30] ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://moderngov.boston.gov.uk/mgWebService.asmx/GetCounci           \n           llorsByWard                                                          \n[16:04:31] [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1007)                                                        \n           Finished attempting to scrape: BOT                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n","start":"2024-04-26 16:04:28.960051","end":"2024-04-26 16:04:31.242959","duration":2}},{"council_id":"BRA","missing":false,"latest_run":{"status_code":1,"log_text":"[16:23:33] Fetching Scraper for: BRA                              handlers.py:23\n           Begin attempting to scrape: BRA                        handlers.py:27\n[16:23:34] Deleting existing data...                                 base.py:257\n[16:23:36] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:23:37] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: BRA                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:23:33.963948","end":"2024-04-26 16:23:37.493339","duration":3}},{"council_id":"CAM","missing":false,"latest_run":{"status_code":1,"log_text":"[15:49:03] Fetching Scraper for: CAM                              handlers.py:23\n           Begin attempting to scrape: CAM                        handlers.py:27\n[15:49:04] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:49:05] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: CAM                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:49:03.636491","end":"2024-04-26 15:49:05.647061","duration":2}},{"council_id":"CAN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:35:21] Fetching Scraper for: CAN                              handlers.py:23\n           Begin attempting to scrape: CAN                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:35:22] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:35:23] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: CAN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:35:21.506327","end":"2024-04-26 15:35:23.487444","duration":1}},{"council_id":"CAS","missing":false,"latest_run":{"status_code":1,"log_text":"[16:06:40] Fetching Scraper for: CAS                              handlers.py:23\n           Begin attempting to scrape: CAS                        handlers.py:27\n[16:06:41] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:06:42] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: CAS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:06:40.857751","end":"2024-04-26 16:06:42.922520","duration":2}},{"council_id":"CAT","missing":false,"latest_run":{"status_code":1,"log_text":"[16:33:45] Fetching Scraper for: CAT                              handlers.py:23\n           Begin attempting to scrape: CAT                        handlers.py:27\n[16:33:46] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:33:47] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://democracy.canterbury.gov.uk/mgWebService.asmx/GetCo           \n           uncillorsByWard                                                      \n[16:33:52] timed out                                              handlers.py:36\n           Finished attempting to scrape: CAT                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: timed out\n","start":"2024-04-26 16:33:45.818018","end":"2024-04-26 16:33:52.873069","duration":7}},{"council_id":"CAY","missing":false,"latest_run":{"status_code":1,"log_text":"[16:03:26] Fetching Scraper for: CAY                              handlers.py:23\n           Begin attempting to scrape: CAY                        handlers.py:27\n[16:03:27] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 69 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n[16:03:28] ...found 69 files in Councillors/raw                      base.py:225\n           ...found 139 files in Councillors                         base.py:225\n           Deleting batch no. 1 consisting of 100 files              base.py:236\n[16:03:29] Deleting batch no. 2 consisting of 39 files               base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://democracy.caerphilly.gov.uk/mgWebService.asmx/GetC           \n           ouncillorsByWard                                                     \n[16:03:35] The read operation timed out                           handlers.py:36\n           Finished attempting to scrape: CAY                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2024-04-26 16:03:26.572526","end":"2024-04-26 16:03:35.905354","duration":9}},{"council_id":"CLK","missing":false,"latest_run":{"status_code":1,"log_text":"[15:35:38] Fetching Scraper for: CLK                              handlers.py:23\n           Begin attempting to scrape: CLK                        handlers.py:27\n[15:35:39] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:35:40] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: CLK                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:35:38.569316","end":"2024-04-26 15:35:40.482072","duration":1}},{"council_id":"COL","missing":false,"latest_run":{"status_code":1,"log_text":"[16:04:34] Fetching Scraper for: COL                              handlers.py:23\n           Begin attempting to scrape: COL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:04:35] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:04:36] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: COL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:04:34.206707","end":"2024-04-26 16:04:36.396550","duration":2}},{"council_id":"DER","missing":false,"latest_run":{"status_code":1,"log_text":"[15:32:04] Fetching Scraper for: DER                              handlers.py:23\n           Begin attempting to scrape: DER                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[15:32:05] ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n[15:32:06] Finished attempting to scrape: DER                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:32:04.030048","end":"2024-04-26 15:32:06.156184","duration":2}},{"council_id":"DRS","missing":false,"latest_run":{"status_code":1,"log_text":"[15:43:04] Fetching Scraper for: DRS                              handlers.py:23\n           Begin attempting to scrape: DRS                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:43:05] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:43:06] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://meetings.derrycityandstrabanedistrict.com/mgWebServ           \n           ice.asmx/GetCouncillorsByWard                                        \n[15:43:11] timed out                                              handlers.py:36\n           Finished attempting to scrape: DRS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: timed out\n","start":"2024-04-26 15:43:04.289749","end":"2024-04-26 15:43:11.210080","duration":6}},{"council_id":"DUD","missing":false,"latest_run":{"status_code":1,"log_text":"[16:13:28] Fetching Scraper for: DUD                              handlers.py:23\n           Begin attempting to scrape: DUD                        handlers.py:27\n[16:13:29] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:13:30] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: DUD                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:13:28.678139","end":"2024-04-26 16:13:30.840438","duration":2}},{"council_id":"EAY","missing":false,"latest_run":{"status_code":1,"log_text":"[16:07:40] Fetching Scraper for: EAY                              handlers.py:23\n           Begin attempting to scrape: EAY                        handlers.py:27\n[16:07:41] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:07:42] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: EAY                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:07:40.675756","end":"2024-04-26 16:07:42.826984","duration":2}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[09:29:18] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n[09:29:19] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:29:20] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://cne-siar.gov.uk/home/your-council/council-members/           \n[09:29:25] Scraping from https://cne-siar.gov.uk/kenneth-j-maclean/   base.py:49\n[09:29:27] 'NoneType' object has no attribute 'get_text'          handlers.py:36\n[09:29:28] Finished attempting to scrape: ELS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ELS-na-h-eileanan-an-iar/councillors.py\", line 25, in get_single_councillor\n    .get_text(strip=True)\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2024-04-27 09:29:18.623410","end":"2024-04-27 09:29:28.022888","duration":9}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[15:56:09] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:56:10] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:56:11] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: ERW                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:56:09.152248","end":"2024-04-26 15:56:11.326456","duration":2}},{"council_id":"ESS","missing":false,"latest_run":{"status_code":1,"log_text":"[08:37:17] Fetching Scraper for: ESS                              handlers.py:23\n           Begin attempting to scrape: ESS                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[08:37:18] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/EssexCmis5/Councillors.aspx                 \n[08:37:21] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/655/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:24] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/327/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:26] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/100/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:29] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/639/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:32] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/882/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:37] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/880/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:40] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/879/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:46] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/648/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:51] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/80/ScreenMode/Ward/Default.a           \n           spx                                                                  \n[08:37:54] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/649/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:56] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/866/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:37:59] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/458/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:03] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/864/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:08] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/887/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:12] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/334/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:14] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/650/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:18] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/364/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:21] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/863/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:24] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/309/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:27] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/91/ScreenMode/Ward/Default.a           \n           spx                                                                  \n[08:38:29] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/877/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:32] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/777/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:34] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/865/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:37] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/359/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:39] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/870/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:41] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/340/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:44] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/94/ScreenMode/Ward/Default.a           \n           spx                                                                  \n[08:38:52] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/883/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:54] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/656/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:38:58] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/872/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:39:01] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/117/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:39:04] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/889/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:39:06] Scraping from                                              base.py:49\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/610/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:39:09] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 66 files                 base.py:297\n[08:39:11] Finished attempting to scrape: ESS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 300, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 281, in get_party_name\n    list_page_html.find_all(\"img\")[-1][\"title\"]\n  File \"/opt/python/bs4/element.py\", line 1930, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2024-04-27 08:37:17.297730","end":"2024-04-27 08:39:11.186055","duration":113}},{"council_id":"FAL","missing":false,"latest_run":{"status_code":1,"log_text":"[15:32:46] Fetching Scraper for: FAL                              handlers.py:23\n           Begin attempting to scrape: FAL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:32:47] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:32:48] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: FAL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:32:46.458873","end":"2024-04-26 15:32:48.300753","duration":1}},{"council_id":"GLG","missing":false,"latest_run":{"status_code":1,"log_text":"[16:01:15] Fetching Scraper for: GLG                              handlers.py:23\n           Begin attempting to scrape: GLG                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:01:16] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:01:17] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: GLG                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:01:15.133018","end":"2024-04-26 16:01:17.477368","duration":2}},{"council_id":"GRE","missing":false,"latest_run":{"status_code":1,"log_text":"[16:26:58] Fetching Scraper for: GRE                              handlers.py:23\n           Begin attempting to scrape: GRE                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:26:59] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:27:00] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: GRE                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:26:58.217066","end":"2024-04-26 16:27:00.405960","duration":2}},{"council_id":"GRT","missing":false,"latest_run":{"status_code":1,"log_text":"[15:28:51] Fetching Scraper for: GRT                              handlers.py:23\n           Begin attempting to scrape: GRT                        handlers.py:27\n[15:28:53] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:28:54] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www2.guildford.gov.uk/councilmeetings/mgWebService           \n           .asmx/GetCouncillorsByWard                                           \n           [Errno -2] Name or service not known                   handlers.py:36\n[15:28:55] Finished attempting to scrape: GRT                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno -2] Name or service not known\n","start":"2024-04-26 15:28:51.330179","end":"2024-04-26 15:28:55.185087","duration":3}},{"council_id":"HAO","missing":false,"latest_run":{"status_code":1,"log_text":"[16:05:26] Fetching Scraper for: HAO                              handlers.py:23\n           Begin attempting to scrape: HAO                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:05:27] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:05:28] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: HAO                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:05:26.478073","end":"2024-04-26 16:05:28.375187","duration":1}},{"council_id":"HIG","missing":false,"latest_run":{"status_code":1,"log_text":"[09:01:49] Fetching Scraper for: HIG                              handlers.py:23\n           Begin attempting to scrape: HIG                        handlers.py:27\n[09:01:50] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 100 files in Councillors/json                    base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n[09:01:51] ...found 100 files in Councillors/raw                     base.py:225\n           ...found 201 files in Councillors                         base.py:225\n           Deleting batch no. 1 consisting of 100 files              base.py:236\n           Deleting batch no. 2 consisting of 100 files              base.py:236\n[09:01:52] Deleting batch no. 3 consisting of 1 files                base.py:236\n[09:01:53] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://democracy.highpeak.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n[09:02:23] The read operation timed out                           handlers.py:36\n           Finished attempting to scrape: HIG                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2024-04-27 09:01:49.750483","end":"2024-04-27 09:02:23.713123","duration":33}},{"council_id":"HLD","missing":false,"latest_run":{"status_code":1,"log_text":"[15:30:59] Fetching Scraper for: HLD                              handlers.py:23\n           Begin attempting to scrape: HLD                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:31:00] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: HLD                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:30:59.026662","end":"2024-04-26 15:31:00.868978","duration":1}},{"council_id":"HPL","missing":false,"latest_run":{"status_code":1,"log_text":"[15:38:58] Fetching Scraper for: HPL                              handlers.py:23\n           Begin attempting to scrape: HPL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:38:59] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:39:00] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: HPL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:38:58.481690","end":"2024-04-26 15:39:00.671220","duration":2}},{"council_id":"IVC","missing":false,"latest_run":{"status_code":1,"log_text":"[16:21:35] Fetching Scraper for: IVC                              handlers.py:23\n           Begin attempting to scrape: IVC                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:21:36] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:21:37] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: IVC                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:21:35.296021","end":"2024-04-26 16:21:37.315833","duration":2}},{"council_id":"LEC","missing":false,"latest_run":{"status_code":1,"log_text":"[15:32:57] Fetching Scraper for: LEC                              handlers.py:23\n           Begin attempting to scrape: LEC                        handlers.py:27\n[15:32:58] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:32:59] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://politics.leics.gov.uk//mgWebService.asmx/GetCouncil           \n           lorsByWard                                                           \n           [Errno 111] Connection refused                         handlers.py:36\n           Finished attempting to scrape: LEC                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n","start":"2024-04-26 15:32:57.832229","end":"2024-04-26 15:32:59.936336","duration":2}},{"council_id":"LIV","missing":false,"latest_run":{"status_code":1,"log_text":"[15:59:24] Fetching Scraper for: LIV                              handlers.py:23\n           Begin attempting to scrape: LIV                        handlers.py:27\n[15:59:25] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 85 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 85 files in Councillors/raw                      base.py:225\n           ...found 171 files in Councillors                         base.py:225\n[15:59:26] Deleting batch no. 1 consisting of 100 files              base.py:236\n           Deleting batch no. 2 consisting of 71 files               base.py:236\n[15:59:27] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://councillors.liverpool.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n[15:59:32] The read operation timed out                           handlers.py:36\n           Finished attempting to scrape: LIV                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2024-04-26 15:59:24.563632","end":"2024-04-26 15:59:32.924977","duration":8}},{"council_id":"LUT","missing":false,"latest_run":{"status_code":1,"log_text":"[15:50:39] Fetching Scraper for: LUT                              handlers.py:23\n           Begin attempting to scrape: LUT                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:50:40] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:50:41] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: LUT                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:50:39.402605","end":"2024-04-26 15:50:41.569286","duration":2}},{"council_id":"MLN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:43:14] Fetching Scraper for: MLN                              handlers.py:23\n           Begin attempting to scrape: MLN                        handlers.py:27\n[15:43:15] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:43:16] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: MLN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:43:14.592737","end":"2024-04-26 15:43:16.688657","duration":2}},{"council_id":"MOL","missing":false,"latest_run":{"status_code":1,"log_text":"[09:58:30] Fetching Scraper for: MOL                              handlers.py:23\n[09:58:31] Begin attempting to scrape: MOL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:58:32] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.molevalley.gov.uk/home/council/councillors/who           \n           -are-your-councillors                                                \n[09:58:34] Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.molevalley.gov.uk/home/council/councillor               \n           s/who-are-your-councillors'                                          \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n           Finished attempting to scrape: MOL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 152, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.molevalley.gov.uk/home/council/councillors/who-are-your-councillors'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2024-04-27 09:58:30.972235","end":"2024-04-27 09:58:34.852109","duration":3}},{"council_id":"MRY","missing":false,"latest_run":{"status_code":1,"log_text":"[15:45:57] Fetching Scraper for: MRY                              handlers.py:23\n           Begin attempting to scrape: MRY                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[15:45:58] ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n[15:45:59] Finished attempting to scrape: MRY                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:45:57.083111","end":"2024-04-26 15:45:59.057452","duration":1}},{"council_id":"MUL","missing":false,"latest_run":{"status_code":1,"log_text":"[15:32:09] Fetching Scraper for: MUL                              handlers.py:23\n           Begin attempting to scrape: MUL                        handlers.py:27\n[15:32:10] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:32:11] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: MUL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:32:09.666642","end":"2024-04-26 15:32:11.861246","duration":2}},{"council_id":"NAY","missing":false,"latest_run":{"status_code":1,"log_text":"[15:35:06] Fetching Scraper for: NAY                              handlers.py:23\n           Begin attempting to scrape: NAY                        handlers.py:27\n[15:35:07] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:35:08] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: NAY                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:35:06.585090","end":"2024-04-26 15:35:08.688415","duration":2}},{"council_id":"NUN","missing":false,"latest_run":{"status_code":1,"log_text":"[16:16:36] Fetching Scraper for: NUN                              handlers.py:23\n           Begin attempting to scrape: NUN                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:16:37] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:16:38] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: NUN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:16:36.270865","end":"2024-04-26 16:16:38.331638","duration":2}},{"council_id":"ORK","missing":false,"latest_run":{"status_code":1,"log_text":"[16:05:02] Fetching Scraper for: ORK                              handlers.py:23\n           Begin attempting to scrape: ORK                        handlers.py:27\n[16:05:03] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n[16:05:04] Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n[16:05:05] Finished attempting to scrape: ORK                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:05:02.877973","end":"2024-04-26 16:05:05.040761","duration":2}},{"council_id":"PEN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:40:02] Fetching Scraper for: PEN                              handlers.py:23\n           Begin attempting to scrape: PEN                        handlers.py:27\n[15:40:03] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:40:04] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: PEN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:40:02.721504","end":"2024-04-26 15:40:04.594120","duration":1}},{"council_id":"PKN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:54:22] Fetching Scraper for: PKN                              handlers.py:23\n           Begin attempting to scrape: PKN                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:54:23] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:54:24] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: PKN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:54:22.446392","end":"2024-04-26 15:54:24.606019","duration":2}},{"council_id":"PLY","missing":false,"latest_run":{"status_code":1,"log_text":"[16:32:14] Fetching Scraper for: PLY                              handlers.py:23\n           Begin attempting to scrape: PLY                        handlers.py:27\n[16:32:15] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:32:16] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://democracy.plymouth.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n[16:32:21] The read operation timed out                           handlers.py:36\n           Finished attempting to scrape: PLY                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2024-04-26 16:32:14.542017","end":"2024-04-26 16:32:21.760200","duration":7}},{"council_id":"RFW","missing":false,"latest_run":{"status_code":1,"log_text":"[15:46:39] Fetching Scraper for: RFW                              handlers.py:23\n           Begin attempting to scrape: RFW                        handlers.py:27\n[15:46:40] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:46:41] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: RFW                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:46:39.755138","end":"2024-04-26 15:46:41.773514","duration":2}},{"council_id":"ROS","missing":false,"latest_run":{"status_code":1,"log_text":"[16:06:26] Fetching Scraper for: ROS                              handlers.py:23\n           Begin attempting to scrape: ROS                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:06:27] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:06:28] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: ROS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:06:26.487363","end":"2024-04-26 16:06:28.782178","duration":2}},{"council_id":"SHE","missing":false,"latest_run":{"status_code":null,"log_text":"[11:28:20] Fetching Scraper for: SHE                              handlers.py:22\n           Begin attempting to scrape: SHE                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in SHE...                               base.py:186\n[11:28:21] Getting all files in SHE/json...                          base.py:186\n           ...found 30 files in SHE/json                             base.py:202\n           Getting all files in SHE/raw...                           base.py:186\n           ...found 30 files in SHE/raw                              base.py:202\n           ...found 61 files in SHE                                  base.py:202\n           Deleting batch no. 1 consisting of 61 files               base.py:211\n[11:28:32] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n           Finished attempting to scrape: SHE                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:28:20.509898","end":"2022-04-04 11:28:32.871624","duration":12}},{"council_id":"SHO","missing":false,"latest_run":{"status_code":1,"log_text":"[15:52:27] Fetching Scraper for: SHO                              handlers.py:23\n           Begin attempting to scrape: SHO                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:52:28] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:52:29] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://democracy.sholland.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1007)                                                        \n           Finished attempting to scrape: SHO                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n","start":"2024-04-26 15:52:27.152076","end":"2024-04-26 15:52:29.346312","duration":2}},{"council_id":"SLK","missing":false,"latest_run":{"status_code":1,"log_text":"[15:57:15] Fetching Scraper for: SLK                              handlers.py:23\n           Begin attempting to scrape: SLK                        handlers.py:27\n[15:57:16] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[15:57:17] ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n[15:57:18] Finished attempting to scrape: SLK                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:57:15.913998","end":"2024-04-26 15:57:18.148186","duration":2}},{"council_id":"SND","missing":false,"latest_run":{"status_code":1,"log_text":"[16:01:50] Fetching Scraper for: SND                              handlers.py:23\n           Begin attempting to scrape: SND                        handlers.py:27\n[16:01:51] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:01:53] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: SND                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:01:50.771129","end":"2024-04-26 16:01:53.729570","duration":2}},{"council_id":"SOL","missing":false,"latest_run":{"status_code":1,"log_text":"[16:33:39] Fetching Scraper for: SOL                              handlers.py:23\n           Begin attempting to scrape: SOL                        handlers.py:27\n[16:33:40] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:33:41] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://eservices.solihull.gov.uk/mgInternet/mgWebService.           \n           asmx/GetCouncillorsByWard                                            \n           [Errno 111] Connection refused                         handlers.py:36\n[16:33:42] Finished attempting to scrape: SOL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 111] Connection refused\n","start":"2024-04-26 16:33:39.782532","end":"2024-04-26 16:33:42.032633","duration":2}},{"council_id":"SST","missing":false,"latest_run":{"status_code":1,"log_text":"[15:29:47] Fetching Scraper for: SST                              handlers.py:23\n           Begin attempting to scrape: SST                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:29:48] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:29:49] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: SST                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:29:47.170087","end":"2024-04-26 15:29:49.235397","duration":2}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[15:54:36] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n[15:54:37] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:54:38] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n[15:54:39] Finished attempting to scrape: STG                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:54:36.764068","end":"2024-04-26 15:54:39.027924","duration":2}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[09:16:10] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[09:16:11] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:16:12] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.threerivers.gov.uk/listing/councillors                   \n[09:16:14] 'NoneType' object has no attribute 'findNext'          handlers.py:36\n[09:16:15] Finished attempting to scrape: THE                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"scrapers/THE-three-rivers/councillors.py\", line 15, in get_list_container\n    return soup.find(\"h3\", text=\"District Councillor\").findNext(\"ul\")\nAttributeError: 'NoneType' object has no attribute 'findNext'\n","start":"2024-04-27 09:16:10.453460","end":"2024-04-27 09:16:15.118084","duration":4}},{"council_id":"TWH","missing":false,"latest_run":{"status_code":1,"log_text":"[15:46:17] Fetching Scraper for: TWH                              handlers.py:23\n           Begin attempting to scrape: TWH                        handlers.py:27\n[15:46:18] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n[15:46:19] ...found 46 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 46 files in Councillors/raw                      base.py:225\n           ...found 93 files in Councillors                          base.py:225\n           Deleting batch no. 1 consisting of 93 files               base.py:236\n[15:46:20] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://democracy.towerhamlets.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n[15:46:25] The read operation timed out                           handlers.py:36\n           Finished attempting to scrape: TWH                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 218, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2024-04-26 15:46:17.653339","end":"2024-04-26 15:46:25.713155","duration":8}},{"council_id":"VGL","missing":false,"latest_run":{"status_code":1,"log_text":"[16:22:13] Fetching Scraper for: VGL                              handlers.py:23\n           Begin attempting to scrape: VGL                        handlers.py:27\n[16:22:14] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:22:15] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: VGL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:22:13.597691","end":"2024-04-26 16:22:15.677344","duration":2}},{"council_id":"WAE","missing":false,"latest_run":{"status_code":1,"log_text":"[09:13:41] Fetching Scraper for: WAE                              handlers.py:23\n           Begin attempting to scrape: WAE                        handlers.py:27\n[09:13:42] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:13:43] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://modgov.waverley.gov.uk/mgWebService.asmx/GetCounci           \n           llorsByWard                                                          \n           [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1007)                                                        \n           Finished attempting to scrape: WAE                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n","start":"2024-04-27 09:13:41.843462","end":"2024-04-27 09:13:43.980352","duration":2}},{"council_id":"WAW","missing":false,"latest_run":{"status_code":1,"log_text":"[16:01:09] Fetching Scraper for: WAW                              handlers.py:23\n           Begin attempting to scrape: WAW                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:01:10] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n[16:01:11] ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: WAW                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:01:09.103923","end":"2024-04-26 16:01:11.222584","duration":2}},{"council_id":"WDU","missing":false,"latest_run":{"status_code":1,"log_text":"[15:59:45] Fetching Scraper for: WDU                              handlers.py:23\n           Begin attempting to scrape: WDU                        handlers.py:27\n[15:59:46] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:59:47] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: WDU                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:59:45.661635","end":"2024-04-26 15:59:47.744294","duration":2}},{"council_id":"WLL","missing":false,"latest_run":{"status_code":1,"log_text":"[16:07:33] Fetching Scraper for: WLL                              handlers.py:23\n           Begin attempting to scrape: WLL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:07:34] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:07:35] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: WLL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 274, in get_councillors\n    req = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:07:33.327098","end":"2024-04-26 16:07:35.347670","duration":2}},{"council_id":"WLN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:45:08] Fetching Scraper for: WLN                              handlers.py:23\n           Begin attempting to scrape: WLN                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[15:45:09] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[15:45:10] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: WLN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/WLN-west-lothian/councillors.py\", line 12, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 15:45:08.130442","end":"2024-04-26 15:45:10.289669","duration":2}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[16:32:09] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:32:10] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:32:11] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: WRT                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:32:09.256014","end":"2024-04-26 16:32:11.172084","duration":1}},{"council_id":"WYE","missing":false,"latest_run":{"status_code":1,"log_text":"[16:01:33] Fetching Scraper for: WYE                              handlers.py:23\n           Begin attempting to scrape: WYE                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[16:01:34] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[16:01:35] ...data deleted.                                          base.py:264\n           ScraperBase.get() got an unexpected keyword argument   handlers.py:36\n           'verify'                                                             \n           Finished attempting to scrape: WYE                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 163, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 154, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(\nTypeError: ScraperBase.get() got an unexpected keyword argument 'verify'\n","start":"2024-04-26 16:01:33.352776","end":"2024-04-26 16:01:35.516905","duration":2}}]
