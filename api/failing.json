[{"council_id":"ABC","missing":false,"latest_run":{"status_code":1,"log_text":"[09:01:32] Fetching Scraper for: ABC                              handlers.py:23\n           Begin attempting to scrape: ABC                        handlers.py:27\n[09:01:34] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:01:35] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.armaghbanbridgecraigavon.gov.uk/councillors/             \n[09:01:37] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: ABC                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 09:01:32.449545","end":"2025-10-03 09:01:37.430084","duration":4}},{"council_id":"AGB","missing":false,"latest_run":{"status_code":1,"log_text":"[08:31:03] Fetching Scraper for: AGB                              handlers.py:23\n           Begin attempting to scrape: AGB                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:31:04] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:31:05] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.argyll-bute.gov.uk/councillor_list                       \n[08:31:06] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: AGB                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 186, in get_councillors\n    soup = self.get_page(url)\n           ^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:31:03.346061","end":"2025-10-03 08:31:06.685089","duration":3}},{"council_id":"ANN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:58:08] Fetching Scraper for: ANN                              handlers.py:23\n           Begin attempting to scrape: ANN                        handlers.py:27\n[08:58:09] Deleting existing data...                                 base.py:263\n[08:58:10] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:58:11] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://antrimandnewtownabbey.gov.uk/councillors/                    \n[08:58:12] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: ANN                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:58:08.501002","end":"2025-10-03 08:58:12.321643","duration":3}},{"council_id":"BFS","missing":false,"latest_run":{"status_code":1,"log_text":"[08:25:29] Fetching Scraper for: BFS                              handlers.py:23\n           Begin attempting to scrape: BFS                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:25:30] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:25:31] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://minutes3.belfastcity.gov.uk/mgWebService.asmx/GetC           \n           ouncillorsByWard                                                     \n           [Errno 16] Device or resource busy                     handlers.py:36\n           Finished attempting to scrape: BFS                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 16] Device or resource busy\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 16] Device or resource busy\n","start":"2025-10-02 08:25:29.361011","end":"2025-10-02 08:25:31.780181","duration":2}},{"council_id":"BLA","missing":false,"latest_run":{"status_code":1,"log_text":"[08:44:23] Fetching Scraper for: BLA                              handlers.py:23\n           Begin attempting to scrape: BLA                        handlers.py:27\n[08:44:25] Deleting existing data...                                 base.py:263\n[08:44:26] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:44:27] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://w3.blaby.gov.uk/decision-making/mgWebService.asmx/           \n           GetCouncillorsByWard                                                 \n           [Errno 16] Device or resource busy                     handlers.py:36\n           Finished attempting to scrape: BLA                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 16] Device or resource busy\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 16] Device or resource busy\n","start":"2025-10-02 08:44:23.910388","end":"2025-10-02 08:44:27.415199","duration":3}},{"council_id":"CAN","missing":false,"latest_run":{"status_code":1,"log_text":"[09:03:56] Fetching Scraper for: CAN                              handlers.py:23\n           Begin attempting to scrape: CAN                        handlers.py:27\n[09:03:57] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n[09:03:58] Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors                                                       \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n[09:03:59] Finished attempting to scrape: CAN                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 09:03:56.884627","end":"2025-10-03 09:03:59.238047","duration":2}},{"council_id":"CLK","missing":false,"latest_run":{"status_code":1,"log_text":"[08:23:05] Fetching Scraper for: CLK                              handlers.py:23\n           Begin attempting to scrape: CLK                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:23:06] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:23:07] ...data deleted.                                          base.py:270\n           Scraping from https://www.clacks.gov.uk/council/wards/     base.py:52\n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: CLK                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:23:05.370599","end":"2025-10-03 08:23:07.890346","duration":2}},{"council_id":"CMD","missing":false,"latest_run":{"status_code":1,"log_text":"[08:29:33] Fetching Scraper for: CMD                              handlers.py:23\n           Begin attempting to scrape: CMD                        handlers.py:27\n[08:29:34] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:29:35] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:29:36] ...data deleted.                                          base.py:270\n           No module named 'curl_cffi'                            handlers.py:36\n           Finished attempting to scrape: CMD                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"scrapers/CMD-camden/councillors.py\", line 8, in get\n    from curl_cffi import requests\nModuleNotFoundError: No module named 'curl_cffi'\n","start":"2025-10-02 08:29:33.916057","end":"2025-10-02 08:29:36.363773","duration":2}},{"council_id":"DND","missing":false,"latest_run":{"status_code":1,"log_text":"[08:50:58] Fetching Scraper for: DND                              handlers.py:23\n           Begin attempting to scrape: DND                        handlers.py:27\n[08:50:59] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:51:00] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:51:01] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.dundeecity.gov.uk/service-area/councillors/cou           \n           ncillors-political-wards-ward-number                                 \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: DND                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-02 08:50:58.972602","end":"2025-10-02 08:51:01.392948","duration":2}},{"council_id":"DST","missing":false,"latest_run":{"status_code":1,"log_text":"[08:35:14] Fetching Scraper for: DST                              handlers.py:23\n           Begin attempting to scrape: DST                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:35:15] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:35:16] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://moderngov.dorsetcouncil.gov.uk/mgWebService.asmx/Ge           \n           tCouncillorsByWard                                                   \n[08:35:26] timed out                                              handlers.py:36\n           Finished attempting to scrape: DST                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 103, in handle_request\n    return self._connection.handle_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/http11.py\", line 136, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 106, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/http11.py\", line 177, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/http11.py\", line 217, in _receive_event\n    data = self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 126, in read\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: timed out\n","start":"2025-10-02 08:35:14.119698","end":"2025-10-02 08:35:26.673644","duration":12}},{"council_id":"EAT","missing":false,"latest_run":{"status_code":1,"log_text":"[09:01:27] Fetching Scraper for: EAT                              handlers.py:23\n           Begin attempting to scrape: EAT                        handlers.py:27\n[09:01:28] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:01:29] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://meetings.eastleigh.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           Client error '403 Forbidden' for url                   handlers.py:36\n           'https://meetings.eastleigh.gov.uk/mgWebService.asmx/G               \n           etCouncillorsByWard'                                                 \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/403                                                               \n           Finished attempting to scrape: EAT                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://meetings.eastleigh.gov.uk/mgWebService.asmx/GetCouncillorsByWard'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n","start":"2025-10-02 09:01:27.725295","end":"2025-10-02 09:01:29.937437","duration":2}},{"council_id":"EDU","missing":false,"latest_run":{"status_code":1,"log_text":"[08:27:09] Fetching Scraper for: EDU                              handlers.py:23\n           Begin attempting to scrape: EDU                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:27:10] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:27:11] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.eastdunbarton.gov.uk/residents/council-democra           \n           cy/committees-and-councillors/councillors-2017                       \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.eastdunbarton.gov.uk/residents/council-de               \n           mocracy/committees-and-councillors/councillors-2017'                 \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n[08:27:12] Finished attempting to scrape: EDU                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 147, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.eastdunbarton.gov.uk/residents/council-democracy/committees-and-councillors/councillors-2017'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-10-02 08:27:09.379963","end":"2025-10-02 08:27:12.049104","duration":2}},{"council_id":"ELM","missing":false,"latest_run":{"status_code":1,"log_text":"[08:57:38] Fetching Scraper for: ELM                              handlers.py:23\n           Begin attempting to scrape: ELM                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:57:39] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:57:40] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://mygov.elmbridge.gov.uk/mgWebService.asmx/GetCouncil           \n           lorsByWard                                                           \n[08:57:50] timed out                                              handlers.py:36\n           Finished attempting to scrape: ELM                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: timed out\n","start":"2025-10-02 08:57:38.357326","end":"2025-10-02 08:57:50.855624","duration":12}},{"council_id":"ELN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:22:40] Fetching Scraper for: ELN                              handlers.py:23\n           Begin attempting to scrape: ELN                        handlers.py:27\n[08:22:41] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:22:42] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.eastlothian.gov.uk/councillors/name                      \n[08:22:43] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: ELN                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:22:40.709147","end":"2025-10-03 08:22:43.341416","duration":2}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[08:32:23] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:32:24] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:32:25] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://cne-siar.gov.uk/home/your-council/council-members/           \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.cne-siar.gov.uk/home/your-council/council               \n           -members/'                                                           \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n[08:32:26] Finished attempting to scrape: ELS                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 147, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.cne-siar.gov.uk/home/your-council/council-members/'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-10-02 08:32:23.250869","end":"2025-10-02 08:32:26.046664","duration":2}},{"council_id":"ERE","missing":false,"latest_run":{"status_code":1,"log_text":"[09:01:42] Fetching Scraper for: ERE                              handlers.py:23\n           Begin attempting to scrape: ERE                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[09:01:43] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:01:44] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://moderngov.erewash.gov.uk/mgWebService.asmx/GetCounc           \n           illorsByWard                                                         \n[09:01:45] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n           Finished attempting to scrape: ERE                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-02 09:01:42.503433","end":"2025-10-02 09:01:45.213394","duration":2}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[08:56:10] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n[08:56:11] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:56:12] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n[08:56:13] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: ERW                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:56:10.881450","end":"2025-10-03 08:56:13.230978","duration":2}},{"council_id":"FAL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:54:14] Fetching Scraper for: FAL                              handlers.py:23\n           Begin attempting to scrape: FAL                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:54:15] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:54:16] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.falkirk.gov.uk/services/council-democracy/coun           \n           cillors-decision-making/councillors/                                 \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.falkirk.gov.uk/services/council-democracy               \n           /councillors-decision-making/councillors/'                           \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n           Finished attempting to scrape: FAL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 147, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.falkirk.gov.uk/services/council-democracy/councillors-decision-making/councillors/'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-10-02 08:54:14.019425","end":"2025-10-02 08:54:16.477364","duration":2}},{"council_id":"FIF","missing":false,"latest_run":{"status_code":1,"log_text":"[13:42:48] Fetching Scraper for: FIF                              handlers.py:23\n           Begin attempting to scrape: FIF                        handlers.py:27\n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor                                                                \n[13:42:50] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/buckhaven,-methil-and-wemyss-villages/cllr.-tom-adam           \n           s                                                                    \n[13:42:51] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/buckhaven,-methil-and-wemyss-villages/cllr-ken-caldw           \n           ell                                                                  \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/buckhaven,-methil-and-wemyss-villages/cllr-john-obri           \n           en                                                                   \n[13:42:52] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/burntisland,-kinghorn-and-western-kirkcaldy/cllr-les           \n           ley-backhouse                                                        \n[13:42:53] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/burntisland,-kinghorn-and-western-kirkcaldy/cllr-kat           \n           hleen-leslie                                                         \n[13:42:54] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/burntisland,-kinghorn-and-western-kirkcaldy/cllr.-ju           \n           lie-macdougall                                                       \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/cowdenbeath/cllr-alistair-bain                                 \n[13:42:55] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/cowdenbeath/cllr-alex-campbell                                 \n[13:42:56] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/cowdenbeath/cllr.-bailey-lee-robb                              \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/cowdenbeath/cllr-darren-watt                                   \n[13:42:57] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/cupar/cllr.-john-caffrey                                       \n[13:42:58] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/cupar/cllr.-stefan-hoggan                                      \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/cupar/cllr-margaret-kennedy                                    \n[13:42:59] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-central/cllr.-aude-boubaker-calder                 \n[13:43:00] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-central/derek-glen                                 \n[13:43:01] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-central/cllr.-jean-hall-muir                       \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-central/cllr-jim-leishman                          \n[13:43:02] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-north/cllr.-auxi-barrera                           \n[13:43:04] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-north/cllr.-gavin-ellis                            \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-north/cllr.-gordon-pryde                           \n[13:43:05] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-south/cllr.-naz-anis-miah                          \n[13:43:06] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-south/cllr.-lynn-wardlaw                           \n[13:43:07] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-south/cllr.-james-calder                           \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/dunfermline-south/cllr.-cara-hilton                            \n[13:43:08] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/east-neuk-and-landward/cllr.-fiona-corps                       \n[13:43:09] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/east-neuk-and-landward/cllr.-sean-dillon                       \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/east-neuk-and-landward/cllr.-alycia-hayes                      \n[13:43:10] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-central-and-thornton/cllr.-lynda-holton             \n[13:43:11] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-central-and-thornton/cllr.-derek-noble              \n[13:43:12] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-central-and-thornton/cllr.-daniel-wilson            \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-north,-leslie-and-markinch/cllr.-john-bea           \n           re                                                                   \n[13:43:13] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-north,-leslie-and-markinch/cllr.-peter-gu           \n           lline                                                                \n[13:43:14] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-north,-leslie-and-markinch/cllr.-lynn-mow           \n           att                                                                  \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-north,-leslie-and-markinch/cllr.-jan-winc           \n           ott                                                                  \n[13:43:15] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-west-and-kinglassie/cllr.-altany-craik              \n[13:43:16] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-west-and-kinglassie/cllr.-julie-ford                \n[13:43:17] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/glenrothes-west-and-kinglassie/cllr.-craig-walker              \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/howe-of-fife-and-tay-coast/cllr.-gary-holt                     \n[13:43:18] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/howe-of-fife-and-tay-coast/cllr.-donald-lothian                \n[13:43:19] Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/howe-of-fife-and-tay-coast/cllr.-david-macdiarmid              \n           Scraping from                                              base.py:52\n           https://www.fife.gov.uk/kb/docs/articles/about-your-counci           \n           l2/politicians-and-committees/your-local-councillors/counc           \n           illor/inverkeithing-and-dalgety-bay/cllr.-david-barratt              \n[13:43:20] 'src'                                                  handlers.py:36\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 130, in run\n    councillor = self.get_single_councillor(councillor_html)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/scrapers/FIF-fife/councillors.py\", line 48, in get_single_councillor\n    soup.select_one(\".asset-contents img\")[\"src\"],\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^\n  File \"/opt/python/bs4/element.py\", line 2214, in __getitem__\n    return self.attrs[key]\n           ~~~~~~~~~~^^^^^\nKeyError: 'src'\n","start":"2025-10-04 13:42:48.014106","end":"2025-10-04 13:43:22.842989","duration":34}},{"council_id":"FYL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:32:21] Fetching Scraper for: FYL                              handlers.py:23\n           Begin attempting to scrape: FYL                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:32:22] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:32:23] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://fylde.cmis.uk.com/fylde/CouncillorsandMP.aspx                \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n[08:32:24] Finished attempting to scrape: FYL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 285, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-03 08:32:21.127959","end":"2025-10-03 08:32:24.269685","duration":3}},{"council_id":"GLG","missing":false,"latest_run":{"status_code":1,"log_text":"[08:37:06] Fetching Scraper for: GLG                              handlers.py:23\n           Begin attempting to scrape: GLG                        handlers.py:27\n[08:37:07] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:37:08] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://onlineservices.glasgow.gov.uk/councillorsandcommit           \n           tees/allMembers.asp?sort=0&page=0&rec=100                            \n[08:37:09] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: GLG                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:37:06.863039","end":"2025-10-03 08:37:09.250018","duration":2}},{"council_id":"HAO","missing":false,"latest_run":{"status_code":1,"log_text":"[08:33:38] Fetching Scraper for: HAO                              handlers.py:23\n[08:33:39] Begin attempting to scrape: HAO                        handlers.py:27\n           Scraping from                                              base.py:52\n           https://cmis.harborough.gov.uk/cmis5/Councillors.aspx                \n[08:33:41] Scraping from                                              base.py:52\n           https://cmis.harborough.gov.uk/cmis5/Councillors/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/1427/ScreenMode/Ward/Defaul           \n           t.aspx                                                               \n[08:33:43] Scraping from                                              base.py:52\n           https://cmis.harborough.gov.uk/cmis5/Councillors/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/2186/ScreenMode/Ward/Defaul           \n           t.aspx                                                               \n[08:33:44] Scraping from                                              base.py:52\n           https://cmis.harborough.gov.uk/cmis5/Councillors/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/2054/ScreenMode/Ward/Defaul           \n           t.aspx                                                               \n[08:33:45] Scraping from                                              base.py:52\n           https://cmis.harborough.gov.uk/cmis5/Councillors/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/2196/ScreenMode/Ward/Defaul           \n           t.aspx                                                               \n[08:33:46] 'title'                                                handlers.py:36\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 256, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/opt/python/bs4/element.py\", line 2214, in __getitem__\n    return self.attrs[key]\n           ~~~~~~~~~~^^^^^\nKeyError: 'title'\n","start":"2025-10-04 08:33:38.996580","end":"2025-10-04 08:33:48.286005","duration":9}},{"council_id":"HAR","missing":false,"latest_run":{"status_code":1,"log_text":"[08:46:42] Fetching Scraper for: HAR                              handlers.py:23\n           Begin attempting to scrape: HAR                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:46:43] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:46:44] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://moderngov.harlow.gov.uk/mgWebService.asmx/GetCounci           \n           llorsByWard                                                          \n[08:46:45] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n           Finished attempting to scrape: HAR                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-03 08:46:42.271528","end":"2025-10-03 08:46:45.291964","duration":3}},{"council_id":"HIG","missing":false,"latest_run":{"status_code":1,"log_text":"[08:45:47] Fetching Scraper for: HIG                              handlers.py:23\n           Begin attempting to scrape: HIG                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:45:48] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:45:49] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://democracy.highpeak.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n[08:45:55] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n[08:45:56] Finished attempting to scrape: HIG                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-03 08:45:47.043373","end":"2025-10-03 08:45:56.112006","duration":9}},{"council_id":"HLD","missing":false,"latest_run":{"status_code":1,"log_text":"[08:42:17] Fetching Scraper for: HLD                              handlers.py:23\n           Begin attempting to scrape: HLD                        handlers.py:27\n[08:42:18] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:42:20] ...data deleted.                                          base.py:270\n           Scraping from https://www.highland.gov.uk/councillors/name base.py:52\n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: HLD                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:42:17.533468","end":"2025-10-03 08:42:20.525160","duration":2}},{"council_id":"HPL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:32:29] Fetching Scraper for: HPL                              handlers.py:23\n           Begin attempting to scrape: HPL                        handlers.py:27\n[08:32:30] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:32:31] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.hartlepool.gov.uk/councillors/name                       \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.hartlepool.gov.uk/councillors/name'                     \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n[08:32:32] Finished attempting to scrape: HPL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 147, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.hartlepool.gov.uk/councillors/name'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-10-02 08:32:29.731304","end":"2025-10-02 08:32:32.116061","duration":2}},{"council_id":"IVC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:32:34] Fetching Scraper for: IVC                              handlers.py:23\n           Begin attempting to scrape: IVC                        handlers.py:27\n[08:32:35] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:32:36] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.inverclyde.gov.uk/meetings/councillors                   \n[08:32:43] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n[08:32:44] Finished attempting to scrape: IVC                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:32:34.820310","end":"2025-10-03 08:32:44.209684","duration":9}},{"council_id":"KHL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:21:31] Fetching Scraper for: KHL                              handlers.py:23\n           Begin attempting to scrape: KHL                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:21:32] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:21:33] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://cmis.hullcc.gov.uk/cmis/CouncillorsandSeniorOffice           \n           rs/CouncillorsandSeniorOfficers.aspx                                 \n           [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1010)                                                        \n           Finished attempting to scrape: KHL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 156, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 154, in start_tls\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 281, in get_councillors\n    req = self.get(\n          ^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n","start":"2025-10-02 08:21:31.320425","end":"2025-10-02 08:21:33.683773","duration":2}},{"council_id":"MAI","missing":false,"latest_run":{"status_code":1,"log_text":"[08:36:06] Fetching Scraper for: MAI                              handlers.py:23\n           Begin attempting to scrape: MAI                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:36:07] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:36:08] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://meetings.maidstone.gov.uk//mgWebService.asmx/GetCo           \n           uncillorsByWard                                                      \n[08:36:18] The read operation timed out                           handlers.py:36\n[08:36:19] Finished attempting to scrape: MAI                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 103, in handle_request\n    return self._connection.handle_request(request)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/http11.py\", line 136, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 106, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/http11.py\", line 177, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/http11.py\", line 217, in _receive_event\n    data = self._network_stream.read(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 126, in read\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2025-10-03 08:36:06.473476","end":"2025-10-03 08:36:19.262240","duration":12}},{"council_id":"MAV","missing":false,"latest_run":{"status_code":1,"log_text":"[08:23:46] Fetching Scraper for: MAV                              handlers.py:23\n           Begin attempting to scrape: MAV                        handlers.py:27\n[08:23:47] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:23:48] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://moderngov.malvernhills.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n           [Errno 104] Connection reset by peer                   handlers.py:36\n[08:23:49] Finished attempting to scrape: MAV                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 156, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 154, in start_tls\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 104] Connection reset by peer\n","start":"2025-10-02 08:23:46.464481","end":"2025-10-02 08:23:49.122025","duration":2}},{"council_id":"MLN","missing":false,"latest_run":{"status_code":1,"log_text":"[09:36:05] Fetching Scraper for: MLN                              handlers.py:23\n           Begin attempting to scrape: MLN                        handlers.py:27\n           Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors.aspx                 \n[09:36:06] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/141/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:08] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/244/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:09] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/143/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:10] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/144/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:12] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/245/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:13] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/19/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[09:36:14] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/140/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:16] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/240/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:17] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/234/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:18] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/241/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[09:36:21] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/20/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[09:36:23] Scraping from                                              base.py:52\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/22/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[09:36:24] 'title'                                                handlers.py:36\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 256, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^\n  File \"/opt/python/bs4/element.py\", line 2214, in __getitem__\n    return self.attrs[key]\n           ~~~~~~~~~~^^^^^\nKeyError: 'title'\n","start":"2025-10-04 09:36:05.376733","end":"2025-10-04 09:36:26.247847","duration":20}},{"council_id":"MOL","missing":false,"latest_run":{"status_code":1,"log_text":"[09:03:25] Fetching Scraper for: MOL                              handlers.py:23\n           Begin attempting to scrape: MOL                        handlers.py:27\n[09:03:26] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:03:27] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.molevalley.gov.uk/councillors-decision-making/           \n           who-are-your-councillors/                                            \n[09:03:31] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: MOL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 09:03:25.773820","end":"2025-10-03 09:03:31.707670","duration":5}},{"council_id":"MTY","missing":false,"latest_run":{"status_code":1,"log_text":"[09:04:26] Fetching Scraper for: MTY                              handlers.py:23\n           Begin attempting to scrape: MTY                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[09:04:27] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:04:28] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://democracy.merthyr.gov.uk/mgWebService.asmx/GetCounc           \n           illorsByWard                                                         \n           [Errno 104] Connection reset by peer                   handlers.py:36\n           Finished attempting to scrape: MTY                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 156, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 154, in start_tls\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 104] Connection reset by peer\n","start":"2025-10-02 09:04:26.030045","end":"2025-10-02 09:04:28.384304","duration":2}},{"council_id":"MUL","missing":false,"latest_run":{"status_code":1,"log_text":"[09:06:12] Fetching Scraper for: MUL                              handlers.py:23\n           Begin attempting to scrape: MUL                        handlers.py:27\n[09:06:13] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:06:14] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://mid-ulster.cmis-ni.org/midulster/Councillors.aspx            \n           [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1010)                                                        \n           Finished attempting to scrape: MUL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 156, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 154, in start_tls\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 281, in get_councillors\n    req = self.get(\n          ^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n","start":"2025-10-02 09:06:12.676204","end":"2025-10-02 09:06:14.886442","duration":2}},{"council_id":"NEC","missing":false,"latest_run":{"status_code":1,"log_text":"[09:04:03] Fetching Scraper for: NEC                              handlers.py:23\n           Begin attempting to scrape: NEC                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[09:04:04] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:04:05] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://moderngov.newcastle-staffs.gov.uk/mgWebService.asmx           \n           /GetCouncillorsByWard                                                \n[09:04:15] timed out                                              handlers.py:36\n           Finished attempting to scrape: NEC                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: timed out\n","start":"2025-10-02 09:04:03.054611","end":"2025-10-02 09:04:15.671700","duration":12}},{"council_id":"NUN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:31:57] Fetching Scraper for: NUN                              handlers.py:23\n           Begin attempting to scrape: NUN                        handlers.py:27\n[08:31:58] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:31:59] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.nuneatonandbedworth.gov.uk/councillors                   \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n[08:32:00] Finished attempting to scrape: NUN                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:31:57.652241","end":"2025-10-03 08:32:00.136866","duration":2}},{"council_id":"ORK","missing":false,"latest_run":{"status_code":1,"log_text":"[08:26:24] Fetching Scraper for: ORK                              handlers.py:23\n           Begin attempting to scrape: ORK                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:26:25] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.orkney.gov.uk/your-council/councillors/council           \n           lors/                                                                \n[08:26:26] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: ORK                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:26:24.144485","end":"2025-10-03 08:26:26.727930","duration":2}},{"council_id":"ROC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:45:07] Fetching Scraper for: ROC                              handlers.py:23\n           Begin attempting to scrape: ROC                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:45:08] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://rochford.cmis.uk.com/rochford/Members/tabid/62/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[08:45:09] Server error '502 Bad Gateway' for url                 handlers.py:36\n           'https://rochford.cmis.uk.com/rochford/Members/tabid/6               \n           2/ScreenMode/Alphabetical/Default.aspx'                              \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/502                                                               \n           Finished attempting to scrape: ROC                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 281, in get_councillors\n    req = self.get(\n          ^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Server error '502 Bad Gateway' for url 'https://rochford.cmis.uk.com/rochford/Members/tabid/62/ScreenMode/Alphabetical/Default.aspx'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/502\n","start":"2025-10-02 08:45:07.031581","end":"2025-10-02 08:45:09.235248","duration":2}},{"council_id":"ROS","missing":false,"latest_run":{"status_code":1,"log_text":"[08:36:41] Fetching Scraper for: ROS                              handlers.py:23\n[08:36:45] Begin attempting to scrape: ROS                        handlers.py:27\n[08:36:51] Deleting existing data...                                 base.py:263\n[08:36:52] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:36:53] ...data deleted.                                          base.py:270\n           Scraping from https://www.rossendale.gov.uk/councillors    base.py:52\n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n[08:36:54] Finished attempting to scrape: ROS                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:36:41.489929","end":"2025-10-03 08:36:54.087799","duration":12}},{"council_id":"RUG","missing":false,"latest_run":{"status_code":1,"log_text":"[08:34:35] Fetching Scraper for: RUG                              handlers.py:23\n           Begin attempting to scrape: RUG                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:34:36] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:34:37] ...data deleted.                                          base.py:270\n           Scraping from https://www.rugby.gov.uk/councillors/        base.py:52\n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: RUG                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:34:35.402317","end":"2025-10-03 08:34:37.978665","duration":2}},{"council_id":"SAY","missing":false,"latest_run":{"status_code":1,"log_text":"[08:42:45] Fetching Scraper for: SAY                              handlers.py:23\n           Begin attempting to scrape: SAY                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:42:46] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:42:47] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.south-ayrshire.gov.uk/councillors/                       \n           Client error '403 Forbidden' for url                   handlers.py:36\n           'https://www.south-ayrshire.gov.uk/councillors/'                     \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/403                                                               \n           Finished attempting to scrape: SAY                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 147, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.south-ayrshire.gov.uk/councillors/'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n","start":"2025-10-02 08:42:45.233645","end":"2025-10-02 08:42:47.608526","duration":2}},{"council_id":"SFK","missing":false,"latest_run":{"status_code":1,"log_text":"[08:51:04] Fetching Scraper for: SFK                              handlers.py:23\n           Begin attempting to scrape: SFK                        handlers.py:27\n[08:51:05] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:51:06] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.suffolk.gov.uk/council-and-democracy/councillo           \n           rs-and-elected-representatives/find-your-councillor/?ward=           \n           &action=SEARCH&party=&name=                                          \n[08:51:07] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: SFK                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-02 08:51:04.768708","end":"2025-10-02 08:51:07.482651","duration":2}},{"council_id":"SFT","missing":false,"latest_run":{"status_code":1,"log_text":"[08:23:11] Fetching Scraper for: SFT                              handlers.py:23\n           Begin attempting to scrape: SFT                        handlers.py:27\n[08:23:12] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:23:13] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://modgov.sefton.gov.uk/mgWebService.asmx/GetCouncill           \n           orsByWard                                                            \n[08:23:14] Client error '403 Forbidden' for url                   handlers.py:36\n           'https://modgov.sefton.gov.uk/mgWebService.asmx/GetCou               \n           ncillorsByWard'                                                      \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/403                                                               \n           Finished attempting to scrape: SFT                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 58, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://modgov.sefton.gov.uk/mgWebService.asmx/GetCouncillorsByWard'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n","start":"2025-10-02 08:23:11.860994","end":"2025-10-02 08:23:14.254639","duration":2}},{"council_id":"SGC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:33:31] Fetching Scraper for: SGC                              handlers.py:23\n           Begin attempting to scrape: SGC                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:33:32] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:33:33] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://council.southglos.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[08:33:43] timed out                                              handlers.py:36\n           Finished attempting to scrape: SGC                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: timed out\n","start":"2025-10-02 08:33:31.437141","end":"2025-10-02 08:33:43.908419","duration":12}},{"council_id":"SPE","missing":false,"latest_run":{"status_code":1,"log_text":"[08:48:02] Fetching Scraper for: SPE                              handlers.py:23\n           Begin attempting to scrape: SPE                        handlers.py:27\n[08:48:03] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:48:04] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://democracy.spelthorne.gov.uk/mgWebService.asmx/GetC           \n           ouncillorsByWard                                                     \n[08:48:05] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n           Finished attempting to scrape: SPE                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-02 08:48:02.658267","end":"2025-10-02 08:48:05.671782","duration":3}},{"council_id":"SRY","missing":false,"latest_run":{"status_code":1,"log_text":"[08:55:04] Fetching Scraper for: SRY                              handlers.py:23\n           Begin attempting to scrape: SRY                        handlers.py:27\n[08:55:05] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:55:06] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://mycouncil.surreycc.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n[08:55:07] Finished attempting to scrape: SRY                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-02 08:55:04.634389","end":"2025-10-02 08:55:07.108548","duration":2}},{"council_id":"STF","missing":false,"latest_run":{"status_code":1,"log_text":"[08:42:36] Fetching Scraper for: STF                              handlers.py:23\n           Begin attempting to scrape: STF                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:42:37] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:42:38] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://democracy.staffsmoorlands.gov.uk/mgWebService.asmx           \n           /GetCouncillorsByWard                                                \n[08:42:43] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n[08:42:44] Finished attempting to scrape: STF                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-03 08:42:36.157510","end":"2025-10-03 08:42:44.088216","duration":7}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[08:51:16] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n[08:51:17] Deleting existing data...                                 base.py:263\n[08:51:18] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:51:19] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.stirling.gov.uk/council-and-committees/politic           \n           ians-and-elections/councillors/                                      \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: STG                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-02 08:51:16.222553","end":"2025-10-02 08:51:19.799595","duration":3}},{"council_id":"STH","missing":false,"latest_run":{"status_code":1,"log_text":"[08:44:23] Fetching Scraper for: STH                              handlers.py:23\n           Begin attempting to scrape: STH                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:44:24] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:44:25] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.southampton.gov.uk/modernGov//mgWebService.asm           \n           x/GetCouncillorsByWard                                               \n[08:44:27] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n           Finished attempting to scrape: STH                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-03 08:44:23.486870","end":"2025-10-03 08:44:27.943259","duration":4}},{"council_id":"STN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:34:23] Fetching Scraper for: STN                              handlers.py:23\n           Begin attempting to scrape: STN                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:34:24] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:34:25] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://moderngov.sutton.gov.uk/mgWebService.asmx/GetCounc           \n           illorsByWard                                                         \n[08:34:35] timed out                                              handlers.py:36\n           Finished attempting to scrape: STN                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: timed out\n","start":"2025-10-02 08:34:23.376135","end":"2025-10-02 08:34:35.773497","duration":12}},{"council_id":"SWL","missing":false,"latest_run":{"status_code":1,"log_text":"[09:02:16] Fetching Scraper for: SWL                              handlers.py:23\n           Begin attempting to scrape: SWL                        handlers.py:27\n[09:02:17] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[09:02:18] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://services.swale.gov.uk/mgWebService.asmx/GetCouncill           \n           orsByWard                                                            \n[09:02:19] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: lxml. Do you need to install a parser                     \n           library?                                                             \n           Finished attempting to scrape: SWL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 226, in get_councillors\n    soup = BeautifulSoup(req.text, \"lxml\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?\n","start":"2025-10-03 09:02:16.533992","end":"2025-10-03 09:02:19.988634","duration":3}},{"council_id":"TAW","missing":false,"latest_run":{"status_code":1,"log_text":"[08:26:45] Fetching Scraper for: TAW                              handlers.py:23\n           Begin attempting to scrape: TAW                        handlers.py:27\n[08:26:46] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:26:47] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://democracy.tamworth.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[08:26:57] timed out                                              handlers.py:36\n[08:26:58] Finished attempting to scrape: TAW                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 124, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 207, in connect_tcp\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: timed out\n","start":"2025-10-02 08:26:45.527237","end":"2025-10-02 08:26:58.249926","duration":12}},{"council_id":"TEN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:29:26] Fetching Scraper for: TEN                              handlers.py:23\n           Begin attempting to scrape: TEN                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:29:27] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:29:28] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://tdcdemocracy.tendringdc.gov.uk/mgWebService.asmx/Ge           \n           tCouncillorsByWard                                                   \n           [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1010)                                                        \n           Finished attempting to scrape: TEN                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 156, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 154, in start_tls\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1010)\n","start":"2025-10-02 08:29:26.248840","end":"2025-10-02 08:29:28.976137","duration":2}},{"council_id":"VGL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:22:25] Fetching Scraper for: VGL                              handlers.py:23\n           Begin attempting to scrape: VGL                        handlers.py:27\n[08:22:26] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:22:27] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Councillors.aspx                               \n[08:22:28] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: VGL                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:22:25.878710","end":"2025-10-03 08:22:28.247359","duration":2}},{"council_id":"WAW","missing":false,"latest_run":{"status_code":1,"log_text":"[08:24:22] Fetching Scraper for: WAW                              handlers.py:23\n           Begin attempting to scrape: WAW                        handlers.py:27\n[08:24:23] Scraping from https://estates8.warwickdc.gov.uk/cmis/      base.py:52\n           Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/327/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:25] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/313/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:26] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/294/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:27] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/310/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:28] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/62/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[08:24:29] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/312/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:30] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/325/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:31] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/314/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:32] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/320/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:33] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/188/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:36] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/189/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:37] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/225/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:38] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/224/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:39] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/315/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[08:24:40] Scraping from                                              base.py:52\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ctl/ViewCMIS_Person/mid/536/id/55/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[08:24:42] No Division for                                        handlers.py:36\n           https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ               \n           /tabid/39/ctl/ViewCMIS_Person/mid/536/id/342/ScreenMod               \n           e/Alphabetical/Default.aspx                                          \n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 258, in get_single_councillor\n    councillor = self.add_councillor(\n                 ^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 37, in add_councillor\n    assert division, f\"No Division for {url}\"\n           ^^^^^^^^\nAssertionError: No Division for https://estates8.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tabid/39/ctl/ViewCMIS_Person/mid/536/id/342/ScreenMode/Alphabetical/Default.aspx\n","start":"2025-10-04 08:24:22.645488","end":"2025-10-04 08:24:44.134462","duration":21}},{"council_id":"WLN","missing":false,"latest_run":{"status_code":1,"log_text":"[14:43:26] Fetching Scraper for: WLN                              handlers.py:23\n           Begin attempting to scrape: WLN                        handlers.py:27\n[14:43:31] Scraping from https://westlothian.gov.uk/councillors       base.py:52\n[14:43:32] Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33888/Linlithgow                  \n           Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33889/Broxburn-Uphall-a           \n           nd-Winchburgh                                                        \n[14:43:33] Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33890/Livingston-North            \n[14:43:34] Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33891/Livingston-South            \n           Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33893/East-Livingston-a           \n           nd-East-Calder                                                       \n[14:43:35] Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33892/Fauldhouse-and-th           \n           e-Breich-Valley                                                      \n           Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33894/Whitburn-and-Blac           \n           kburn                                                                \n[14:43:36] Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33895/Bathgate                    \n           Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33896/Armadale-and-Blac           \n           kridge                                                               \n[14:43:37] Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33897/Councillor-Tom-Co           \n           nn                                                                   \n           Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33898/Councillor-Paulin           \n           e-Orr                                                                \n[14:43:38] Scraping from                                              base.py:52\n           https://westlothian.gov.uk/article/33899/Councillor-Sally-           \n           Pattle                                                               \n[14:43:39] list index out of range                                handlers.py:36\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 130, in run\n    councillor = self.get_single_councillor(councillor_html)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/scrapers/WLN-west-lothian/councillors.py\", line 37, in get_single_councillor\n    councillor.email = soup.select(\"a[href^=mailto]\")[0].get_text(strip=True)\n                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^\nIndexError: list index out of range\n","start":"2025-10-04 14:43:26.860899","end":"2025-10-04 14:43:45.875497","duration":19}},{"council_id":"WOC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:32:48] Fetching Scraper for: WOC                              handlers.py:23\n           Begin attempting to scrape: WOC                        handlers.py:27\n[08:32:49] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n[08:32:50] ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:32:51] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           http://committee.worcester.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           [Errno 104] Connection reset by peer                   handlers.py:36\n           Finished attempting to scrape: WOC                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 156, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 154, in start_tls\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 104] Connection reset by peer\n","start":"2025-10-02 08:32:48.939246","end":"2025-10-02 08:32:51.474368","duration":2}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[08:35:09] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n[08:35:10] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:35:11] ...data deleted.                                          base.py:270\n           Scraping from https://www.warrington.gov.uk/councillors    base.py:52\n[08:35:12] Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: WRT                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:35:09.591834","end":"2025-10-03 08:35:12.304825","duration":2}},{"council_id":"WYC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:58:57] Fetching Scraper for: WYC                              handlers.py:23\n           Begin attempting to scrape: WYC                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:58:58] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:58:59] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://mgov.wychavon.gov.uk/mgWebService.asmx/GetCouncill           \n           orsByWard                                                            \n           [Errno 104] Connection reset by peer                   handlers.py:36\n           Finished attempting to scrape: WYC                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 101, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 250, in handle_request\n    resp = self._pool.handle_request(req)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 256, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 236, in handle_request\n    response = connection.handle_request(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 78, in handle_request\n    stream = self._connect(request)\n             ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_sync/connection.py\", line 156, in _connect\n    stream = stream.start_tls(**kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpcore/_backends/sync.py\", line 154, in start_tls\n    with map_exceptions(exc_map):\n         ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno 104] Connection reset by peer\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 205, in run\n    wards = self.get_councillors()\n            ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 224, in get_councillors\n    req = self.get(self.format_councillor_api_url(), extra_headers=self.extra_headers)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/scrapers/base.py\", line 57, in get\n    response = self.http_client.get(url, headers=headers, timeout=self.timeout)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1053, in get\n    return self.request(\n           ^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 825, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_client.py\", line 1014, in _send_single_request\n    response = transport.handle_request(request)\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/httpx/_transports/default.py\", line 249, in handle_request\n    with map_httpcore_exceptions():\n         ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/lang/lib/python3.12/contextlib.py\", line 158, in __exit__\n    self.gen.throw(value)\n  File \"/opt/python/httpx/_transports/default.py\", line 118, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno 104] Connection reset by peer\n","start":"2025-10-02 08:58:57.497633","end":"2025-10-02 08:58:59.829271","duration":2}},{"council_id":"WYE","missing":false,"latest_run":{"status_code":1,"log_text":"[08:49:00] Fetching Scraper for: WYE                              handlers.py:23\n           Begin attempting to scrape: WYE                        handlers.py:27\n           Deleting existing data...                                 base.py:263\n[08:49:01] Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n           Deleting batch no. 1 consisting of 1 files                base.py:242\n[08:49:02] ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://forms.wyreforestdc.gov.uk/council/committees/com55           \n           .htm                                                                 \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n           Finished attempting to scrape: WYE                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 167, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-02 08:49:00.036201","end":"2025-10-02 08:49:02.573994","duration":2}},{"council_id":"ZET","missing":false,"latest_run":{"status_code":1,"log_text":"[08:23:39] Fetching Scraper for: ZET                              handlers.py:23\n           Begin attempting to scrape: ZET                        handlers.py:27\n[08:23:40] Deleting existing data...                                 base.py:263\n           Getting all files in Councillors...                       base.py:215\n           ...found 1 files in Councillors                           base.py:231\n[08:23:41] Deleting batch no. 1 consisting of 1 files                base.py:242\n           ...data deleted.                                          base.py:270\n           Scraping from                                              base.py:52\n           https://coins.shetland.gov.uk/allMembers.asp?sort=0&page=0           \n           &rec=24                                                              \n           Couldn't find a tree builder with the features you     handlers.py:36\n           requested: html5lib. Do you need to install a parser                 \n           library?                                                             \n[08:23:42] Finished attempting to scrape: ZET                        base.py:351\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 61, in run\n    for councillor_html in self.get_councillors():\n                           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"scrapers/ZET-shetland-islands/councillors.py\", line 17, in get_councillors\n    container = self.get_list_container()\n                ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_page\n    return BeautifulSoup(page, \"html5lib\")\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/python/bs4/__init__.py\", line 364, in __init__\n    raise FeatureNotFound(\nbs4.exceptions.FeatureNotFound: Couldn't find a tree builder with the features you requested: html5lib. Do you need to install a parser library?\n","start":"2025-10-03 08:23:39.732568","end":"2025-10-03 08:23:42.129594","duration":2}}]
