[{"council_id":"AND","missing":false,"latest_run":{"status_code":1,"log_text":"[09:57:33] Fetching Scraper for: AND                              handlers.py:23\n           Begin attempting to scrape: AND                        handlers.py:27\n[09:57:34] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[09:57:35] Getting all files in Councillors/json...                  base.py:209\n           ...found 16 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 16 files in Councillors/raw                      base.py:225\n           ...found 33 files in Councillors                          base.py:225\n           Deleting batch no. 1 consisting of 33 files               base.py:236\n[09:57:36] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.armaghbanbridgecraigavon.gov.uk/councillors/             \n           Server error '500 Internal Server Error' for url       handlers.py:36\n           'https://www.armaghbanbridgecraigavon.gov.uk/councillo               \n           rs/'                                                                 \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/500                                                               \n           Finished attempting to scrape: AND                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 152, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://www.armaghbanbridgecraigavon.gov.uk/councillors/'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n","start":"2025-03-16 09:57:33.920528","end":"2025-03-16 09:57:36.668391","duration":2}},{"council_id":"BDG","missing":false,"latest_run":{"status_code":1,"log_text":"[10:09:10] Fetching Scraper for: BDG                              handlers.py:23\n           Begin attempting to scrape: BDG                        handlers.py:27\n[10:09:11] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[10:09:12] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://modgov.lbbd.gov.uk/internet/mgWebService.asmx/GetC           \n           ouncillorsByWard                                                     \n[10:09:28] [Errno 110] Connection timed out                       handlers.py:36\n           Finished attempting to scrape: BDG                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: [Errno 110] Connection timed out\n","start":"2025-03-16 10:09:10.671328","end":"2025-03-16 10:09:28.594079","duration":17}},{"council_id":"BFS","missing":false,"latest_run":{"status_code":1,"log_text":"[09:10:19] Fetching Scraper for: BFS                              handlers.py:23\n           Begin attempting to scrape: BFS                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[09:10:20] Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 60 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 60 files in Councillors/raw                      base.py:225\n           ...found 121 files in Councillors                         base.py:225\n           Deleting batch no. 1 consisting of 100 files              base.py:236\n[09:10:21] Deleting batch no. 2 consisting of 21 files               base.py:236\n[09:10:22] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://minutes3.belfastcity.gov.uk/mgWebService.asmx/GetC           \n           ouncillorsByWard                                                     \n[09:10:52] The read operation timed out                           handlers.py:36\n           Finished attempting to scrape: BFS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2025-03-16 09:10:19.126302","end":"2025-03-16 09:10:52.508262","duration":33}},{"council_id":"BIR","missing":false,"latest_run":{"status_code":1,"log_text":"[10:24:14] Fetching Scraper for: BIR                              handlers.py:23\n           Begin attempting to scrape: BIR                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[10:24:15] Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 107 files in Councillors/json                    base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 107 files in Councillors/raw                     base.py:225\n[10:24:16] ...found 215 files in Councillors                         base.py:225\n           Deleting batch no. 1 consisting of 100 files              base.py:236\n           Deleting batch no. 2 consisting of 100 files              base.py:236\n[10:24:17] Deleting batch no. 3 consisting of 15 files               base.py:236\n[10:24:18] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors.aspx           \n[10:24:22] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1005/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:24:25] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1005/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:24:28] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1282/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:24:30] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1282/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:24:33] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1006/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:24:35] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1006/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:24:36] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/80/ScreenMode/Alphabet           \n           ical/Default.aspx                                                    \n[10:24:39] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/5/deirdre_alden            \n           ;                                                                    \n[10:24:41] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/126/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[10:24:47] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/7/robert_alden ;           \n[10:24:49] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/73/ScreenMode/Alphabet           \n           ical/Default.aspx                                                    \n[10:24:52] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/11/gurdial_singh           \n           _atwal ;                                                             \n[10:24:54] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1228/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:24:59] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1228/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:02] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1009/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:05] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1009/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:13] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1245/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:19] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1245/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:27] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1229/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:32] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1229/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:38] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/128/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[10:25:47] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/14/david_barrie            \n           ;                                                                    \n           Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/180/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[10:25:51] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/16/matt_bennett            \n           ;                                                                    \n[10:25:53] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1240/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:25:57] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1240/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:02] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1238/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:05] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1238/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:11] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1251/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:15] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1251/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:18] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/14/ScreenMode/Alphabet           \n           ical/Default.aspx                                                    \n[10:26:21] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/19/sir_albert_bo           \n           re ;                                                                 \n[10:26:23] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1023/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:28] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1023/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:32] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1227/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:34] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1227/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:36] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/15/ScreenMode/Alphabet           \n           ical/Default.aspx                                                    \n[10:26:39] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/22/marje_bridle            \n           ;                                                                    \n[10:26:40] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1231/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:45] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/1231/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[10:26:48] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/118/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[10:27:18] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/23/mick_brown ;            \n[10:27:19] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/130/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[10:27:27] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/28/zaker_choudhr           \n           y ;                                                                  \n[10:27:28] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/182/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[10:27:58] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/29/debbie_clancy           \n           ;                                                                    \n           Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/967/ScreenMode/Alphabe           \n           tical/Default.aspx                                                   \n[10:28:24] Scraping from                                              base.py:49\n           https://www.birmingham.gov.uk/councillors/123/liz_clements           \n[10:28:25] Scraping from                                              base.py:49\n           https://birmingham.cmis.uk.com/birmingham/Councillors/tabi           \n           d/63/ctl/ViewCMIS_Person/mid/383/id/30/ScreenMode/Alphabet           \n           ical/Default.aspx                                                    \n[10:28:56] The read operation timed out                           handlers.py:36\n           Committing batch 1 consisting of 48 files                 base.py:297\n[10:28:57] Finished attempting to scrape: BIR                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/BIR-birmingham/councillors.py\", line 45, in get_single_councillor\n    req = self.get(url)\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2025-03-16 10:24:14.381736","end":"2025-03-16 10:28:57.642881","duration":283}},{"council_id":"BOL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:41:52] Fetching Scraper for: BOL                              handlers.py:23\n           Begin attempting to scrape: BOL                        handlers.py:27\n[08:41:53] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[08:41:54] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.democracy.bolton.gov.uk/cmis5/People.aspx                \n           [Errno -2] Name or service not known                   handlers.py:36\n           Finished attempting to scrape: BOL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 272, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno -2] Name or service not known\n","start":"2025-03-16 08:41:52.726035","end":"2025-03-16 08:41:54.846738","duration":2}},{"council_id":"BRT","missing":false,"latest_run":{"status_code":1,"log_text":"[08:55:56] Fetching Scraper for: BRT                              handlers.py:23\n           Begin attempting to scrape: BRT                        handlers.py:27\n[08:55:57] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 44 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 44 files in Councillors/raw                      base.py:225\n           ...found 89 files in Councillors                          base.py:225\n[08:55:58] Deleting batch no. 1 consisting of 89 files               base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://democracy.broxtowe.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[08:56:14] [Errno 110] Connection timed out                       handlers.py:36\n           Finished attempting to scrape: BRT                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: [Errno 110] Connection timed out\n","start":"2025-03-16 08:55:56.677089","end":"2025-03-16 08:56:14.561508","duration":17}},{"council_id":"CAN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:36:21] Fetching Scraper for: CAN                              handlers.py:23\n           Begin attempting to scrape: CAN                        handlers.py:27\n[08:36:23] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n[08:36:24] Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors                                                       \n[08:36:26] list index out of range                                handlers.py:36\n[08:36:27] Finished attempting to scrape: CAN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 158, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2025-03-16 08:36:21.005281","end":"2025-03-16 08:36:27.127061","duration":6}},{"council_id":"CAS","missing":false,"latest_run":{"status_code":1,"log_text":"[08:48:10] Fetching Scraper for: CAS                              handlers.py:23\n           Begin attempting to scrape: CAS                        handlers.py:27\n[08:48:11] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[08:48:12] ...data deleted.                                          base.py:264\n           Scraping from https://www.castlepoint.gov.uk/councillors   base.py:49\n[08:48:13] Scraping from                                              base.py:49\n           https://www.castlepoint.gov.ukhttps://castlepoint.cmis.uk.           \n           com/castlepoint/Committees/CurrentCommittees.aspx?a=1                \n           [Errno -2] Name or service not known                   handlers.py:36\n[08:48:14] Finished attempting to scrape: CAS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/CAS-castle-point/councillors.py\", line 15, in get_single_councillor\n    soup = self.get_page(url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [Errno -2] Name or service not known\n","start":"2025-03-16 08:48:10.624967","end":"2025-03-16 08:48:14.074116","duration":3}},{"council_id":"CHE","missing":false,"latest_run":{"status_code":1,"log_text":"[09:51:10] Fetching Scraper for: CHE                              handlers.py:23\n           Begin attempting to scrape: CHE                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[09:51:11] ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://moderngov.cheshireeast.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n[09:51:12] [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1007)                                                        \n           Finished attempting to scrape: CHE                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n","start":"2025-03-16 09:51:10.054993","end":"2025-03-16 09:51:12.268036","duration":2}},{"council_id":"DAC","missing":false,"latest_run":{"status_code":1,"log_text":"[09:26:57] Fetching Scraper for: DAC                              handlers.py:23\n           Begin attempting to scrape: DAC                        handlers.py:27\n[09:26:58] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 51 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n[09:26:59] ...found 51 files in Councillors/raw                      base.py:225\n           ...found 103 files in Councillors                         base.py:225\n           Deleting batch no. 1 consisting of 100 files              base.py:236\n           Deleting batch no. 2 consisting of 3 files                base.py:236\n[09:27:00] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://democracy.dacorum.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[09:27:16] [Errno 110] Connection timed out                       handlers.py:36\n           Finished attempting to scrape: DAC                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: [Errno 110] Connection timed out\n","start":"2025-03-16 09:26:57.593043","end":"2025-03-16 09:27:16.385798","duration":18}},{"council_id":"EAY","missing":false,"latest_run":{"status_code":1,"log_text":"[09:06:10] Fetching Scraper for: EAY                              handlers.py:23\n           Begin attempting to scrape: EAY                        handlers.py:27\n[09:06:11] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[09:06:12] ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.east-ayrshire.gov.uk/CouncilAndGovernment/Abou           \n           t-the-Council/Councillors-and-Provost/YourCouncillor.aspx            \n[09:06:13] Scraping from                                              base.py:49\n           https://www.east-ayrshire.gov.uk/CouncilAndGovernment/Abou           \n           t-the-Council/Councillors-and-Provost/YourCouncillor.aspx?           \n           9                                                                    \n[09:06:14] 'NoneType' object has no attribute 'find_parent'       handlers.py:36\n           Finished attempting to scrape: EAY                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/EAY-east-ayrshire/councillors.py\", line 22, in get_single_councillor\n    .find_parent(\"div\")\nAttributeError: 'NoneType' object has no attribute 'find_parent'\n","start":"2025-03-16 09:06:10.834234","end":"2025-03-16 09:06:14.805118","duration":3}},{"council_id":"EDU","missing":false,"latest_run":{"status_code":1,"log_text":"[09:44:24] Fetching Scraper for: EDU                              handlers.py:23\n           Begin attempting to scrape: EDU                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[09:44:25] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:44:26] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.eastdunbarton.gov.uk/residents/council-democra           \n           cy/committees-and-councillors/councillors-2017                       \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.eastdunbarton.gov.uk/residents/council-de               \n           mocracy/committees-and-councillors/councillors-2017'                 \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n           Finished attempting to scrape: EDU                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 152, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.eastdunbarton.gov.uk/residents/council-democracy/committees-and-councillors/councillors-2017'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-03-16 09:44:24.283261","end":"2025-03-16 09:44:26.647009","duration":2}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[08:37:05] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[08:37:06] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[08:37:07] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://cne-siar.gov.uk/home/your-council/council-members/           \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.cne-siar.gov.uk/home/your-council/council               \n           -members/'                                                           \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n           Finished attempting to scrape: ELS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 152, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.cne-siar.gov.uk/home/your-council/council-members/'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-03-16 08:37:05.332660","end":"2025-03-16 08:37:07.879274","duration":2}},{"council_id":"FAL","missing":false,"latest_run":{"status_code":1,"log_text":"[09:43:18] Fetching Scraper for: FAL                              handlers.py:23\n           Begin attempting to scrape: FAL                        handlers.py:27\n[09:43:19] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:43:20] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.falkirk.gov.uk/services/council-democracy/coun           \n           cillors-decision-making/councillors/                                 \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.falkirk.gov.uk/services/council-democracy               \n           /councillors-decision-making/councillors/'                           \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n           Finished attempting to scrape: FAL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 152, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.falkirk.gov.uk/services/council-democracy/councillors-decision-making/councillors/'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-03-16 09:43:18.606522","end":"2025-03-16 09:43:20.907923","duration":2}},{"council_id":"FYL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:56:19] Fetching Scraper for: FYL                              handlers.py:23\n           Begin attempting to scrape: FYL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[08:56:20] Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 17 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 17 files in Councillors/raw                      base.py:225\n           ...found 35 files in Councillors                          base.py:225\n           Deleting batch no. 1 consisting of 35 files               base.py:236\n[08:56:21] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://fylde.cmis.uk.com/fylde/CouncillorsandMP.aspx                \n[08:56:22] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/48/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:25] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/48/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:26] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/148/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:29] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/148/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:30] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/84/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:35] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/84/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:36] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/211/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:39] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/211/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:40] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/207/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:42] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/207/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:44] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/52/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:47] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/52/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:48] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/208/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:51] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/208/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:56:52] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/43/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:55] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/43/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:56:58] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/172/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:01] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/172/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:02] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/213/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:05] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/213/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:06] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/209/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:09] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/209/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:10] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/15/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:57:13] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/15/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:57:15] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/206/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:17] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/206/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:20] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/174/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:23] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/174/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:24] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/14/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:57:27] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/14/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:57:29] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/186/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:32] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/186/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:33] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/176/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:36] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/176/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:37] Scraping from                                              base.py:49\n           http://fylde.cmis.uk.com/fylde/CouncillorsandMP/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/222/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:57:40] 'NoneType' object has no attribute 'parent'            handlers.py:36\n           Committing batch 1 consisting of 34 files                 base.py:297\n[08:57:41] Finished attempting to scrape: FYL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 300, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"scrapers/FYL-fylde/councillors.py\", line 17, in get_party_name\n    .parent.get_text(strip=True)\nAttributeError: 'NoneType' object has no attribute 'parent'\n","start":"2025-03-16 08:56:19.044276","end":"2025-03-16 08:57:41.444699","duration":82}},{"council_id":"GLG","missing":false,"latest_run":{"status_code":1,"log_text":"[10:20:59] Fetching Scraper for: GLG                              handlers.py:23\n           Begin attempting to scrape: GLG                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[10:21:00] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[10:21:01] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.glasgow.gov.uk/councillorsandcommittees/allMem           \n           bers.asp?sort=0&page=0&rec=100                                       \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://www.glasgow.gov.uk/councillorsandcommittees/a               \n           llMembers.asp?sort=0&page=0&rec=100'                                 \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n           Finished attempting to scrape: GLG                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 152, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://www.glasgow.gov.uk/councillorsandcommittees/allMembers.asp?sort=0&page=0&rec=100'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-03-16 10:20:59.357191","end":"2025-03-16 10:21:01.635180","duration":2}},{"council_id":"GLS","missing":false,"latest_run":{"status_code":1,"log_text":"[10:07:33] Fetching Scraper for: GLS                              handlers.py:23\n           Begin attempting to scrape: GLS                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[10:07:34] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[10:07:35] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://glostext.gloucestershire.gov.uk//mgWebService.asmx/           \n           GetCouncillorsByWard                                                 \n[10:07:51] [Errno 110] Connection timed out                       handlers.py:36\n           Finished attempting to scrape: GLS                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 122, in _connect\n    stream = self._network_backend.connect_tcp(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 205, in connect_tcp\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectTimeout: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectTimeout: [Errno 110] Connection timed out\n","start":"2025-03-16 10:07:33.524026","end":"2025-03-16 10:07:51.558482","duration":18}},{"council_id":"GRE","missing":false,"latest_run":{"status_code":1,"log_text":"[09:43:23] Fetching Scraper for: GRE                              handlers.py:23\n           Begin attempting to scrape: GRE                        handlers.py:27\n[09:43:24] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:43:25] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://committees.royalgreenwich.gov.uk/Councillors/tabid           \n           /63/ScreenMode/Alphabetical/Default.aspx                             \n           Client error '404 Not Found' for url                   handlers.py:36\n           'https://committees.royalgreenwich.gov.uk/mgError.aspx               \n           '                                                                    \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/404                                                               \n           Finished attempting to scrape: GRE                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 272, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'https://committees.royalgreenwich.gov.uk/mgError.aspx'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n","start":"2025-03-16 09:43:23.671153","end":"2025-03-16 09:43:25.967803","duration":2}},{"council_id":"KHL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:50:37] Fetching Scraper for: KHL                              handlers.py:23\n           Begin attempting to scrape: KHL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[08:50:38] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[08:50:39] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://cmis.hullcc.gov.uk/cmis/CouncillorsandSeniorOffice           \n           rs/CouncillorsandSeniorOfficers.aspx                                 \n           [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify    handlers.py:36\n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1007)                                                        \n           Finished attempting to scrape: KHL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 99, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/connection.py\", line 76, in handle_request\n    stream = self._connect(request)\n  File \"/opt/python/httpcore/_sync/connection.py\", line 154, in _connect\n    stream = stream.start_tls(**kwargs)\n  File \"/opt/python/httpcore/_backends/sync.py\", line 152, in start_tls\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 272, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ConnectError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n","start":"2025-03-16 08:50:37.246620","end":"2025-03-16 08:50:39.327077","duration":2}},{"council_id":"MLN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:37:43] Fetching Scraper for: MLN                              handlers.py:23\n           Begin attempting to scrape: MLN                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n[08:37:44] Getting all files in Councillors/json...                  base.py:209\n           ...found 12 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 12 files in Councillors/raw                      base.py:225\n           ...found 25 files in Councillors                          base.py:225\n           Deleting batch no. 1 consisting of 25 files               base.py:236\n[08:37:45] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors.aspx                 \n[08:37:46] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/141/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:37:47] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/244/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:37:49] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/143/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:37:50] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/144/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:37:52] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/245/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:37:53] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/19/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:37:55] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/140/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:37:56] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/240/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:37:58] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/234/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:38:00] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/241/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[08:38:02] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/20/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:38:04] Scraping from                                              base.py:49\n           https://midlothian.cmis.uk.com/live/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/22/ScreenMode/Alphabetical/D           \n           efault.aspx                                                          \n[08:38:05] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 24 files                 base.py:297\n[08:38:07] Finished attempting to scrape: MLN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 300, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 281, in get_party_name\n    list_page_html.find_all(\"img\")[-1][\"title\"]\n  File \"/opt/python/bs4/element.py\", line 1930, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2025-03-16 08:37:43.012241","end":"2025-03-16 08:38:07.078591","duration":24}},{"council_id":"NAY","missing":false,"latest_run":{"status_code":1,"log_text":"[10:30:02] Fetching Scraper for: NAY                              handlers.py:23\n           Begin attempting to scrape: NAY                        handlers.py:27\n[10:30:03] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 33 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n[10:30:04] ...found 33 files in Councillors/raw                      base.py:225\n           ...found 67 files in Councillors                          base.py:225\n           Deleting batch no. 1 consisting of 67 files               base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://north-ayrshire.cmis.uk.com/north-ayrshire/Councill           \n           ors/CurrentCouncillors.aspx                                          \n[10:30:19] Scraping from                                              base.py:49\n           http://north-ayrshire.cmis.uk.com/north-ayrshire/Councillo           \n           rs/CurrentCouncillors/tabid/98/ctl/ViewCMIS_Person/mid/437           \n           /id/323/ScreenMode/Ward/Default.aspx                                 \n[10:30:49] The read operation timed out                           handlers.py:36\n           Finished attempting to scrape: NAY                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 310, in get_single_councillor\n    req = self.get(url)\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2025-03-16 10:30:02.648976","end":"2025-03-16 10:30:49.839344","duration":47}},{"council_id":"NGM","missing":false,"latest_run":{"status_code":1,"log_text":"[09:43:51] Fetching Scraper for: NGM                              handlers.py:23\n           Begin attempting to scrape: NGM                        handlers.py:27\n[09:43:52] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:43:53] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://committee.nottinghamcity.gov.uk/mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n           Server disconnected without sending a response.        handlers.py:36\n[09:43:54] Finished attempting to scrape: NGM                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 238, in _receive_event\n    raise RemoteProtocolError(msg)\nhttpcore.RemoteProtocolError: Server disconnected without sending a response.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.RemoteProtocolError: Server disconnected without sending a response.\n","start":"2025-03-16 09:43:51.880328","end":"2025-03-16 09:43:54.087338","duration":2}},{"council_id":"NLK","missing":false,"latest_run":{"status_code":1,"log_text":"[10:15:45] Fetching Scraper for: NLK                              handlers.py:23\n           Begin attempting to scrape: NLK                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[10:15:46] Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 77 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n[10:15:47] ...found 77 files in Councillors/raw                      base.py:225\n           ...found 155 files in Councillors                         base.py:225\n           Deleting batch no. 1 consisting of 100 files              base.py:236\n           Deleting batch no. 2 consisting of 55 files               base.py:236\n[10:15:48] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://northlanarkshire.cmis.uk.com/Councillors.aspx                \n[10:15:50] Scraping from                                              base.py:49\n           http://northlanarkshire.cmis.uk.com/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/156/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[10:15:53] Scraping from                                              base.py:49\n           http://northlanarkshire.cmis.uk.com/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/161/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[10:15:56] Scraping from                                              base.py:49\n           http://northlanarkshire.cmis.uk.com/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/160/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[10:15:59] Scraping from                                              base.py:49\n           http://northlanarkshire.cmis.uk.com/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/162/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[10:16:03] Scraping from                                              base.py:49\n           http://northlanarkshire.cmis.uk.com/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/163/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[10:16:06] Scraping from                                              base.py:49\n           http://northlanarkshire.cmis.uk.com/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/165/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[10:16:11] Scraping from                                              base.py:49\n           http://northlanarkshire.cmis.uk.com/Councillors/tabid/63/c           \n           tl/ViewCMIS_Person/mid/383/id/164/ScreenMode/Alphabetical/           \n           Default.aspx                                                         \n[10:16:41] The read operation timed out                           handlers.py:36\n           Committing batch 1 consisting of 12 files                 base.py:297\n[10:16:42] Finished attempting to scrape: NLK                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 310, in get_single_councillor\n    req = self.get(url)\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2025-03-16 10:15:45.458673","end":"2025-03-16 10:16:42.639118","duration":57}},{"council_id":"SHE","missing":false,"latest_run":{"status_code":null,"log_text":"[11:28:20] Fetching Scraper for: SHE                              handlers.py:22\n           Begin attempting to scrape: SHE                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in SHE...                               base.py:186\n[11:28:21] Getting all files in SHE/json...                          base.py:186\n           ...found 30 files in SHE/json                             base.py:202\n           Getting all files in SHE/raw...                           base.py:186\n           ...found 30 files in SHE/raw                              base.py:202\n           ...found 61 files in SHE                                  base.py:202\n           Deleting batch no. 1 consisting of 61 files               base.py:211\n[11:28:32] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n           Finished attempting to scrape: SHE                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:28:20.509898","end":"2022-04-04 11:28:32.871624","duration":12}},{"council_id":"TAW","missing":false,"latest_run":{"status_code":1,"log_text":"[09:55:33] Fetching Scraper for: TAW                              handlers.py:23\n           Begin attempting to scrape: TAW                        handlers.py:27\n[09:55:34] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n[09:55:35] Deleting batch no. 1 consisting of 1 files                base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           http://democracy.tamworth.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[09:56:05] The read operation timed out                           handlers.py:36\n[09:56:06] Finished attempting to scrape: TAW                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/httpx/_transports/default.py\", line 69, in map_httpcore_exceptions\n    yield\n  File \"/opt/python/httpx/_transports/default.py\", line 233, in handle_request\n    resp = self._pool.handle_request(req)\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 216, in handle_request\n    raise exc from None\n  File \"/opt/python/httpcore/_sync/connection_pool.py\", line 196, in handle_request\n    response = connection.handle_request(\n  File \"/opt/python/httpcore/_sync/connection.py\", line 101, in handle_request\n    return self._connection.handle_request(request)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 143, in handle_request\n    raise exc\n  File \"/opt/python/httpcore/_sync/http11.py\", line 113, in handle_request\n    ) = self._receive_response_headers(**kwargs)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 186, in _receive_response_headers\n    event = self._receive_event(timeout=timeout)\n  File \"/opt/python/httpcore/_sync/http11.py\", line 224, in _receive_event\n    data = self._network_stream.read(\n  File \"/opt/python/httpcore/_backends/sync.py\", line 124, in read\n    with map_exceptions(exc_map):\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpcore/_exceptions.py\", line 14, in map_exceptions\n    raise to_exc(exc) from exc\nhttpcore.ReadTimeout: The read operation timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 55, in get\n    response = self.http_client.get(url, headers=headers, timeout=30)\n  File \"/opt/python/httpx/_client.py\", line 1054, in get\n    return self.request(\n  File \"/opt/python/httpx/_client.py\", line 827, in request\n    return self.send(request, auth=auth, follow_redirects=follow_redirects)\n  File \"/opt/python/httpx/_client.py\", line 914, in send\n    response = self._send_handling_auth(\n  File \"/opt/python/httpx/_client.py\", line 942, in _send_handling_auth\n    response = self._send_handling_redirects(\n  File \"/opt/python/httpx/_client.py\", line 979, in _send_handling_redirects\n    response = self._send_single_request(request)\n  File \"/opt/python/httpx/_client.py\", line 1015, in _send_single_request\n    response = transport.handle_request(request)\n  File \"/opt/python/httpx/_transports/default.py\", line 232, in handle_request\n    with map_httpcore_exceptions():\n  File \"/var/lang/lib/python3.10/contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"/opt/python/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\n    raise mapped_exc(message) from exc\nhttpx.ReadTimeout: The read operation timed out\n","start":"2025-03-16 09:55:33.841765","end":"2025-03-16 09:56:06.321092","duration":32}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[08:21:40] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n[08:21:41] Deleting existing data...                                 base.py:257\n           Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[08:21:42] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.threerivers.gov.uk/listing/councillors                   \n[08:21:45] 'NoneType' object has no attribute 'findNext'          handlers.py:36\n[08:21:46] Finished attempting to scrape: THE                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"scrapers/THE-three-rivers/councillors.py\", line 15, in get_list_container\n    return soup.find(\"h3\", text=\"District Councillor\").findNext(\"ul\")\nAttributeError: 'NoneType' object has no attribute 'findNext'\n","start":"2025-03-16 08:21:40.879801","end":"2025-03-16 08:21:46.009474","duration":5}},{"council_id":"TOB","missing":false,"latest_run":{"status_code":1,"log_text":"[10:20:53] Fetching Scraper for: TOB                              handlers.py:23\n           Begin attempting to scrape: TOB                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[10:20:54] Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 36 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 36 files in Councillors/raw                      base.py:225\n           ...found 73 files in Councillors                          base.py:225\n           Deleting batch no. 1 consisting of 73 files               base.py:236\n[10:20:55] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.torbay.gov.uk/DemocraticServices//mgWebService           \n           .asmx/GetCouncillorsByWard                                           \n           Server error '503 Service Unavailable' for url         handlers.py:36\n           'https://www.torbay.gov.uk/DemocraticServices//mgWebSe               \n           rvice.asmx/GetCouncillorsByWard'                                     \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/503                                                               \n[10:20:56] Finished attempting to scrape: TOB                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 197, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 216, in get_councillors\n    req = self.get(self.format_councillor_api_url())\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Server error '503 Service Unavailable' for url 'https://www.torbay.gov.uk/DemocraticServices//mgWebService.asmx/GetCouncillorsByWard'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/503\n","start":"2025-03-16 10:20:53.465154","end":"2025-03-16 10:20:56.059538","duration":2}},{"council_id":"VGL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:49:35] Fetching Scraper for: VGL                              handlers.py:23\n           Begin attempting to scrape: VGL                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[08:49:36] Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 16 files in Councillors/json                     base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n           ...found 16 files in Councillors/raw                      base.py:225\n           ...found 33 files in Councillors                          base.py:225\n           Deleting batch no. 1 consisting of 33 files               base.py:236\n[08:49:37] ...data deleted.                                          base.py:264\n           Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Councillors.aspx                               \n[08:49:39] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Asbrey-Anne.aspx                               \n[08:49:40] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Aviet-Julie.aspx                               \n[08:49:42] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Ball-Gareth.aspx                               \n[08:49:43] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Birch-Rhiannon.aspx                            \n[08:49:45] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Brooks-Bronwen.aspx                            \n[08:49:46] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Bruce-Gillian.aspx                             \n[08:49:48] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Buckley-Ian.aspx                               \n[08:49:49] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Burnett-Lis.aspx                               \n[08:49:51] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Campbell-Samantha.aspx                         \n[08:49:52] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Carroll-George.aspx                            \n[08:49:55] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Cave-Christine.aspx                            \n[08:49:56] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Champion-Charles.aspx                          \n[08:49:58] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Charles-Janice.aspx                            \n[08:49:59] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Collins-Amelia.aspx                            \n[08:50:01] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Cowpe-Marianne.aspx                            \n[08:50:02] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Drake-Pamela.aspx                              \n[08:50:04] Scraping from                                              base.py:49\n           https://www.valeofglamorgan.gov.uk/en/our_council/Council-           \n           Structure/councillors/Driscoll-Vincent.aspx                          \n[08:50:05] 'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Committing batch 1 consisting of 32 files                 base.py:297\n[08:50:06] Finished attempting to scrape: VGL                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/VGL-the-vale-of-glamorgan/councillors.py\", line 36, in get_single_councillor\n    email_text = soup.select_one(\"#S4_EmailPlaceholder\").get_text(\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2025-03-16 08:49:35.022329","end":"2025-03-16 08:50:06.977460","duration":31}},{"council_id":"WLN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:23:59] Fetching Scraper for: WLN                              handlers.py:23\n           Begin attempting to scrape: WLN                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[08:24:00] Getting all files in Councillors...                       base.py:209\n           Getting all files in Councillors/json...                  base.py:209\n           ...found 2 files in Councillors/json                      base.py:225\n           Getting all files in Councillors/raw...                   base.py:209\n[08:24:01] ...found 2 files in Councillors/raw                       base.py:225\n           ...found 5 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 5 files                base.py:236\n           ...data deleted.                                          base.py:264\n           Scraping from https://westlothian.gov.uk/councillors       base.py:49\n[08:24:05] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33888/Linlithgow                  \n[08:24:06] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33889/Broxburn-Uphall-a           \n           nd-Winchburgh                                                        \n[08:24:07] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33890/Livingston-North            \n[08:24:08] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33891/Livingston-South            \n[08:24:09] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33893/East-Livingston-a           \n           nd-East-Calder                                                       \n           Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33892/Fauldhouse-and-th           \n           e-Breich-Valley                                                      \n[08:24:10] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33894/Whitburn-and-Blac           \n           kburn                                                                \n[08:24:11] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33895/Bathgate                    \n[08:24:12] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33896/Armadale-and-Blac           \n           kridge                                                               \n           Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33897/Councillor-Tom-Co           \n           nn                                                                   \n[08:24:13] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33898/Councillor-Paulin           \n           e-Orr                                                                \n[08:24:14] Scraping from                                              base.py:49\n           https://westlothian.gov.uk/article/33899/Councillor-Sally-           \n           Pattle                                                               \n[08:24:15] list index out of range                                handlers.py:36\n           Committing batch 1 consisting of 4 files                  base.py:297\n[08:24:17] Finished attempting to scrape: WLN                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 57, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/WLN-west-lothian/councillors.py\", line 37, in get_single_councillor\n    councillor.email = soup.select(\"a[href^=mailto]\")[0].get_text(\nIndexError: list index out of range\n","start":"2025-03-16 08:23:59.441968","end":"2025-03-16 08:24:17.024039","duration":17}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[09:53:12] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n           Deleting existing data...                                 base.py:257\n[09:53:13] Getting all files in Councillors...                       base.py:209\n           ...found 1 files in Councillors                           base.py:225\n           Deleting batch no. 1 consisting of 1 files                base.py:236\n[09:53:14] ...data deleted.                                          base.py:264\n           Scraping from https://www.warrington.gov.uk/councillors    base.py:49\n           Client error '403 Forbidden' for url                   handlers.py:36\n           'https://www.warrington.gov.uk/councillors'                          \n           For more information check:                                          \n           https://developer.mozilla.org/en-US/docs/Web/HTTP/Stat               \n           us/403                                                               \n           Finished attempting to scrape: WRT                        base.py:345\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 55, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 152, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_page\n    page = self.get(url, extra_headers=self.extra_headers).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 56, in get\n    response.raise_for_status()\n  File \"/opt/python/httpx/_models.py\", line 761, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://www.warrington.gov.uk/councillors'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n","start":"2025-03-16 09:53:12.164606","end":"2025-03-16 09:53:14.310484","duration":2}}]
