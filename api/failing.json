[{"council_id":"ABE","missing":false,"latest_run":{"status_code":1,"log_text":"[13:15:09] Fetching Scraper for: ABE                              handlers.py:23\n           Begin attempting to scrape: ABE                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:15:10] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 45 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 45 files in Councillors/raw                      base.py:207\n           ...found 91 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 91 files               base.py:216\n[13:15:11] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://committees.aberdeencity.gov.uk/mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n[13:15:21] HTTPSConnectionPool(host='committees.aberdeencity.gov. handlers.py:36\n           uk', port=443): Max retries exceeded with url:                       \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPSConnectio               \n           n object at 0x7fe7f5797550>: Failed to establish a new               \n           connection: [Errno -2] Name or service not known'))                  \n[13:15:22] Finished attempting to scrape: ABE                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 358, in connect\n    self.sock = conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fe7f5797550>: Failed to establish a new connection: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.aberdeencity.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe7f5797550>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='committees.aberdeencity.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe7f5797550>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n","start":"2023-03-27 13:15:09.247571","end":"2023-03-27 13:15:22.019564","duration":12}},{"council_id":"AYL","missing":false,"latest_run":{"status_code":null,"log_text":"[11:15:36] Fetching Scraper for: AYL                              handlers.py:22\n           Begin attempting to scrape: AYL                        handlers.py:25\n[11:15:37] Deleting existing data...                                 base.py:234\n           Getting all files in AYL...                               base.py:186\n[11:15:38] ...found 1 files in AYL                                   base.py:202\n           Deleting batch no. 1 consisting of 1 files                base.py:211\n[11:15:39] ...data deleted.                                          base.py:241\n           Scraping from https://democracy.aylesburyvaledc.gov.uk//mg base.py:40\n           WebService.asmx/GetCouncillorsByWard                                 \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:34\n           'Connection reset by peer'))                                         \n[11:15:40] Finished attempting to scrape: AYL                        base.py:319\n","errors":"('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))","start":"2022-04-04 11:15:36.995407","end":"2022-04-04 11:15:40.039549","duration":3}},{"council_id":"BAN","missing":false,"latest_run":{"status_code":1,"log_text":"[14:26:47] Fetching Scraper for: BAN                              handlers.py:23\n           Begin attempting to scrape: BAN                        handlers.py:27\n[14:26:48] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:26:49] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://democracy.basingstoke.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n           HTTPSConnectionPool(host='democracy.basingstoke.gov.uk handlers.py:36\n           ', port=443): Max retries exceeded with url:                         \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: BAN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.basingstoke.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.basingstoke.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 14:26:47.697049","end":"2023-03-27 14:26:49.989532","duration":2}},{"council_id":"BDF","missing":false,"latest_run":{"status_code":1,"log_text":"[13:40:15] Fetching Scraper for: BDF                              handlers.py:23\n           Begin attempting to scrape: BDF                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:40:16] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:40:17] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://www.councillorsupport.bedford.gov.uk/mgWebService.a           \n           smx/GetCouncillorsByWard                                             \n           403 Client Error: Forbidden ( The server denied the    handlers.py:36\n           specified Uniform Resource Locator (URL). Contact the                \n           server administrator.  ) for url:                                    \n           http://www.councillorsupport.bedford.gov.uk/mgWebServi               \n           ce.asmx/GetCouncillorsByWard                                         \n           Finished attempting to scrape: BDF                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 403 Client Error: Forbidden ( The server denied the specified Uniform Resource Locator (URL). Contact the server administrator.  ) for url: http://www.councillorsupport.bedford.gov.uk/mgWebService.asmx/GetCouncillorsByWard\n","start":"2023-03-27 13:40:15.492365","end":"2023-03-27 13:40:17.726447","duration":2}},{"council_id":"BDG","missing":false,"latest_run":{"status_code":1,"log_text":"[15:28:42] Fetching Scraper for: BDG                              handlers.py:23\n           Begin attempting to scrape: BDG                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:28:43] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:28:44] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://modgov.lbbd.gov.uk/internet/mgWebService.asmx/GetC           \n           ouncillorsByWard                                                     \n           HTTPSConnectionPool(host='modgov.lbbd.gov.uk',         handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /internet/mgWebService.asmx/GetCouncillorsByWard                     \n           (Caused by SSLError(SSLCertVerificationError(1, '[SSL:               \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: BDG                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='modgov.lbbd.gov.uk', port=443): Max retries exceeded with url: /internet/mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='modgov.lbbd.gov.uk', port=443): Max retries exceeded with url: /internet/mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 15:28:42.392083","end":"2023-03-27 15:28:44.601543","duration":2}},{"council_id":"BKM","missing":false,"latest_run":{"status_code":null,"log_text":"[11:30:01] Fetching Scraper for: BKM                              handlers.py:22\n           Begin attempting to scrape: BKM                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n[11:30:02] Getting all files in BKM...                               base.py:186\n[11:30:03] ...found 1 files in BKM                                   base.py:202\n           Deleting batch no. 1 consisting of 1 files                base.py:211\n[11:30:04] ...data deleted.                                          base.py:241\n           Scraping from https://democracy.buckscc.gov.uk//mgWebServi base.py:40\n           ce.asmx/GetCouncillorsByWard                                         \n           HTTPSConnectionPool(host='democracy.buckscc.gov.uk',   handlers.py:34\n           port=443): Max retries exceeded with url:                            \n           //mgWebService.asmx/GetCouncillorsByWard (Caused by Ne               \n           wConnectionError('<urllib3.connection.HTTPSConnection                \n           object at 0x7f7145a3f250>: Failed to establish a new                 \n           connection: [Errno -2] Name or service not known'))                  \n           Finished attempting to scrape: BKM                        base.py:319\n","errors":"None: Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by None)","start":"2022-04-04 11:30:01.746246","end":"2022-04-04 11:30:04.815110","duration":3}},{"council_id":"BOL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:15:35] Fetching Scraper for: BOL                              handlers.py:23\n           Begin attempting to scrape: BOL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[14:15:36] Getting all files in Councillors/json...                  base.py:191\n           ...found 36 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 36 files in Councillors/raw                      base.py:207\n           ...found 73 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 73 files               base.py:216\n[14:15:37] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People.aspx                \n[14:15:40] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/563/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:44] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/908/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:45] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/562/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:49] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/710/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:51] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/718/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:53] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/856/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:55] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/889/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:56] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/904/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:57] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/879/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:15:59] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/890/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:00] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/911/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:01] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/882/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:04] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/906/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:05] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/891/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:06] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/918/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:08] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/914/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:09] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/878/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:11] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/899/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:12] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/642/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:15] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/707/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:17] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/647/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:20] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/907/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:22] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/585/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:28] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/586/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:32] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/842/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:35] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/884/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:37] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/658/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:39] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/610/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:43] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/885/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:44] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/617/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:48] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/886/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:49] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/903/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:51] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/897/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:53] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/894/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:54] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/691/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:16:56] Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People/tabid/62/           \n           ctl/ViewCMIS_Person/mid/480/id/568/ScreenMode/Ward/Default           \n           .aspx                                                                \n[14:17:00] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 72 files                 base.py:274\n[14:17:02] Finished attempting to scrape: BOL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1519, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2023-03-27 14:15:35.077637","end":"2023-03-27 14:17:02.448962","duration":87}},{"council_id":"CAM","missing":false,"latest_run":{"status_code":1,"log_text":"[13:41:47] Fetching Scraper for: CAM                              handlers.py:23\n           Begin attempting to scrape: CAM                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:41:48] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 60 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 60 files in Councillors/raw                      base.py:207\n           ...found 121 files in Councillors                         base.py:207\n           Deleting batch no. 1 consisting of 100 files              base.py:216\n[13:41:49] Deleting batch no. 2 consisting of 21 files               base.py:216\n[13:41:50] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://cambridgeshire.cmis.uk.com/ccc_live/Councillors.as           \n           px                                                                   \n[13:41:52] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/276/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:41:53] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1534/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:41:54] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/306/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:41:56] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1517/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:41:57] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1532/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:41:58] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1528/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:41:59] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/305/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:01] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/307/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:02] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1518/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:04] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/316/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:06] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/303/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:08] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1543/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:09] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/314/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:10] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/321/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:12] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1539/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:13] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/320/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:14] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1523/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:16] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1525/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:17] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/279/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:19] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1542/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:20] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/322/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:22] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/311/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:24] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/325/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:26] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1536/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:27] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1390/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:29] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1521/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:30] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1535/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:31] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/287/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:33] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1541/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:34] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/299/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:36] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/295/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:37] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1540/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:39] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/313/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:40] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/296/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:43] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1530/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:44] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1533/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:45] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/294/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:46] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1463/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:48] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/315/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:49] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/336/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:51] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1520/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:52] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1526/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:54] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/328/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:42:55] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1531/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:56] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/1522/ScreenMode/Alpha           \n           betical/Default.aspx                                                 \n[13:42:58] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/334/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:43:01] Committing batch 1 consisting of 92 files                 base.py:274\n[13:43:02] Scraping from                                              base.py:42\n           http://cambridgeshire.cmis.uk.com/ccc_live/Councillors/tab           \n           id/63/ctl/ViewCMIS_Person/mid/383/id/324/ScreenMode/Alphab           \n           etical/Default.aspx                                                  \n[13:43:04] 'title'                                                handlers.py:36\n           Committing batch 2 consisting of 2 files                  base.py:274\n[13:43:05] Finished attempting to scrape: CAM                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1519, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2023-03-27 13:41:47.482895","end":"2023-03-27 13:43:05.881123","duration":78}},{"council_id":"CAN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:01:44] Fetching Scraper for: CAN                              handlers.py:23\n           Begin attempting to scrape: CAN                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:01:45] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 41 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 41 files in Councillors/raw                      base.py:207\n           ...found 83 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 83 files               base.py:216\n[15:01:46] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors                                                       \n[15:01:52] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/louis-arduino                                              \n[15:01:54] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/josh-bancroft                                              \n[15:01:57] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/andrea-beach                                               \n[15:01:59] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/anthony-boucker                                            \n[15:02:02] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors/martyn-buttery                                        \n[15:02:04] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors/sheila-cartwright-0                                   \n[15:02:06] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/stuart-crabtree                                            \n[15:02:08] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-us/council           \n           lors/mandy-dunnett                                                   \n[15:02:11] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/jo-elson                                                   \n[15:02:13] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/paul-fisher                                                \n[15:02:15] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/adrienne-fitzgerald                                        \n[15:02:18] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/darren-foley                                               \n[15:02:20] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/charlie-frew                                               \n[15:02:22] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/philippa-haden                                             \n[15:02:24] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/mike-hoare                                                 \n[15:02:27] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/robert-hughes                                              \n[15:02:29] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors/justin-johnson                                        \n[15:02:31] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors/tony-johnson                                          \n[15:02:33] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-us/council           \n           lors/pam-johnson                                                     \n[15:02:36] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/paul-jones                                                 \n[15:02:38] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/val-jones                                                  \n[15:02:40] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/bryan-jones                                                \n[15:02:42] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/phil-jones                                                 \n[15:02:44] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/bill-kenny                                                 \n[15:02:48] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors/john-kraujalis                                        \n[15:02:50] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/peter-kruskonjic                                           \n[15:02:52] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/olivia-lyons                                               \n[15:02:54] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/nick-lyons                                                 \n[15:02:56] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/gerald-molineux                                            \n[15:02:58] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/andrea-muckley                                             \n[15:03:01] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/josh-newbury                                               \n[15:03:03] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/alan-pearson                                               \n[15:03:05] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors/john-preece                                           \n[15:03:07] Scraping from                                              base.py:42\n           https://www.cannockchasedc.gov.uk/council/about-council/co           \n           uncillors/jacquie-prestwood                                          \n[15:03:10] 'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 68 files                 base.py:274\n[15:03:11] Finished attempting to scrape: CAN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/CAN-cannock-chase/councillors.py\", line 19, in get_single_councillor\n    url = urljoin(self.base_url, link[\"href\"])\nTypeError: 'NoneType' object is not subscriptable\n","start":"2023-03-27 15:01:44.570546","end":"2023-03-27 15:03:11.753318","duration":87}},{"council_id":"CAY","missing":false,"latest_run":{"status_code":1,"log_text":"[14:29:48] Fetching Scraper for: CAY                              handlers.py:23\n           Begin attempting to scrape: CAY                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[14:29:49] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://www.democracy.caerphilly.gov.uk/mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n[14:29:50] HTTPSConnectionPool(host='www.democracy.caerphilly.gov handlers.py:36\n           .uk', port=443): Max retries exceeded with url:                      \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(CertificateError(\"hostname                                  \n           'www.democracy.caerphilly.gov.uk' doesn't match either               \n           of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\")))                    \n           Finished attempting to scrape: CAY                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 467, in connect\n    _match_hostname(cert, self.assert_hostname or server_hostname)\n  File \"/opt/python/urllib3/connection.py\", line 540, in _match_hostname\n    match_hostname(cert, asserted_hostname)\n  File \"/opt/python/urllib3/util/ssl_match_hostname.py\", line 150, in match_hostname\n    raise CertificateError(\nurllib3.util.ssl_match_hostname.CertificateError: hostname 'www.democracy.caerphilly.gov.uk' doesn't match either of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.democracy.caerphilly.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(CertificateError(\"hostname 'www.democracy.caerphilly.gov.uk' doesn't match either of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\")))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 723, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 723, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='www.democracy.caerphilly.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(CertificateError(\"hostname 'www.democracy.caerphilly.gov.uk' doesn't match either of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\")))\n","start":"2023-03-27 14:29:48.195115","end":"2023-03-27 14:29:50.295391","duration":2}},{"council_id":"CHN","missing":false,"latest_run":{"status_code":null,"log_text":"[12:03:34] Fetching Scraper for: CHN                              handlers.py:22\n           Begin attempting to scrape: CHN                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in CHN...                               base.py:186\n           ...found 1 files in CHN                                   base.py:202\n           Deleting batch no. 1 consisting of 1 files                base.py:211\n[12:03:35] ...data deleted.                                          base.py:241\n           Scraping from https://isa.chiltern.gov.uk/democracy//mgWeb base.py:40\n           Service.asmx/GetCouncillorsByWard                                    \n[12:03:36] HTTPSConnectionPool(host='isa.chiltern.gov.uk',        handlers.py:34\n           port=443): Max retries exceeded with url:                            \n           /democracy//mgWebService.asmx/GetCouncillorsByWard                   \n           (Caused by SSLError(SSLCertVerificationError(1, '[SSL:               \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: CHN                        base.py:319\n","errors":"None: Max retries exceeded with url: /democracy//mgWebService.asmx/GetCouncillorsByWard (Caused by None)","start":"2022-04-04 12:03:34.242216","end":"2022-04-04 12:03:36.388924","duration":2}},{"council_id":"CMD","missing":false,"latest_run":{"status_code":1,"log_text":"[14:19:24] Fetching Scraper for: CMD                              handlers.py:23\n           Begin attempting to scrape: CMD                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[14:19:25] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:19:26] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.camden.gov.uk/mgWebService.asmx/GetCounci           \n           llorsByWard                                                          \n           HTTPSConnectionPool(host='democracy.camden.gov.uk',    handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: CMD                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.camden.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 723, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 723, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.camden.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 14:19:24.151093","end":"2023-03-27 14:19:26.828459","duration":2}},{"council_id":"COT","missing":false,"latest_run":{"status_code":1,"log_text":"[15:35:56] Fetching Scraper for: COT                              handlers.py:23\n           Begin attempting to scrape: COT                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[15:35:57] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://www.cmis.cotswold.gov.uk/cmis5/People/tabid/62/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[15:35:58] HTTPConnectionPool(host='www.cmis.cotswold.gov.uk',    handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default               \n           .aspx (Caused by                                                     \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fb9d59423a0>: Failed to establish a new                 \n           connection: [Errno -2] Name or service not known'))                  \n           Finished attempting to scrape: COT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb9d59423a0>: Failed to establish a new connection: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb9d59423a0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb9d59423a0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n","start":"2023-03-27 15:35:56.168903","end":"2023-03-27 15:35:58.396005","duration":2}},{"council_id":"DAV","missing":false,"latest_run":{"status_code":null,"log_text":"[11:58:53] Fetching Scraper for: DAV                              handlers.py:22\n           Begin attempting to scrape: DAV                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in DAV...                               base.py:186\n           ...found 1 files in DAV                                   base.py:202\n           Deleting batch no. 1 consisting of 1 files                base.py:211\n[11:59:00] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n[11:59:01] Finished attempting to scrape: DAV                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:58:53.136987","end":"2022-04-04 11:59:01.267076","duration":8}},{"council_id":"DEB","missing":false,"latest_run":{"status_code":1,"log_text":"[13:59:01] Fetching Scraper for: DEB                              handlers.py:23\n           Begin attempting to scrape: DEB                        handlers.py:27\n[13:59:02] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:59:03] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://derbyshiredales.gov.uk/your-council/councillors              \n[13:59:04] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.derbyshiredales.gov.uk/your-council/counci               \n           llors                                                                \n           Finished attempting to scrape: DEB                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.derbyshiredales.gov.uk/your-council/councillors\n","start":"2023-03-27 13:59:01.975156","end":"2023-03-27 13:59:04.283295","duration":2}},{"council_id":"DOR","missing":false,"latest_run":{"status_code":null,"log_text":"[11:39:51] Created log commit                                        base.py:376\n           7d969257019c53df761c79c3b803b440f109f557                             \n           Attempting to create merge commit...                      base.py:281\n[11:39:56] An error occurred (InvalidTargetBranchException) when  handlers.py:34\n           calling the MergeBranchesBySquash operation: Target                  \n           branch parameter must point to either source or                      \n           destination tip commit. Verify your target branch                    \n           value is valid and then try again                                    \n[11:39:58] Finished attempting to scrape: DOR                        base.py:319\n","errors":"An error occurred (InvalidTargetBranchException) when calling the MergeBranchesBySquash operation: Target branch parameter must point to either source or destination tip commit. Verify your target branch value is valid and then try again","start":"2022-04-04 11:39:43.716736","end":"2022-04-04 11:39:58.245766","duration":14}},{"council_id":"EAL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:21:41] Fetching Scraper for: EAL                              handlers.py:23\n           Begin attempting to scrape: EAL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[14:21:42] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://ealing.cmis.uk.com/ealing/Councillors.aspx                    \n[14:21:43] 404 Client Error: Not Found for url:                   handlers.py:36\n           http://ealing.cmis.uk.com/ealing/Councillors.aspx                    \n           Finished attempting to scrape: EAL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://ealing.cmis.uk.com/ealing/Councillors.aspx\n","start":"2023-03-27 14:21:41.130950","end":"2023-03-27 14:21:43.281964","duration":2}},{"council_id":"EAS","missing":false,"latest_run":{"status_code":1,"log_text":"[14:42:53] Fetching Scraper for: EAS                              handlers.py:23\n           Begin attempting to scrape: EAS                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:42:54] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 20 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 20 files in Councillors/raw                      base.py:207\n           ...found 41 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 41 files               base.py:216\n[14:42:55] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://democracy.lewes-eastbourne.gov.uk/mgWebService.asm           \n           x/GetCouncillorsByWard                                               \n[14:42:58] argument of type 'NoneType' is not iterable            handlers.py:36\n           Committing batch 1 consisting of 40 files                 base.py:274\n[14:43:00] Finished attempting to scrape: EAS                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 177, in run\n    councillor = self.get_single_councillor(ward, councillor_xml)\n  File \"scrapers/EAS-eastbourne/councillors.py\", line 14, in get_single_councillor\n    if \"eastbourne.gov.uk\" in email:\nTypeError: argument of type 'NoneType' is not iterable\n","start":"2023-03-27 14:42:53.451647","end":"2023-03-27 14:43:00.179269","duration":6}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[13:15:41] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[13:15:42] Getting all files in Councillors/json...                  base.py:191\n           ...found 29 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 29 files in Councillors/raw                      base.py:207\n           ...found 59 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 59 files               base.py:216\n[13:15:43] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/                                                \n[13:15:46] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/iain-a-macneil/                                 \n[13:15:47] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/kenneth-j-maclean/                              \n[13:15:48] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/iain-m-macleod/                                 \n[13:15:50] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/paul-f-steele/                                  \n[13:15:53] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/susan-thomson/                                  \n[13:15:56] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/mustapha-hocine/                                \n[13:16:09] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/uisdean-robertson/                              \n[13:17:14] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/grant-fulton/                                   \n[13:17:15] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/paul-a-finnegan/                                \n[13:17:16] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/angus-morrison/                                 \n[13:17:17] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/robert-mackenzie/                               \n[13:17:18] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/alasdair-r-fraser/                              \n[13:17:59] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/norman-misty-macdonald/                         \n[13:18:00] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/donald-macsween/                                \n[13:18:16] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/john-n-macleod/                                 \n[13:18:17] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/kenneth-macleod/                                \n[13:18:18] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/calum-maclean/                                  \n[13:18:19] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/donald-crichton/                                \n[13:18:21] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/john-a-maciver/                                 \n[13:18:22] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/duncan-macinnes/                                \n[13:18:23] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/gordon-murray/                                  \n[13:18:24] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/iain-m-macaulay/                                \n[13:18:25] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/malcolm-k-macdonald/                            \n[13:19:16] Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/angus-mccormack/                                \n           500 Server Error: Exception while creating a value.    handlers.py:36\n           for url:                                                             \n           https://www.cne-siar.gov.uk/your-council/wards-and-cou               \n           ncillors/council-members/angus-mccormack/                            \n           Committing batch 1 consisting of 46 files                 base.py:274\n[13:19:17] Finished attempting to scrape: ELS                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ELS-na-h-eileanan-an-iar/councillors.py\", line 17, in get_single_councillor\n    soup = self.get_page(url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 500 Server Error: Exception while creating a value. for url: https://www.cne-siar.gov.uk/your-council/wards-and-councillors/council-members/angus-mccormack/\n","start":"2023-03-27 13:15:41.043460","end":"2023-03-27 13:19:17.891150","duration":216}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[14:03:40] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:03:41] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:03:42] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n[14:03:43] Scraping from                                              base.py:42\n           https://www.eastrenfrewshire.gov.uk/councillor-angela-conv           \n           ery                                                                  \n[14:03:44] 'NoneType' object is not subscriptable                 handlers.py:36\n           Finished attempting to scrape: ERW                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ERW-east-renfrewshire/councillors.py\", line 51, in get_single_councillor\n    contact_url = soup.select_one(\".panel__list--relarticles a.panel__link\")[\"href\"]\nTypeError: 'NoneType' object is not subscriptable\n","start":"2023-03-27 14:03:40.323248","end":"2023-03-27 14:03:44.645865","duration":4}},{"council_id":"ESS","missing":false,"latest_run":{"status_code":1,"log_text":"[15:31:01] Fetching Scraper for: ESS                              handlers.py:23\n           Begin attempting to scrape: ESS                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:31:02] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:31:03] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx               \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx               \n           Finished attempting to scrape: ESS                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx\n","start":"2023-03-27 15:31:01.433768","end":"2023-03-27 15:31:03.537633","duration":2}},{"council_id":"FOR","missing":false,"latest_run":{"status_code":null,"log_text":"[11:18:09] Fetching Scraper for: FOR                              handlers.py:22\n           Begin attempting to scrape: FOR                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n[11:18:10] Getting all files in FOR...                               base.py:186\n           Getting all files in FOR/json...                          base.py:186\n           ...found 64 files in FOR/json                             base.py:202\n           Getting all files in FOR/raw...                           base.py:186\n[11:18:11] ...found 64 files in FOR/raw                              base.py:202\n           ...found 129 files in FOR                                 base.py:202\n           Deleting batch no. 1 consisting of 100 files              base.py:211\n[11:18:17] Deleting batch no. 2 consisting of 29 files               base.py:211\n[11:18:19] ...data deleted.                                          base.py:241\n           Scraping from https://democracy.westsuffolk.gov.uk/mgWebSe base.py:40\n           rvice.asmx/GetCouncillorsByWard                                      \n[11:18:23] Committing batch 1 consisting of 92 files                 base.py:269\n[11:18:26] Committing batch 2 consisting of 36 files                 base.py:269\n[11:18:32] Finished attempting to scrape: FOR                        base.py:319\n","errors":"","start":"2022-04-04 11:18:09.650110","end":"2022-04-04 11:18:32.962955","duration":23}},{"council_id":"GLO","missing":false,"latest_run":{"status_code":1,"log_text":"[17:17:25] Fetching Scraper for: GLO                              handlers.py:23\n           Begin attempting to scrape: GLO                        handlers.py:27\n           Deleting existing data...                                 base.py:228\n[17:17:26] Getting all files in GLO...                               base.py:180\n           ...found 1 files in GLO                                   base.py:196\n           Deleting batch no. 1 consisting of 1 files                base.py:205\n[17:17:27] ...data deleted.                                          base.py:235\n           Scraping from http://democracy.gloucester.gov.uk/mgWebServ base.py:40\n           ice.asmx/GetCouncillorsByWard                                        \n[17:19:37] HTTPConnectionPool(host='democracy.gloucester.gov.uk', handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fd748d57bb0>: Failed to establish a new                 \n           connection: [Errno 110] Connection timed out'))                      \n[17:19:38] Finished attempting to scrape: GLO                        base.py:313\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fd748d57bb0>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='democracy.gloucester.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd748d57bb0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 166, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 183, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 46, in get\n    response = requests.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/api.py\", line 75, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/opt/python/requests/api.py\", line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='democracy.gloucester.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fd748d57bb0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2022-04-21 17:17:25.288674","end":"2022-04-21 17:19:38.148756","duration":132}},{"council_id":"HAO","missing":false,"latest_run":{"status_code":1,"log_text":"[15:06:50] Fetching Scraper for: HAO                              handlers.py:23\n           Begin attempting to scrape: HAO                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:06:51] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:06:52] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://cmis.harborough.gov.uk/cmis5/Councillors.aspx                \n           HTTPSConnectionPool(host='cmis.harborough.gov.uk',     handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /cmis5/Councillors.aspx (Caused by                                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: HAO                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cmis.harborough.gov.uk', port=443): Max retries exceeded with url: /cmis5/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='cmis.harborough.gov.uk', port=443): Max retries exceeded with url: /cmis5/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 15:06:50.348597","end":"2023-03-27 15:06:52.903542","duration":2}},{"council_id":"HAV","missing":false,"latest_run":{"status_code":1,"log_text":"[13:56:26] Fetching Scraper for: HAV                              handlers.py:23\n           Begin attempting to scrape: HAV                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[13:56:27] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.havering.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[13:56:28] HTTPSConnectionPool(host='democracy.havering.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: HAV                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.havering.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 723, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 723, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.havering.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 13:56:26.179071","end":"2023-03-27 13:56:28.373730","duration":2}},{"council_id":"HER","missing":false,"latest_run":{"status_code":1,"log_text":"[15:27:29] Fetching Scraper for: HER                              handlers.py:23\n           Begin attempting to scrape: HER                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:27:30] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www5.hertsmere.gov.uk/democracy//mgWebService.asmx           \n           /GetCouncillorsByWard                                                \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www5.hertsmere.gov.uk/democracy//mgWebService.               \n           asmx/GetCouncillorsByWard                                            \n[15:27:31] Finished attempting to scrape: HER                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www5.hertsmere.gov.uk/democracy//mgWebService.asmx/GetCouncillorsByWard\n","start":"2023-03-27 15:27:29.008897","end":"2023-03-27 15:27:31.129130","duration":2}},{"council_id":"LAC","missing":false,"latest_run":{"status_code":1,"log_text":"[13:46:23] Fetching Scraper for: LAC                              handlers.py:23\n           Begin attempting to scrape: LAC                        handlers.py:27\n[13:46:24] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:46:25] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://committeeadmin.lancaster.gov.uk/mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n           HTTPSConnectionPool(host='committeeadmin.lancaster.gov handlers.py:36\n           .uk', port=443): Max retries exceeded with url:                      \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n[13:46:26] Finished attempting to scrape: LAC                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committeeadmin.lancaster.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 723, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 723, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='committeeadmin.lancaster.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 13:46:23.912629","end":"2023-03-27 13:46:26.190117","duration":2}},{"council_id":"LEE","missing":false,"latest_run":{"status_code":1,"log_text":"[14:04:18] Fetching Scraper for: LEE                              handlers.py:23\n           Begin attempting to scrape: LEE                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[14:04:19] Getting all files in Councillors/json...                  base.py:191\n           ...found 40 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 40 files in Councillors/raw                      base.py:207\n           ...found 81 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 81 files               base.py:216\n[14:04:20] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.lewes-eastbourne.gov.uk//mgWebService.asm           \n           x/GetCouncillorsByWard                                               \n[14:04:23] argument of type 'NoneType' is not iterable            handlers.py:36\n           Committing batch 1 consisting of 80 files                 base.py:274\n[14:04:25] Finished attempting to scrape: LEE                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 177, in run\n    councillor = self.get_single_councillor(ward, councillor_xml)\n  File \"scrapers/LEE-lewes/councillors.py\", line 13, in get_single_councillor\n    if \"lewes.gov.uk\" in email:\nTypeError: argument of type 'NoneType' is not iterable\n","start":"2023-03-27 14:04:18.136459","end":"2023-03-27 14:04:25.286134","duration":7}},{"council_id":"MOL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:09:52] Fetching Scraper for: MOL                              handlers.py:23\n           Begin attempting to scrape: MOL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:09:53] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:09:54] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/home/council/councillors/who           \n           -are-your-councillors                                                \n           HTTPSConnectionPool(host='www.molevalley.gov.uk',      handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /home/council/councillors/who-are-your-councillors                   \n           (Caused by SSLError(SSLCertVerificationError(1, '[SSL:               \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: MOL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.molevalley.gov.uk', port=443): Max retries exceeded with url: /home/council/councillors/who-are-your-councillors (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='www.molevalley.gov.uk', port=443): Max retries exceeded with url: /home/council/councillors/who-are-your-councillors (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 14:09:52.544755","end":"2023-03-27 14:09:54.688119","duration":2}},{"council_id":"NEL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:58:26] Fetching Scraper for: NEL                              handlers.py:23\n           Begin attempting to scrape: NEL                        handlers.py:27\n[14:58:27] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:58:28] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.nelincs.gov.uk/your-council/councillors-mps-an           \n           d-meps/find-your-councillor/councillors-by-party/                    \n[14:58:33] More than one element selected                         handlers.py:36\n           Finished attempting to scrape: NEL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 137, in get_list_container\n    raise ValueError(\"More than one element selected\")\nValueError: More than one element selected\n","start":"2023-03-27 14:58:26.849618","end":"2023-03-27 14:58:33.400392","duration":6}},{"council_id":"NFK","missing":false,"latest_run":{"status_code":1,"log_text":"[13:58:16] Fetching Scraper for: NFK                              handlers.py:23\n           Begin attempting to scrape: NFK                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:58:17] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:58:18] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://norfolkcc.cmis.uk.com/norfolkcc/Councillors.aspx             \n[13:58:20] 'NoneType' object has no attribute 'next'              handlers.py:36\n[13:58:21] Finished attempting to scrape: NFK                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 258, in get_single_councillor\n    division = list_page_html.find(text=self.division_text).next.strip()\nAttributeError: 'NoneType' object has no attribute 'next'\n","start":"2023-03-27 13:58:16.588866","end":"2023-03-27 13:58:21.173542","duration":4}},{"council_id":"NNO","missing":false,"latest_run":{"status_code":1,"log_text":"[15:08:41] Fetching Scraper for: NNO                              handlers.py:23\n           Begin attempting to scrape: NNO                        handlers.py:27\n[15:08:42] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:08:43] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.north-norfolk.gov.uk/members/#filter-form                \n[15:08:45] list index out of range                                handlers.py:36\n           Finished attempting to scrape: NNO                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/NNO-north-norfolk/councillors.py\", line 15, in get_councillors\n    return super().get_councillors()[1:]\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 138, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2023-03-27 15:08:41.936181","end":"2023-03-27 15:08:45.526579","duration":3}},{"council_id":"NTH","missing":false,"latest_run":{"status_code":1,"log_text":"[08:45:02] Fetching Scraper for: NTH                              handlers.py:23\n           Begin attempting to scrape: NTH                        handlers.py:26\n           Deleting existing data...                                 base.py:228\n[08:45:03] Getting all files in NTH...                               base.py:180\n           ...found 1 files in NTH                                   base.py:196\n           Deleting batch no. 1 consisting of 1 files                base.py:205\n[08:45:04] ...data deleted.                                          base.py:235\n           Scraping from https://cmis.northamptonshire.gov.uk/cmis5li base.py:40\n           ve/Councillors.aspx                                                  \n[08:47:15] HTTPSConnectionPool(host='cmis.northamptonshire.gov.uk handlers.py:35\n           ', port=443): Max retries exceeded with url:                         \n           /cmis5live/Councillors.aspx (Caused by NewConnectionEr               \n           ror('<urllib3.connection.HTTPSConnection object at                   \n           0x7fa258adad00>: Failed to establish a new connection:               \n           [Errno 110] Connection timed out'))                                  \n[08:47:16] Finished attempting to scrape: NTH                        base.py:313\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 358, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fa258adad00>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cmis.northamptonshire.gov.uk', port=443): Max retries exceeded with url: /cmis5live/Councillors.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa258adad00>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 31, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 235, in get_councillors\n    req = self.get(self.base_url)\n  File \"/var/task/lgsf/scrapers/base.py\", line 46, in get\n    response = requests.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/api.py\", line 75, in get\n    return request('get', url, params=params, **kwargs)\n  File \"/opt/python/requests/api.py\", line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='cmis.northamptonshire.gov.uk', port=443): Max retries exceeded with url: /cmis5live/Councillors.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fa258adad00>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2022-04-12 08:45:02.766789","end":"2022-04-12 08:47:16.144661","duration":133}},{"council_id":"OAD","missing":false,"latest_run":{"status_code":1,"log_text":"[13:57:47] Fetching Scraper for: OAD                              handlers.py:23\n           Begin attempting to scrape: OAD                        handlers.py:27\n[13:57:48] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:57:49] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.oadby-wigston.gov.uk/mgWebService.asmx/Ge           \n           tCouncillorsByWard                                                   \n[13:57:53] ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: OAD                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 449, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 444, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 449, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 444, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 547, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2023-03-27 13:57:47.905249","end":"2023-03-27 13:57:53.940207","duration":6}},{"council_id":"ORK","missing":false,"latest_run":{"status_code":1,"log_text":"[15:10:12] Fetching Scraper for: ORK                              handlers.py:23\n           Begin attempting to scrape: ORK                        handlers.py:27\n[15:10:13] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:10:14] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.orkney.gov.uk/Council/Councillors/councillor-p           \n           rofiles.htm                                                          \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.orkney.gov.uk/Council/Councillors/councill               \n           or-profiles.htm                                                      \n[15:10:15] Finished attempting to scrape: ORK                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.orkney.gov.uk/Council/Councillors/councillor-profiles.htm\n","start":"2023-03-27 15:10:12.677472","end":"2023-03-27 15:10:15.110763","duration":2}},{"council_id":"PEN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:03:22] Fetching Scraper for: PEN                              handlers.py:23\n           Begin attempting to scrape: PEN                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:03:23] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 15 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n[15:03:24] ...found 15 files in Councillors/raw                      base.py:207\n           ...found 31 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 31 files               base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from https://www.pendle.gov.uk/councillors/name   base.py:42\n[15:03:26] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/76/mohammed_adnan              \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/83/faraz_ahmad                 \n[15:03:27] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/2/nadeem_ahmed                 \n[15:03:28] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/91/sajjad_ahmed                \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/78/david_albin                 \n[15:03:29] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/67/zafar_ali                   \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/88/mohammad_ammer              \n[15:03:30] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/84/ruby_anwar                  \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/8/naeem_hussain_ashr           \n           af                                                                   \n[15:03:31] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/65/mohammad_aslam              \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/12/neil_butterworth            \n[15:03:32] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/13/rosemary_e_carrol           \n           l                                                                    \n[15:03:33] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/85/chris_church                \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/75/david_cockburn-pr           \n           ice                                                                  \n[15:03:34] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/64/sarah_cockburn-pr           \n           ice                                                                  \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/92/david_gallear               \n[15:03:36] 'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 30 files                 base.py:274\n[15:03:37] Finished attempting to scrape: PEN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/PEN-pendle/councillors.py\", line 48, in get_single_councillor\n    councillor.email = soup.select_one(\"li a[href^=mailto]\")[\"href\"].replace(\nTypeError: 'NoneType' object is not subscriptable\n","start":"2023-03-27 15:03:22.668050","end":"2023-03-27 15:03:37.600034","duration":14}},{"council_id":"PLY","missing":false,"latest_run":{"status_code":1,"log_text":"[14:22:40] Fetching Scraper for: PLY                              handlers.py:23\n           Begin attempting to scrape: PLY                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:22:41] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:22:42] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://democracy.plymouth.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           HTTPSConnectionPool(host='democracy.plymouth.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: PLY                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 414, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.plymouth.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 563, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.plymouth.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-03-27 14:22:40.578532","end":"2023-03-27 14:22:42.821671","duration":2}},{"council_id":"RCC","missing":false,"latest_run":{"status_code":1,"log_text":"[14:21:46] Fetching Scraper for: RCC                              handlers.py:23\n           Begin attempting to scrape: RCC                        handlers.py:27\n[14:21:47] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:21:48] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.redcar-cleveland.gov.uk/about-the-council/coun           \n           cillors/_api/web/lists/getbytitle('Councillors%20List')/it           \n           ems?$orderby=Title                                                   \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.redcar-cleveland.gov.uk/about-the-council/               \n           councillors/_api/web/lists/getbytitle('Councillors%20L               \n           ist')/items?$orderby=Title                                           \n[14:21:49] Finished attempting to scrape: RCC                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/RCC-redcar-and-cleveland/councillors.py\", line 10, in get_councillors\n    councillor_list = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.redcar-cleveland.gov.uk/about-the-council/councillors/_api/web/lists/getbytitle('Councillors%20List')/items?$orderby=Title\n","start":"2023-03-27 14:21:46.700319","end":"2023-03-27 14:21:49.285184","duration":2}},{"council_id":"RIH","missing":false,"latest_run":{"status_code":1,"log_text":"[13:45:15] Fetching Scraper for: RIH                              handlers.py:23\n           Begin attempting to scrape: RIH                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:45:16] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:45:17] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.richmondshire.gov.uk/mgWebService.asmx/Ge           \n           tCouncillorsByWard                                                   \n           HTTPConnectionPool(host='democracy.richmondshire.gov.u handlers.py:36\n           k', port=80): Max retries exceeded with url:                         \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fe7f5418970>: Failed to establish a new                 \n           connection: [Errno 111] Connection refused'))                        \n           Finished attempting to scrape: RIH                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe7f5418970>: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='democracy.richmondshire.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe7f5418970>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='democracy.richmondshire.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe7f5418970>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","start":"2023-03-27 13:45:15.638133","end":"2023-03-27 13:45:17.900952","duration":2}},{"council_id":"SED","missing":false,"latest_run":{"status_code":null,"log_text":"[00:02:54] Fetching Scraper for: SED                              handlers.py:22\n           Begin attempting to scrape: SED                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n[00:02:55] Getting all files in SED...                               base.py:186\n           Getting all files in SED/json...                          base.py:186\n[00:02:56] ...found 64 files in SED/json                             base.py:202\n           Getting all files in SED/raw...                           base.py:186\n           ...found 64 files in SED/raw                              base.py:202\n           ...found 129 files in SED                                 base.py:202\n           Deleting batch no. 1 consisting of 100 files              base.py:211\n[00:02:57] Deleting batch no. 2 consisting of 29 files               base.py:211\n[00:02:59] ...data deleted.                                          base.py:241\n           Scraping from https://democracy.westsuffolk.gov.uk/mgWebSe base.py:40\n           rvice.asmx/GetCouncillorsByWard                                      \n[00:03:02] Committing batch 1 consisting of 92 files                 base.py:269\n[00:03:04] Committing batch 2 consisting of 36 files                 base.py:269\n[00:03:05] Finished attempting to scrape: SED                        base.py:319\n","errors":"","start":"2022-04-05 00:02:54.814040","end":"2022-04-05 00:03:05.633472","duration":10}},{"council_id":"SHE","missing":false,"latest_run":{"status_code":null,"log_text":"[11:28:20] Fetching Scraper for: SHE                              handlers.py:22\n           Begin attempting to scrape: SHE                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in SHE...                               base.py:186\n[11:28:21] Getting all files in SHE/json...                          base.py:186\n           ...found 30 files in SHE/json                             base.py:202\n           Getting all files in SHE/raw...                           base.py:186\n           ...found 30 files in SHE/raw                              base.py:202\n           ...found 61 files in SHE                                  base.py:202\n           Deleting batch no. 1 consisting of 61 files               base.py:211\n[11:28:32] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n           Finished attempting to scrape: SHE                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:28:20.509898","end":"2022-04-04 11:28:32.871624","duration":12}},{"council_id":"SHN","missing":false,"latest_run":{"status_code":1,"log_text":"[15:36:15] Fetching Scraper for: SHN                              handlers.py:23\n           Begin attempting to scrape: SHN                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:36:16] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:36:17] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.sthelens.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[15:36:19] HTTPConnectionPool(host='moderngov.sthelens.gov.uk',   handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fb9d57431c0>: Failed to establish a new                 \n           connection: [Errno 113] No route to host'))                          \n           Finished attempting to scrape: SHN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fb9d57431c0>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='moderngov.sthelens.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb9d57431c0>: Failed to establish a new connection: [Errno 113] No route to host'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='moderngov.sthelens.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fb9d57431c0>: Failed to establish a new connection: [Errno 113] No route to host'))\n","start":"2023-03-27 15:36:15.435858","end":"2023-03-27 15:36:19.910302","duration":4}},{"council_id":"SST","missing":false,"latest_run":{"status_code":1,"log_text":"[15:34:30] Fetching Scraper for: SST                              handlers.py:23\n           Begin attempting to scrape: SST                        handlers.py:27\n[15:34:31] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:34:32] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://services.sstaffs.gov.uk/cmis/Councillors.aspx                \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://services.sstaffs.gov.uk/cmis/Councillors.aspx                \n           Finished attempting to scrape: SST                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://services.sstaffs.gov.uk/cmis/Councillors.aspx\n","start":"2023-03-27 15:34:30.818652","end":"2023-03-27 15:34:32.920889","duration":2}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[15:29:23] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:29:24] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:29:25] ...data deleted.                                          base.py:246\n           Scraping from https://www.stirling.gov.uk/councillors      base.py:42\n[15:29:27] list index out of range                                handlers.py:36\n           Finished attempting to scrape: STG                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 138, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2023-03-27 15:29:23.536432","end":"2023-03-27 15:29:27.429820","duration":3}},{"council_id":"STS","missing":false,"latest_run":{"status_code":1,"log_text":"[14:33:38] Fetching Scraper for: STS                              handlers.py:23\n           Begin attempting to scrape: STS                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:33:39] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:33:40] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.staffordshire.gov.uk//mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n[14:35:49] HTTPConnectionPool(host='moderngov.staffordshire.gov.u handlers.py:36\n           k', port=80): Max retries exceeded with url:                         \n           //mgWebService.asmx/GetCouncillorsByWard (Caused by                  \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fe7f5b382e0>: Failed to establish a new                 \n           connection: [Errno 110] Connection timed out'))                      \n[14:35:50] Finished attempting to scrape: STS                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe7f5b382e0>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='moderngov.staffordshire.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe7f5b382e0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='moderngov.staffordshire.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe7f5b382e0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2023-03-27 14:33:38.669014","end":"2023-03-27 14:35:50.322269","duration":131}},{"council_id":"TES","missing":false,"latest_run":{"status_code":1,"log_text":"[15:32:53] Fetching Scraper for: TES                              handlers.py:23\n           Begin attempting to scrape: TES                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:32:54] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:32:55] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepr           \n           esentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#           \n           MemberSectionA                                                       \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://testvalley.cmis.uk.com/testvalleypublic/Elected               \n           Representatives/tabid/63/ScreenMode/Alphabetical/Defau               \n           lt.aspx#MemberSectionA                                               \n           Finished attempting to scrape: TES                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepresentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#MemberSectionA\n","start":"2023-03-27 15:32:53.640865","end":"2023-03-27 15:32:55.743022","duration":2}},{"council_id":"TFW","missing":false,"latest_run":{"status_code":1,"log_text":"[15:10:42] Fetching Scraper for: TFW                              handlers.py:23\n           Begin attempting to scrape: TFW                        handlers.py:27\n[15:10:43] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 54 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n[15:10:44] ...found 54 files in Councillors/raw                      base.py:207\n           ...found 109 files in Councillors                         base.py:207\n           Deleting batch no. 1 consisting of 100 files              base.py:216\n[15:10:45] Deleting batch no. 2 consisting of 9 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.telford.gov.uk/mgWebService.asmx/GetCounc           \n           illorsByWard                                                         \n[15:20:22] Exceeded 30 redirects.                                 handlers.py:36\n           Finished attempting to scrape: TFW                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 723, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 723, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 191, in resolve_redirects\n    raise TooManyRedirects(\nrequests.exceptions.TooManyRedirects: Exceeded 30 redirects.\n","start":"2023-03-27 15:10:42.825878","end":"2023-03-27 15:20:22.458649","duration":579}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[13:58:31] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:58:32] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:58:33] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/listing/councillors                   \n[13:58:34] 'NoneType' object has no attribute 'findNext'          handlers.py:36\n           Finished attempting to scrape: THE                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"scrapers/THE-three-rivers/councillors.py\", line 13, in get_list_container\n    return soup.find(\"h3\", text=\"District Councillor\").findNext(\"ul\")\nAttributeError: 'NoneType' object has no attribute 'findNext'\n","start":"2023-03-27 13:58:31.583209","end":"2023-03-27 13:58:34.612055","duration":3}},{"council_id":"TUN","missing":false,"latest_run":{"status_code":1,"log_text":"[14:30:35] Fetching Scraper for: TUN                              handlers.py:23\n           Begin attempting to scrape: TUN                        handlers.py:27\n[14:30:36] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:30:37] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.tunbridgewells.gov.uk/mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n           HTTPConnectionPool(host='democracy.tunbridgewells.gov. handlers.py:36\n           uk', port=80): Max retries exceeded with url:                        \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fe7f519f460>: Failed to establish a new                 \n           connection: [Errno 111] Connection refused'))                        \n[14:30:38] Finished attempting to scrape: TUN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe7f519f460>: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='democracy.tunbridgewells.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe7f519f460>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='democracy.tunbridgewells.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe7f519f460>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","start":"2023-03-27 14:30:35.887038","end":"2023-03-27 14:30:38.213873","duration":2}},{"council_id":"WAW","missing":false,"latest_run":{"status_code":1,"log_text":"[14:43:54] Fetching Scraper for: WAW                              handlers.py:23\n           Begin attempting to scrape: WAW                        handlers.py:27\n[14:43:55] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:43:56] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://estates4.warwickdc.gov.uk/cmis/CouncillorsAtoZ/tab           \n           id/39/ScreenMode/Ward/Default.aspx                                   \n           HTTPSConnectionPool(host='estates4.warwickdc.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /cmis/CouncillorsAtoZ/tabid/39/ScreenMode/Ward/Default               \n           .aspx (Caused by                                                     \n           NewConnectionError('<urllib3.connection.HTTPSConnectio               \n           n object at 0x7fe7f572b790>: Failed to establish a new               \n           connection: [Errno -5] No address associated with                    \n           hostname'))                                                          \n           Finished attempting to scrape: WAW                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -5] No address associated with hostname\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1042, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 358, in connect\n    self.sock = conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fe7f572b790>: Failed to establish a new connection: [Errno -5] No address associated with hostname\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 489, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 787, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='estates4.warwickdc.gov.uk', port=443): Max retries exceeded with url: /cmis/CouncillorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe7f572b790>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 600, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 587, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 701, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 565, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='estates4.warwickdc.gov.uk', port=443): Max retries exceeded with url: /cmis/CouncillorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe7f572b790>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n","start":"2023-03-27 14:43:54.856159","end":"2023-03-27 14:43:56.952331","duration":2}},{"council_id":"WDO","missing":false,"latest_run":{"status_code":null,"log_text":"[11:51:56] Fetching Scraper for: WDO                              handlers.py:22\n           Begin attempting to scrape: WDO                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in WDO...                               base.py:186\n           WDO Does not exist                                        base.py:206\n           ...no data to delete.                                     base.py:238\n           Scraping from https://moderngovdcp.dorsetforyou.gov.uk/mgW base.py:40\n           ebService.asmx/GetCouncillorsByWard                                  \n           HTTPSConnectionPool(host='moderngovdcp.dorsetforyou.go handlers.py:34\n           v.uk', port=443): Max retries exceeded with url:                     \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by New               \n           ConnectionError('<urllib3.connection.HTTPSConnection                 \n           object at 0x7ff8d06f5370>: Failed to establish a new                 \n           connection: [Errno -2] Name or service not known'))                  \n[11:51:57] Finished attempting to scrape: WDO                        base.py:319\n","errors":"None: Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by None)","start":"2022-04-04 11:51:56.051223","end":"2022-04-04 11:51:57.033088","duration":0}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[15:31:32] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[15:31:33] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:31:34] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.warrington.gov.uk/councillors/name                       \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.warrington.gov.uk/councillors/name                       \n[15:31:35] Finished attempting to scrape: WRT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.warrington.gov.uk/councillors/name\n","start":"2023-03-27 15:31:32.204326","end":"2023-03-27 15:31:35.105801","duration":2}},{"council_id":"WYE","missing":false,"latest_run":{"status_code":1,"log_text":"[13:59:07] Fetching Scraper for: WYE                              handlers.py:23\n           Begin attempting to scrape: WYE                        handlers.py:27\n[13:59:08] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:59:09] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://www.wyreforestdc.gov.uk/the-council/councillors-com           \n           mittees-and-meetings/your-district-councillor.aspx                   \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.wyreforestdc.gov.uk/the-council/councillor               \n           s-committees-and-meetings/your-district-councillor.asp               \n           x                                                                    \n           Finished attempting to scrape: WYE                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.wyreforestdc.gov.uk/the-council/councillors-committees-and-meetings/your-district-councillor.aspx\n","start":"2023-03-27 13:59:07.767780","end":"2023-03-27 13:59:09.995054","duration":2}},{"council_id":"WYO","missing":false,"latest_run":{"status_code":null,"log_text":"[00:08:35] Fetching Scraper for: WYO                              handlers.py:22\n[00:08:36] Begin attempting to scrape: WYO                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in WYO...                               base.py:186\n           ...found 1 files in WYO                                   base.py:202\n           Deleting batch no. 1 consisting of 1 files                base.py:211\n[00:08:37] ...data deleted.                                          base.py:241\n           Scraping from https://councillors.wycombe.gov.uk/mgWebServ base.py:40\n           ice.asmx/GetCouncillorsByWard                                        \n           HTTPSConnectionPool(host='councillors.wycombe.gov.uk', handlers.py:34\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           certificate has expired (_ssl.c:1131)')))                            \n           Finished attempting to scrape: WYO                        base.py:319\n","errors":"None: Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by None)","start":"2022-04-05 00:08:35.972422","end":"2022-04-05 00:08:38.024568","duration":2}}]
