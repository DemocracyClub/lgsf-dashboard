[{"council_id":"AGB","missing":false,"latest_run":{"status_code":1,"log_text":"[12:50:13] Fetching Scraper for: AGB                              handlers.py:23\n           Begin attempting to scrape: AGB                        handlers.py:27\n[12:50:14] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:50:15] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillor_list                       \n[12:50:25] list index out of range                                handlers.py:36\n[12:50:26] Finished attempting to scrape: AGB                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 145, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2023-09-25 12:50:13.936840","end":"2023-09-25 12:50:26.308776","duration":12}},{"council_id":"BRT","missing":false,"latest_run":{"status_code":1,"log_text":"[12:54:16] Fetching Scraper for: BRT                              handlers.py:23\n           Begin attempting to scrape: BRT                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:54:17] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 44 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 44 files in Councillors/raw                      base.py:207\n           ...found 89 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 89 files               base.py:216\n[12:54:18] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.broxtowe.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[12:56:29] HTTPConnectionPool(host='democracy.broxtowe.gov.uk',   handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f8ee7ec79a0>: Failed to establish a new                 \n           connection: [Errno 110] Connection timed out'))                      \n           Finished attempting to scrape: BRT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 415, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 244, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8ee7ec79a0>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='democracy.broxtowe.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ee7ec79a0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='democracy.broxtowe.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ee7ec79a0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2023-09-25 12:54:16.341180","end":"2023-09-25 12:56:29.613874","duration":133}},{"council_id":"BRX","missing":false,"latest_run":{"status_code":1,"log_text":"[14:57:29] Fetching Scraper for: BRX                              handlers.py:23\n           Begin attempting to scrape: BRX                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:57:30] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:57:31] ...data deleted.                                          base.py:246\n           Scraping from https://www.broxbourne.gov.uk/councillors    base.py:42\n[14:57:32] list index out of range                                handlers.py:36\n           Finished attempting to scrape: BRX                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 145, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2023-09-25 14:57:29.642774","end":"2023-09-25 14:57:32.538188","duration":2}},{"council_id":"BRY","missing":false,"latest_run":{"status_code":1,"log_text":"[14:28:12] Fetching Scraper for: BRY                              handlers.py:23\n           Begin attempting to scrape: BRY                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:28:13] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:28:14] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://cds.bromley.gov.uk/mgWebService.asmx/GetCouncillors           \n           ByWard                                                               \n[14:30:25] HTTPConnectionPool(host='cds.bromley.gov.uk',          handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f3962829520>: Failed to establish a new                 \n           connection: [Errno 110] Connection timed out'))                      \n[14:30:26] Finished attempting to scrape: BRY                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 415, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 244, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f3962829520>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='cds.bromley.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3962829520>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='cds.bromley.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f3962829520>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2023-09-25 14:28:12.365353","end":"2023-09-25 14:30:26.889064","duration":134}},{"council_id":"CAY","missing":false,"latest_run":{"status_code":1,"log_text":"[13:26:01] Fetching Scraper for: CAY                              handlers.py:23\n           Begin attempting to scrape: CAY                        handlers.py:27\n[13:26:02] Deleting existing data...                                 base.py:239\n[13:26:03] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:26:04] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://www.democracy.caerphilly.gov.uk/mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n           HTTPSConnectionPool(host='www.democracy.caerphilly.gov handlers.py:36\n           .uk', port=443): Max retries exceeded with url:                      \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(CertificateError(\"hostname                                  \n           'www.democracy.caerphilly.gov.uk' doesn't match either               \n           of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\")))                    \n           Finished attempting to scrape: CAY                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 472, in connect\n    _match_hostname(cert, self.assert_hostname or server_hostname)\n  File \"/opt/python/urllib3/connection.py\", line 545, in _match_hostname\n    match_hostname(cert, asserted_hostname)\n  File \"/opt/python/urllib3/util/ssl_match_hostname.py\", line 150, in match_hostname\n    raise CertificateError(\nurllib3.util.ssl_match_hostname.CertificateError: hostname 'www.democracy.caerphilly.gov.uk' doesn't match either of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.democracy.caerphilly.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(CertificateError(\"hostname 'www.democracy.caerphilly.gov.uk' doesn't match either of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\")))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='www.democracy.caerphilly.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(CertificateError(\"hostname 'www.democracy.caerphilly.gov.uk' doesn't match either of '*.caerphilly.gov.uk', 'caerphilly.gov.uk'\")))\n","start":"2023-09-25 13:26:01.481567","end":"2023-09-25 13:26:04.927346","duration":3}},{"council_id":"CLK","missing":false,"latest_run":{"status_code":1,"log_text":"[14:40:59] Fetching Scraper for: CLK                              handlers.py:23\n           Begin attempting to scrape: CLK                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[14:41:00] Getting all files in Councillors/json...                  base.py:191\n           ...found 18 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 18 files in Councillors/raw                      base.py:207\n           ...found 37 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 37 files               base.py:216\n[14:41:01] ...data deleted.                                          base.py:246\n           Scraping from https://www.clacks.gov.uk/council/wards/     base.py:42\n[14:41:02] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshirece           \n           ntral/9/                                                             \n[14:41:03] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshirece           \n           ntral/2/                                                             \n[14:41:04] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshirece           \n           ntral/12/                                                            \n[14:41:05] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireea           \n           st/3/                                                                \n[14:41:06] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireea           \n           st/16/                                                               \n[14:41:07] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireea           \n           st/18/                                                               \n[14:41:08] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireno           \n           rth/7/                                                               \n[14:41:09] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireno           \n           rth/6/                                                               \n[14:41:10] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireno           \n           rth/4/                                                               \n[14:41:11] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireno           \n           rth/1/                                                               \n[14:41:12] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireso           \n           uth/15/                                                              \n           Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireso           \n           uth/14/                                                              \n[14:41:13] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireso           \n           uth/13/                                                              \n[14:41:14] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshireso           \n           uth/17/                                                              \n[14:41:17] Scraping from                                              base.py:42\n           https://www.clacks.gov.uk/council/wards/clackmannanshirewe           \n           st/5/                                                                \n[14:41:18] 'NoneType' object has no attribute 'find_parent'       handlers.py:36\n           Committing batch 1 consisting of 28 files                 base.py:274\n[14:41:19] Finished attempting to scrape: CLK                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 51, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/CLK-clackmannanshire/councillors.py\", line 24, in get_single_councillor\n    soup.find(\"strong\", text=re.compile(\"Ward:\"))\nAttributeError: 'NoneType' object has no attribute 'find_parent'\n","start":"2023-09-25 14:40:59.068282","end":"2023-09-25 14:41:19.776691","duration":20}},{"council_id":"CMD","missing":false,"latest_run":{"status_code":1,"log_text":"[14:17:40] Fetching Scraper for: CMD                              handlers.py:23\n           Begin attempting to scrape: CMD                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:17:41] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:17:42] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.camden.gov.uk/mgWebService.asmx/GetCounci           \n           llorsByWard                                                          \n           HTTPSConnectionPool(host='democracy.camden.gov.uk',    handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: CMD                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.camden.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.camden.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-09-25 14:17:40.342192","end":"2023-09-25 14:17:42.594407","duration":2}},{"council_id":"COT","missing":false,"latest_run":{"status_code":1,"log_text":"[12:07:32] Fetching Scraper for: COT                              handlers.py:23\n           Begin attempting to scrape: COT                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:07:33] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:07:34] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://www.cmis.cotswold.gov.uk/cmis5/People/tabid/62/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n           HTTPConnectionPool(host='www.cmis.cotswold.gov.uk',    handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default               \n           .aspx (Caused by                                                     \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f8ee7f9c910>: Failed to establish a new                 \n           connection: [Errno -2] Name or service not known'))                  \n           Finished attempting to scrape: COT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 415, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 244, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8ee7f9c910>: Failed to establish a new connection: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ee7f9c910>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 248, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ee7f9c910>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n","start":"2023-09-25 12:07:32.394725","end":"2023-09-25 12:07:34.542648","duration":2}},{"council_id":"CWY","missing":false,"latest_run":{"status_code":1,"log_text":"[15:00:47] Fetching Scraper for: CWY                              handlers.py:23\n           Begin attempting to scrape: CWY                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[15:00:48] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[15:00:49] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://modgoveng.conwy.gov.uk/mgWebService.asmx/GetCouncil           \n           lorsByWard                                                           \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://modgoveng.conwy.gov.uk/mgWebService.asmx/GetCou               \n           ncillorsByWard                                                       \n           Finished attempting to scrape: CWY                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://modgoveng.conwy.gov.uk/mgWebService.asmx/GetCouncillorsByWard\n","start":"2023-09-25 15:00:47.626095","end":"2023-09-25 15:00:49.679496","duration":2}},{"council_id":"EAL","missing":false,"latest_run":{"status_code":1,"log_text":"[13:54:31] Fetching Scraper for: EAL                              handlers.py:23\n           Begin attempting to scrape: EAL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:54:32] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:54:33] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://ealing.cmis.uk.com/ealing/Councillors.aspx                    \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://ealing.cmis.uk.com/ealing/Councillors.aspx                    \n           Finished attempting to scrape: EAL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 248, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://ealing.cmis.uk.com/ealing/Councillors.aspx\n","start":"2023-09-25 13:54:31.595847","end":"2023-09-25 13:54:33.923639","duration":2}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[14:37:46] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:37:47] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:37:48] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n[14:37:49] Scraping from                                              base.py:42\n           https://www.eastrenfrewshire.gov.uk/councillor-angela-conv           \n           ery                                                                  \n[14:37:50] 'NoneType' object is not subscriptable                 handlers.py:36\n           Finished attempting to scrape: ERW                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 51, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ERW-east-renfrewshire/councillors.py\", line 50, in get_single_councillor\n    contact_url = soup.select_one(\".panel__list--relarticles a.panel__link\")[\"href\"]\nTypeError: 'NoneType' object is not subscriptable\n","start":"2023-09-25 14:37:46.388435","end":"2023-09-25 14:37:50.908168","duration":4}},{"council_id":"FYL","missing":false,"latest_run":{"status_code":1,"log_text":"[12:06:28] Fetching Scraper for: FYL                              handlers.py:23\n           Begin attempting to scrape: FYL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:06:29] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://fylde.cmis.uk.com/fylde/CouncillorsandMP.aspx                \n[12:06:31] 'title'                                                handlers.py:36\n           Finished attempting to scrape: FYL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 51, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 267, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 253, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1573, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2023-09-25 12:06:28.256471","end":"2023-09-25 12:06:31.381089","duration":3}},{"council_id":"HAO","missing":false,"latest_run":{"status_code":1,"log_text":"[13:05:15] Fetching Scraper for: HAO                              handlers.py:23\n           Begin attempting to scrape: HAO                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:05:16] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:05:17] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://cmis.harborough.gov.uk/cmis5/Councillors.aspx                \n           HTTPSConnectionPool(host='cmis.harborough.gov.uk',     handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /cmis5/Councillors.aspx (Caused by                                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n[13:05:18] Finished attempting to scrape: HAO                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cmis.harborough.gov.uk', port=443): Max retries exceeded with url: /cmis5/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 248, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='cmis.harborough.gov.uk', port=443): Max retries exceeded with url: /cmis5/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-09-25 13:05:15.678175","end":"2023-09-25 13:05:18.088345","duration":2}},{"council_id":"HER","missing":false,"latest_run":{"status_code":1,"log_text":"[13:21:15] Fetching Scraper for: HER                              handlers.py:23\n           Begin attempting to scrape: HER                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[13:21:16] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www5.hertsmere.gov.uk/democracy//mgWebService.asmx           \n           /GetCouncillorsByWard                                                \n[13:21:17] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://www5.hertsmere.gov.uk/democracy//mgWebService.               \n           asmx/GetCouncillorsByWard                                            \n           Finished attempting to scrape: HER                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www5.hertsmere.gov.uk/democracy//mgWebService.asmx/GetCouncillorsByWard\n","start":"2023-09-25 13:21:15.002164","end":"2023-09-25 13:21:17.414903","duration":2}},{"council_id":"HMF","missing":false,"latest_run":{"status_code":1,"log_text":"[12:27:03] Fetching Scraper for: HMF                              handlers.py:23\n           Begin attempting to scrape: HMF                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:27:04] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:27:05] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.lbhf.gov.uk/mgWebService.asmx/GetCouncill           \n           orsByWard                                                            \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://democracy.lbhf.gov.uk/mgWebService.asmx/GetCoun               \n           cillorsByWard                                                        \n           Finished attempting to scrape: HMF                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://democracy.lbhf.gov.uk/mgWebService.asmx/GetCouncillorsByWard\n","start":"2023-09-25 12:27:03.308132","end":"2023-09-25 12:27:05.558427","duration":2}},{"council_id":"HNS","missing":false,"latest_run":{"status_code":1,"log_text":"[13:35:48] Fetching Scraper for: HNS                              handlers.py:23\n           Begin attempting to scrape: HNS                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:35:49] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:35:50] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://democraticservices.hounslow.gov.uk/mgWebService.as           \n           mx/GetCouncillorsByWard                                              \n           HTTPSConnectionPool(host='democraticservices.hounslow. handlers.py:36\n           gov.uk', port=443): Max retries exceeded with url:                   \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: HNS                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democraticservices.hounslow.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democraticservices.hounslow.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-09-25 13:35:48.298205","end":"2023-09-25 13:35:50.520337","duration":2}},{"council_id":"HUN","missing":false,"latest_run":{"status_code":1,"log_text":"[12:14:34] Fetching Scraper for: HUN                              handlers.py:23\n           Begin attempting to scrape: HUN                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:14:35] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:14:36] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://applications.huntingdonshire.gov.uk/moderngov//mgWe           \n           bService.asmx/GetCouncillorsByWard                                   \n           500 Server Error: Internal Server Error for url:       handlers.py:36\n           http://applications.huntingdonshire.gov.uk/moderngov//               \n           mgWebService.asmx/GetCouncillorsByWard                               \n           Finished attempting to scrape: HUN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 500 Server Error: Internal Server Error for url: http://applications.huntingdonshire.gov.uk/moderngov//mgWebService.asmx/GetCouncillorsByWard\n","start":"2023-09-25 12:14:34.351283","end":"2023-09-25 12:14:36.627916","duration":2}},{"council_id":"KEC","missing":false,"latest_run":{"status_code":1,"log_text":"[12:52:10] Fetching Scraper for: KEC                              handlers.py:23\n           Begin attempting to scrape: KEC                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:52:11] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:52:12] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.rbkc.gov.uk/committees/Councillors.aspx                  \n[12:52:16] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://rbkc.moderngov.co.uk/Committees/mgError.aspx                 \n           Finished attempting to scrape: KEC                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 248, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://rbkc.moderngov.co.uk/Committees/mgError.aspx\n","start":"2023-09-25 12:52:10.223701","end":"2023-09-25 12:52:16.729765","duration":6}},{"council_id":"LEE","missing":false,"latest_run":{"status_code":1,"log_text":"[14:58:46] Fetching Scraper for: LEE                              handlers.py:23\n           Begin attempting to scrape: LEE                        handlers.py:27\n[14:58:47] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[14:58:48] Getting all files in Councillors/json...                  base.py:191\n           ...found 40 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 40 files in Councillors/raw                      base.py:207\n           ...found 81 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 81 files               base.py:216\n[14:58:49] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.lewes-eastbourne.gov.uk//mgWebService.asm           \n           x/GetCouncillorsByWard                                               \n[14:58:52] argument of type 'NoneType' is not iterable            handlers.py:36\n[14:58:53] Committing batch 1 consisting of 80 files                 base.py:274\n[14:58:54] Finished attempting to scrape: LEE                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 183, in run\n    councillor = self.get_single_councillor(ward, councillor_xml)\n  File \"scrapers/LEE-lewes/councillors.py\", line 13, in get_single_councillor\n    if \"lewes.gov.uk\" in email:\nTypeError: argument of type 'NoneType' is not iterable\n","start":"2023-09-25 14:58:46.008830","end":"2023-09-25 14:58:54.240076","duration":8}},{"council_id":"MIK","missing":false,"latest_run":{"status_code":1,"log_text":"[14:42:31] Fetching Scraper for: MIK                              handlers.py:23\n           Begin attempting to scrape: MIK                        handlers.py:27\n[14:42:32] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:42:33] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://milton-keynes.cmis.uk.com/milton-keynes/Councillors           \n           .aspx                                                                \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://milton-keynes.cmis.uk.com/milton-keynes/Council               \n           lors.aspx                                                            \n[14:42:34] Finished attempting to scrape: MIK                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 248, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://milton-keynes.cmis.uk.com/milton-keynes/Councillors.aspx\n","start":"2023-09-25 14:42:31.925045","end":"2023-09-25 14:42:34.025304","duration":2}},{"council_id":"MOL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:42:05] Fetching Scraper for: MOL                              handlers.py:23\n           Begin attempting to scrape: MOL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:42:06] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:42:07] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/home/council/councillors/who           \n           -are-your-councillors                                                \n           HTTPSConnectionPool(host='www.molevalley.gov.uk',      handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /home/council/councillors/who-are-your-councillors                   \n           (Caused by SSLError(SSLCertVerificationError(1, '[SSL:               \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: MOL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.molevalley.gov.uk', port=443): Max retries exceeded with url: /home/council/councillors/who-are-your-councillors (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 130, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='www.molevalley.gov.uk', port=443): Max retries exceeded with url: /home/council/councillors/who-are-your-councillors (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-09-25 14:42:05.383280","end":"2023-09-25 14:42:07.476489","duration":2}},{"council_id":"NEL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:27:39] Fetching Scraper for: NEL                              handlers.py:23\n           Begin attempting to scrape: NEL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:27:40] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:27:41] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.nelincs.gov.uk/your-council/councillors-mps-an           \n           d-meps/find-your-councillor/councillors-by-party/                    \n[14:27:45] More than one element selected                         handlers.py:36\n           Finished attempting to scrape: NEL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 144, in get_list_container\n    raise ValueError(\"More than one element selected\")\nValueError: More than one element selected\n","start":"2023-09-25 14:27:39.516972","end":"2023-09-25 14:27:45.440226","duration":5}},{"council_id":"NNO","missing":false,"latest_run":{"status_code":1,"log_text":"[14:59:35] Fetching Scraper for: NNO                              handlers.py:23\n           Begin attempting to scrape: NNO                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:59:36] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:59:37] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.north-norfolk.gov.uk/members/#filter-form                \n[14:59:39] list index out of range                                handlers.py:36\n           Finished attempting to scrape: NNO                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/NNO-north-norfolk/councillors.py\", line 15, in get_councillors\n    return super().get_councillors()[1:]\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 145, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2023-09-25 14:59:35.503827","end":"2023-09-25 14:59:39.954183","duration":4}},{"council_id":"OAD","missing":false,"latest_run":{"status_code":1,"log_text":"[14:50:57] Fetching Scraper for: OAD                              handlers.py:23\n           Begin attempting to scrape: OAD                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:50:58] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:50:59] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.oadby-wigston.gov.uk/mgWebService.asmx/Ge           \n           tCouncillorsByWard                                                   \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: OAD                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 466, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 461, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 466, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 461, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2023-09-25 14:50:57.337358","end":"2023-09-25 14:50:59.740879","duration":2}},{"council_id":"ORK","missing":false,"latest_run":{"status_code":1,"log_text":"[12:27:13] Fetching Scraper for: ORK                              handlers.py:23\n           Begin attempting to scrape: ORK                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:27:14] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:27:15] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.orkney.gov.uk/Council/Councillors/councillor-p           \n           rofiles.htm                                                          \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.orkney.gov.uk/Council/Councillors/councill               \n           or-profiles.htm                                                      \n           Finished attempting to scrape: ORK                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 130, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.orkney.gov.uk/Council/Councillors/councillor-profiles.htm\n","start":"2023-09-25 12:27:13.254441","end":"2023-09-25 12:27:15.730413","duration":2}},{"council_id":"PEN","missing":false,"latest_run":{"status_code":1,"log_text":"[12:05:20] Fetching Scraper for: PEN                              handlers.py:23\n           Begin attempting to scrape: PEN                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:05:21] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 15 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n[12:05:22] ...found 15 files in Councillors/raw                      base.py:207\n           ...found 31 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 31 files               base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from https://www.pendle.gov.uk/councillors/name   base.py:42\n[12:05:24] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/76/mohammed_adnan              \n[12:05:25] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/83/faraz_ahmad                 \n[12:05:26] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/2/nadeem_ahmed                 \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/91/sajjad_ahmed                \n[12:05:27] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/78/david_albin                 \n[12:05:28] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/67/zafar_ali                   \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/88/mohammad_ammer              \n[12:05:29] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/84/ruby_anwar                  \n[12:05:30] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/8/naeem_hussain_ashr           \n           af                                                                   \n[12:05:31] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/94/mohammad_aslam              \n[12:05:32] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/12/neil_butterworth            \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/13/rosemary_e_carrol           \n           l                                                                    \n[12:05:33] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/85/chris_church                \n[12:05:34] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/75/david_cockburn-pr           \n           ice                                                                  \n[12:05:35] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/64/sarah_cockburn-pr           \n           ice                                                                  \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/92/david_gallear               \n[12:05:36] 'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 30 files                 base.py:274\n[12:05:38] Finished attempting to scrape: PEN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 51, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/PEN-pendle/councillors.py\", line 48, in get_single_councillor\n    councillor.email = soup.select_one(\"li a[href^=mailto]\")[\"href\"].replace(\nTypeError: 'NoneType' object is not subscriptable\n","start":"2023-09-25 12:05:20.661666","end":"2023-09-25 12:05:38.008009","duration":17}},{"council_id":"SFT","missing":false,"latest_run":{"status_code":1,"log_text":"[12:32:53] Fetching Scraper for: SFT                              handlers.py:23\n           Begin attempting to scrape: SFT                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:32:54] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:32:55] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://modgov.sefton.gov.uk/mgWebService.asmx/GetCouncillo           \n           rsByWard                                                             \n           HTTPSConnectionPool(host='modgov.sefton.gov.uk',       handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: SFT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='modgov.sefton.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='modgov.sefton.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-09-25 12:32:53.567113","end":"2023-09-25 12:32:55.818813","duration":2}},{"council_id":"SHE","missing":false,"latest_run":{"status_code":null,"log_text":"[11:28:20] Fetching Scraper for: SHE                              handlers.py:22\n           Begin attempting to scrape: SHE                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in SHE...                               base.py:186\n[11:28:21] Getting all files in SHE/json...                          base.py:186\n           ...found 30 files in SHE/json                             base.py:202\n           Getting all files in SHE/raw...                           base.py:186\n           ...found 30 files in SHE/raw                              base.py:202\n           ...found 61 files in SHE                                  base.py:202\n           Deleting batch no. 1 consisting of 61 files               base.py:211\n[11:28:32] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n           Finished attempting to scrape: SHE                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:28:20.509898","end":"2022-04-04 11:28:32.871624","duration":12}},{"council_id":"SHN","missing":false,"latest_run":{"status_code":1,"log_text":"[12:12:57] Fetching Scraper for: SHN                              handlers.py:23\n           Begin attempting to scrape: SHN                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n[12:12:58] Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.sthelens.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[12:13:02] HTTPConnectionPool(host='moderngov.sthelens.gov.uk',   handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f8ee7ab4340>: Failed to establish a new                 \n           connection: [Errno 113] No route to host'))                          \n           Finished attempting to scrape: SHN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 415, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 244, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f8ee7ab4340>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='moderngov.sthelens.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ee7ab4340>: Failed to establish a new connection: [Errno 113] No route to host'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='moderngov.sthelens.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f8ee7ab4340>: Failed to establish a new connection: [Errno 113] No route to host'))\n","start":"2023-09-25 12:12:57.004637","end":"2023-09-25 12:13:02.235802","duration":5}},{"council_id":"SNO","missing":false,"latest_run":{"status_code":1,"log_text":"[12:11:13] Fetching Scraper for: SNO                              handlers.py:23\n           Begin attempting to scrape: SNO                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[12:11:14] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:11:15] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.southnorfolkandbroadland.gov.uk/directory/3/so           \n           uth-norfolk-councillor-directory/category/11                         \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.southnorfolkandbroadland.gov.uk/directory/               \n           3/south-norfolk-councillor-directory/category/11                     \n           Finished attempting to scrape: SNO                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 130, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.southnorfolkandbroadland.gov.uk/directory/3/south-norfolk-councillor-directory/category/11\n","start":"2023-09-25 12:11:13.200642","end":"2023-09-25 12:11:15.611873","duration":2}},{"council_id":"SST","missing":false,"latest_run":{"status_code":1,"log_text":"[13:00:31] Fetching Scraper for: SST                              handlers.py:23\n           Begin attempting to scrape: SST                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:00:32] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:00:33] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://services.sstaffs.gov.uk/cmis/Councillors.aspx                \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://services.sstaffs.gov.uk/cmis/Councillors.aspx                \n           Finished attempting to scrape: SST                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 248, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://services.sstaffs.gov.uk/cmis/Councillors.aspx\n","start":"2023-09-25 13:00:31.507706","end":"2023-09-25 13:00:33.672087","duration":2}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[13:23:24] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:23:25] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:23:26] ...data deleted.                                          base.py:246\n           Scraping from https://www.stirling.gov.uk/councillors      base.py:42\n[13:23:29] list index out of range                                handlers.py:36\n           Finished attempting to scrape: STG                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 145, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2023-09-25 13:23:24.243714","end":"2023-09-25 13:23:29.444233","duration":5}},{"council_id":"TES","missing":false,"latest_run":{"status_code":1,"log_text":"[14:26:04] Fetching Scraper for: TES                              handlers.py:23\n           Begin attempting to scrape: TES                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[14:26:05] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[14:26:06] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepr           \n           esentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#           \n           MemberSectionA                                                       \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://testvalley.cmis.uk.com/testvalleypublic/Elected               \n           Representatives/tabid/63/ScreenMode/Alphabetical/Defau               \n           lt.aspx#MemberSectionA                                               \n           Finished attempting to scrape: TES                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 248, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepresentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#MemberSectionA\n","start":"2023-09-25 14:26:04.438208","end":"2023-09-25 14:26:06.518189","duration":2}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[12:57:46] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n[12:57:47] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[12:57:48] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/listing/councillors                   \n[12:57:50] 'NoneType' object has no attribute 'findNext'          handlers.py:36\n           Finished attempting to scrape: THE                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"scrapers/THE-three-rivers/councillors.py\", line 13, in get_list_container\n    return soup.find(\"h3\", text=\"District Councillor\").findNext(\"ul\")\nAttributeError: 'NoneType' object has no attribute 'findNext'\n","start":"2023-09-25 12:57:46.930546","end":"2023-09-25 12:57:50.577072","duration":3}},{"council_id":"TWH","missing":false,"latest_run":{"status_code":1,"log_text":"[13:36:08] Fetching Scraper for: TWH                              handlers.py:23\n           Begin attempting to scrape: TWH                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[13:36:09] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:36:10] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://democracy.towerhamlets.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n           HTTPSConnectionPool(host='democracy.towerhamlets.gov.u handlers.py:36\n           k', port=443): Max retries exceeded with url:                        \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n[13:36:11] Finished attempting to scrape: TWH                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 714, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 403, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1053, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 798, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.towerhamlets.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 179, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 196, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.towerhamlets.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-09-25 13:36:08.662375","end":"2023-09-25 13:36:11.166046","duration":2}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[13:35:38] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n[13:35:39] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[13:35:40] ...data deleted.                                          base.py:246\n           Scraping from https://www.warrington.gov.uk/councillors    base.py:42\n[13:35:43] More than one element selected                         handlers.py:36\n           Finished attempting to scrape: WRT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 49, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 144, in get_list_container\n    raise ValueError(\"More than one element selected\")\nValueError: More than one element selected\n","start":"2023-09-25 13:35:38.726049","end":"2023-09-25 13:35:43.202602","duration":4}}]
