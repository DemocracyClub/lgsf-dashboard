[{"council_id":"CAN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:56:27] Fetching Scraper for: CAN                              handlers.py:23\n           Begin attempting to scrape: CAN                        handlers.py:27\n[08:56:28] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:56:29] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors                                                       \n[08:56:32] list index out of range                                handlers.py:36\n           Finished attempting to scrape: CAN                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2024-01-06 08:56:27.609672","end":"2024-01-06 08:56:32.658554","duration":5}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[10:08:57] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[10:08:58] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[10:08:59] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://cne-siar.gov.uk/home/your-council/council-members/           \n[10:09:01] Scraping from https://cne-siar.gov.uk/kenneth-j-maclean/   base.py:41\n[10:09:03] 'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Finished attempting to scrape: ELS                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ELS-na-h-eileanan-an-iar/councillors.py\", line 25, in get_single_councillor\n    .get_text(strip=True)\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2024-01-06 10:08:57.490987","end":"2024-01-06 10:09:03.722790","duration":6}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[09:43:56] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[09:43:57] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:43:58] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: ERW                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 09:43:56.264287","end":"2024-01-06 09:43:58.530547","duration":2}},{"council_id":"FEN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:30:07] Fetching Scraper for: FEN                              handlers.py:23\n           Begin attempting to scrape: FEN                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:30:08] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:30:09] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.fenland.gov.uk/localgov//mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: FEN                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 08:30:07.508816","end":"2024-01-06 08:30:09.681015","duration":2}},{"council_id":"GRE","missing":false,"latest_run":{"status_code":1,"log_text":"[08:51:16] Fetching Scraper for: GRE                              handlers.py:23\n           Begin attempting to scrape: GRE                        handlers.py:27\n[08:51:17] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:51:18] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://committees.royalgreenwich.gov.uk/Councillors/tabid           \n           /63/ScreenMode/Alphabetical/Default.aspx                             \n           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36\n           v.uk', port=443): Max retries exceeded with url:                     \n           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               \n           aspx (Caused by SSLError(SSLCertVerificationError(1,                 \n           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 \n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1007)')))                                                    \n           Finished attempting to scrape: GRE                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 277, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n","start":"2024-01-06 08:51:16.842780","end":"2024-01-06 08:51:18.960538","duration":2}},{"council_id":"GRT","missing":false,"latest_run":{"status_code":1,"log_text":"[09:42:39] Fetching Scraper for: GRT                              handlers.py:23\n           Begin attempting to scrape: GRT                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:42:40] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:42:41] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www2.guildford.gov.uk/councilmeetings/mgWebService           \n           .asmx/GetCouncillorsByWard                                           \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: GRT                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 09:42:39.025762","end":"2024-01-06 09:42:41.630847","duration":2}},{"council_id":"LEC","missing":false,"latest_run":{"status_code":1,"log_text":"[09:09:30] Fetching Scraper for: LEC                              handlers.py:23\n           Begin attempting to scrape: LEC                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:09:31] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://politics.leics.gov.uk//mgWebService.asmx/GetCouncil           \n           lorsByWard                                                           \n[09:09:32] HTTPConnectionPool(host='politics.leics.gov.uk',       handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           //mgWebService.asmx/GetCouncillorsByWard (Caused by                  \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fe8a1d11b40>: Failed to establish a new                 \n           connection: [Errno 111] Connection refused'))                        \n           Finished attempting to scrape: LEC                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 497, in _make_request\n    conn.request(\n  File \"/opt/python/urllib3/connection.py\", line 395, in request\n    self.endheaders()\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1278, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1038, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 976, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 243, in connect\n    self.sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 218, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fe8a1d11b40>: Failed to establish a new connection: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='politics.leics.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe8a1d11b40>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='politics.leics.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fe8a1d11b40>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","start":"2024-01-06 09:09:30.163700","end":"2024-01-06 09:09:32.207745","duration":2}},{"council_id":"MAV","missing":false,"latest_run":{"status_code":1,"log_text":"[09:40:33] Fetching Scraper for: MAV                              handlers.py:23\n           Begin attempting to scrape: MAV                        handlers.py:27\n[09:40:34] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:40:35] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://moderngov.malvernhills.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n[09:40:36] Finished attempting to scrape: MAV                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 09:40:33.819812","end":"2024-01-06 09:40:36.240049","duration":2}},{"council_id":"MOL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:30:12] Fetching Scraper for: MOL                              handlers.py:23\n           Begin attempting to scrape: MOL                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:30:13] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:30:14] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.molevalley.gov.uk/home/council/councillors/who           \n           -are-your-councillors                                                \n[08:30:20] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.molevalley.gov.uk/home/council/councillors               \n           /who-are-your-councillors                                            \n           Finished attempting to scrape: MOL                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 50, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.molevalley.gov.uk/home/council/councillors/who-are-your-councillors\n","start":"2024-01-06 08:30:12.363357","end":"2024-01-06 08:30:20.680055","duration":8}},{"council_id":"MTY","missing":false,"latest_run":{"status_code":1,"log_text":"[09:56:37] Fetching Scraper for: MTY                              handlers.py:23\n           Begin attempting to scrape: MTY                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:56:38] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://democracy.merthyr.gov.uk/mgWebService.asmx/GetCounc           \n           illorsByWard                                                         \n[09:56:39] ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: MTY                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 09:56:37.120599","end":"2024-01-06 09:56:39.300878","duration":2}},{"council_id":"ROS","missing":false,"latest_run":{"status_code":1,"log_text":"[09:34:51] Fetching Scraper for: ROS                              handlers.py:23\n           Begin attempting to scrape: ROS                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:34:52] Getting all files in Councillors/json...                  base.py:203\n           ...found 35 files in Councillors/json                     base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 35 files in Councillors/raw                      base.py:219\n           ...found 71 files in Councillors                          base.py:219\n           Deleting batch no. 1 consisting of 71 files               base.py:230\n[09:34:53] ...data deleted.                                          base.py:258\n           Scraping from https://www.rossendale.gov.uk/councillors    base.py:41\n[09:34:54] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10079/adrian-lyt           \n           hgoe                                                                 \n[09:34:55] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10084/alan-neal            \n[09:34:56] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10093/alan-woods           \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10062/alyson-bar           \n           nes                                                                  \n[09:34:57] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10095/andrew-wal           \n           msley                                                                \n[09:34:58] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10080/andy-macna           \n           e                                                                    \n[09:34:59] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10102/ann-hodgki           \n           ss                                                                   \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10077/ann-kenyon           \n[09:35:00] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10065/anne-cartn           \n           er-cheetham                                                          \n[09:35:02] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10097/annie-mcma           \n           hon                                                                  \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10061/barbara-as           \n           hworth                                                               \n[09:35:03] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10101/caroline-s           \n           nowden                                                               \n[09:35:04] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10103/christine-           \n           gill                                                                 \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10107/danielle-a           \n           shworth                                                              \n[09:35:05] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10070/david-foxc           \n           roft                                                                 \n[09:35:06] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10096/dayne-powe           \n           ll                                                                   \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10071/gemma-rook           \n           e                                                                    \n[09:35:07] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10083/granville-           \n           morris                                                               \n[09:35:08] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10085/jacqueline           \n           -oakes                                                               \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10067/james-eato           \n           n                                                                    \n[09:35:09] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10060/janet-whit           \n           ehead                                                                \n[09:35:10] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10075/janice-joh           \n           nson                                                                 \n[09:35:11] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10064/jenny-rigb           \n           y                                                                    \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10105/judith-dri           \n           ver                                                                  \n[09:35:12] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10088/julie-adsh           \n           ead                                                                  \n[09:35:13] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10087/laura-beth           \n           -thompson                                                            \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10100/liz-mcinne           \n           s                                                                    \n[09:35:14] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10086/marilyn-pr           \n           octer                                                                \n[09:35:15] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10099/mary-cooga           \n           n                                                                    \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10106/matt-norto           \n           n                                                                    \n[09:35:16] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10098/michelle-s           \n           mith                                                                 \n[09:35:17] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10104/neil-looke           \n           r                                                                    \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10081/patrick-ma           \n           rriott                                                               \n[09:35:18] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10072/samara-bar           \n           nes                                                                  \n[09:35:19] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10063/scott-smit           \n           h                                                                    \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10074/vacant-vac           \n           ant                                                                  \n[09:35:20] 'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 70 files                 base.py:291\n[09:35:21] Finished attempting to scrape: ROS                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ROS-rossendale/councillors.py\", line 48, in get_single_councillor\n    councillor.email = soup.select_one(\".page-meta a[href^=mailto]\")[\nTypeError: 'NoneType' object is not subscriptable\n","start":"2024-01-06 09:34:51.175568","end":"2024-01-06 09:35:21.868914","duration":30}},{"council_id":"RUG","missing":false,"latest_run":{"status_code":1,"log_text":"[08:20:23] Fetching Scraper for: RUG                              handlers.py:23\n           Begin attempting to scrape: RUG                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:20:24] Getting all files in Councillors...                       base.py:203\n           Getting all files in Councillors/json...                  base.py:203\n           ...found 40 files in Councillors/json                     base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 40 files in Councillors/raw                      base.py:219\n           ...found 81 files in Councillors                          base.py:219\n           Deleting batch no. 1 consisting of 81 files               base.py:230\n[08:20:25] ...data deleted.                                          base.py:258\n           Scraping from https://www.rugby.gov.uk/councillors/        base.py:41\n[08:20:29] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19045/6618404/maximized                                              \n[08:20:30] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19117/6618404/maximized                                              \n[08:20:32] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19189/6618404/maximized                                              \n[08:20:33] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19135/6618404/maximized                                              \n[08:20:34] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19171/6618404/maximized                                              \n[08:20:36] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19099/6618404/maximized                                              \n[08:20:37] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19153/6618404/maximized                                              \n[08:20:38] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18865/6618404/maximized                                              \n[08:20:40] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18883/6618404/maximized                                              \n[08:20:41] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18828/6618404/maximized                                              \n[08:20:43] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18846/6618404/maximized                                              \n[08:20:44] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18774/6618404/maximized                                              \n[08:20:46] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18792/6618404/maximized                                              \n[08:20:47] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18810/6618404/maximized                                              \n[08:20:49] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18648/6618404/maximized                                              \n[08:20:51] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18611/6618404/maximized                                              \n[08:20:52] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18665/6618404/maximized                                              \n[08:20:53] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18629/6618404/maximized                                              \n[08:20:55] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18456/6618404/maximized                                              \n[08:20:56] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18492/6618404/maximized                                              \n[08:20:57] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18474/6618404/maximized                                              \n[08:20:58] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18701/6618404/maximized                                              \n[08:21:00] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18683/6618404/maximized                                              \n[08:21:01] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18737/6618404/maximized                                              \n[08:21:02] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18719/6618404/maximized                                              \n[08:21:04] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18756/6618404/maximized                                              \n[08:21:05] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19009/6618404/maximized                                              \n[08:21:07] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18955/6618404/maximized                                              \n[08:21:08] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18901/6618404/maximized                                              \n[08:21:09] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18991/6618404/maximized                                              \n[08:21:11] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19027/6618404/maximized                                              \n[08:21:12] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18938/6618404/maximized                                              \n[08:21:13] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18919/6618404/maximized                                              \n[08:21:15] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           18973/6618404/maximized                                              \n[08:21:17] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19207/6618404/maximized                                              \n[08:21:19] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19243/6618404/maximized                                              \n[08:21:20] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19225/6618404/maximized                                              \n[08:21:21] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19297/6618404/maximized                                              \n[08:21:23] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19279/6618404/maximized                                              \n[08:21:24] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19261/6618404/maximized                                              \n[08:21:25] Scraping from                                              base.py:41\n           https://www.rugby.gov.uk/councillors//-/ddl_display/ddl/66           \n           19082/6618404/maximized                                              \n[08:21:27] 'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Committing batch 1 consisting of 80 files                 base.py:291\n[08:21:28] Finished attempting to scrape: RUG                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/RUG-rugby/councillors.py\", line 49, in get_single_councillor\n    councillor.email = soup.find(\"span\", text=re.compile(\"@\")).get_text(strip=True)\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2024-01-06 08:20:23.501708","end":"2024-01-06 08:21:28.579367","duration":65}},{"council_id":"SAY","missing":false,"latest_run":{"status_code":1,"log_text":"[08:27:20] Fetching Scraper for: SAY                              handlers.py:23\n[08:27:21] Begin attempting to scrape: SAY                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:27:22] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.south-ayrshire.gov.uk/councillors/                       \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n[08:27:23] Finished attempting to scrape: SAY                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 08:27:20.971784","end":"2024-01-06 08:27:23.111499","duration":2}},{"council_id":"SHE","missing":false,"latest_run":{"status_code":null,"log_text":"[11:28:20] Fetching Scraper for: SHE                              handlers.py:22\n           Begin attempting to scrape: SHE                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in SHE...                               base.py:186\n[11:28:21] Getting all files in SHE/json...                          base.py:186\n           ...found 30 files in SHE/json                             base.py:202\n           Getting all files in SHE/raw...                           base.py:186\n           ...found 30 files in SHE/raw                              base.py:202\n           ...found 61 files in SHE                                  base.py:202\n           Deleting batch no. 1 consisting of 61 files               base.py:211\n[11:28:32] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n           Finished attempting to scrape: SHE                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:28:20.509898","end":"2022-04-04 11:28:32.871624","duration":12}},{"council_id":"SOL","missing":false,"latest_run":{"status_code":1,"log_text":"[09:57:20] Fetching Scraper for: SOL                              handlers.py:23\n           Begin attempting to scrape: SOL                        handlers.py:27\n[09:57:21] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:57:22] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://eservices.solihull.gov.uk/mgInternet/mgWebService.           \n           asmx/GetCouncillorsByWard                                            \n           503 Server Error: Service Unavailable for url:         handlers.py:36\n           https://eservices.solihull.gov.uk/mgInternet/mgWebServ               \n           ice.asmx/GetCouncillorsByWard                                        \n[09:57:23] Finished attempting to scrape: SOL                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 50, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 503 Server Error: Service Unavailable for url: https://eservices.solihull.gov.uk/mgInternet/mgWebService.asmx/GetCouncillorsByWard\n","start":"2024-01-06 09:57:20.921078","end":"2024-01-06 09:57:23.211892","duration":2}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[09:58:13] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:58:14] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from https://www.stirling.gov.uk/councillors      base.py:41\n[09:58:16] list index out of range                                handlers.py:36\n[09:58:17] Finished attempting to scrape: STG                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2024-01-06 09:58:13.065168","end":"2024-01-06 09:58:17.112079","duration":4}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[09:20:42] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n[09:20:43] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:20:44] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.threerivers.gov.uk/listing/councillors                   \n[09:20:46] 'NoneType' object has no attribute 'findNext'          handlers.py:36\n           Finished attempting to scrape: THE                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"scrapers/THE-three-rivers/councillors.py\", line 15, in get_list_container\n    return soup.find(\"h3\", text=\"District Councillor\").findNext(\"ul\")\nAttributeError: 'NoneType' object has no attribute 'findNext'\n","start":"2024-01-06 09:20:42.690851","end":"2024-01-06 09:20:46.168948","duration":3}},{"council_id":"WLN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:23:10] Fetching Scraper for: WLN                              handlers.py:23\n           Begin attempting to scrape: WLN                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:23:11] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:23:12] ...data deleted.                                          base.py:258\n           Scraping from https://westlothian.gov.uk/councillors       base.py:41\n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: WLN                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/WLN-west-lothian/councillors.py\", line 12, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 08:23:10.593099","end":"2024-01-06 08:23:12.761039","duration":2}},{"council_id":"WOC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:36:10] Fetching Scraper for: WOC                              handlers.py:23\n           Begin attempting to scrape: WOC                        handlers.py:27\n[08:36:11] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:36:12] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://committee.worcester.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: WOC                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-01-06 08:36:10.698761","end":"2024-01-06 08:36:12.870746","duration":2}}]
