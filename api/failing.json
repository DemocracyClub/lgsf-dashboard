[{"council_id":"BOT","missing":false,"latest_run":{"status_code":1,"log_text":"[09:15:03] Fetching Scraper for: BOT                              handlers.py:23\n           Begin attempting to scrape: BOT                        handlers.py:27\n[09:15:04] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:15:05] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://moderngov.boston.gov.uk/mgWebService.asmx/GetCounci           \n           llorsByWard                                                          \n           HTTPSConnectionPool(host='democracy.boston.gov.uk',    handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1007)')))                                                    \n[09:15:06] Finished attempting to scrape: BOT                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.boston.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.boston.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n","start":"2024-04-16 09:15:03.873401","end":"2024-04-16 09:15:06.166683","duration":2}},{"council_id":"CAN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:36:48] Fetching Scraper for: CAN                              handlers.py:23\n           Begin attempting to scrape: CAN                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[08:36:49] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.cannockchasedc.gov.uk/council/about-council/yo           \n           ur-councillors                                                       \n[08:36:52] list index out of range                                handlers.py:36\n[08:36:53] Finished attempting to scrape: CAN                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2024-04-16 08:36:48.129847","end":"2024-04-16 08:36:53.141323","duration":5}},{"council_id":"CAT","missing":false,"latest_run":{"status_code":1,"log_text":"[08:38:37] Fetching Scraper for: CAT                              handlers.py:23\n           Begin attempting to scrape: CAT                        handlers.py:27\n[08:38:38] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:38:39] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://democracy.canterbury.gov.uk/mgWebService.asmx/GetCo           \n           uncillorsByWard                                                      \n[08:40:49] HTTPConnectionPool(host='democracy.canterbury.gov.uk', handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           ConnectTimeoutError(<urllib3.connection.HTTPConnection               \n           object at 0x7f7451b0e830>, 'Connection to                            \n           democracy.canterbury.gov.uk timed out. (connect                      \n           timeout=None)'))                                                     \n           Finished attempting to scrape: CAT                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 497, in _make_request\n    conn.request(\n  File \"/opt/python/urllib3/connection.py\", line 395, in request\n    self.endheaders()\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1278, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1038, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 976, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 243, in connect\n    self.sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 212, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x7f7451b0e830>, 'Connection to democracy.canterbury.gov.uk timed out. (connect timeout=None)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='democracy.canterbury.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f7451b0e830>, 'Connection to democracy.canterbury.gov.uk timed out. (connect timeout=None)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 507, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: HTTPConnectionPool(host='democracy.canterbury.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f7451b0e830>, 'Connection to democracy.canterbury.gov.uk timed out. (connect timeout=None)'))\n","start":"2024-04-16 08:38:37.923573","end":"2024-04-16 08:40:49.658215","duration":131}},{"council_id":"DRS","missing":false,"latest_run":{"status_code":1,"log_text":"[09:50:29] Fetching Scraper for: DRS                              handlers.py:23\n           Begin attempting to scrape: DRS                        handlers.py:27\n[09:50:30] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:50:31] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://meetings.derrycityandstrabanedistrict.com/mgWebServ           \n           ice.asmx/GetCouncillorsByWard                                        \n[09:52:41] HTTPConnectionPool(host='meetings.derrycityandstrabane handlers.py:36\n           district.com', port=80): Max retries exceeded with                   \n           url: /mgWebService.asmx/GetCouncillorsByWard (Caused                 \n           by                                                                   \n           ConnectTimeoutError(<urllib3.connection.HTTPConnection               \n           object at 0x7f7a876f45b0>, 'Connection to                            \n           meetings.derrycityandstrabanedistrict.com timed out.                 \n           (connect timeout=None)'))                                            \n           Finished attempting to scrape: DRS                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 497, in _make_request\n    conn.request(\n  File \"/opt/python/urllib3/connection.py\", line 395, in request\n    self.endheaders()\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1278, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1038, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 976, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 243, in connect\n    self.sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 212, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPConnection object at 0x7f7a876f45b0>, 'Connection to meetings.derrycityandstrabanedistrict.com timed out. (connect timeout=None)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='meetings.derrycityandstrabanedistrict.com', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f7a876f45b0>, 'Connection to meetings.derrycityandstrabanedistrict.com timed out. (connect timeout=None)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 507, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: HTTPConnectionPool(host='meetings.derrycityandstrabanedistrict.com', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f7a876f45b0>, 'Connection to meetings.derrycityandstrabanedistrict.com timed out. (connect timeout=None)'))\n","start":"2024-04-16 09:50:29.923790","end":"2024-04-16 09:52:41.473551","duration":131}},{"council_id":"DUD","missing":false,"latest_run":{"status_code":1,"log_text":"[10:22:32] Fetching Scraper for: DUD                              handlers.py:23\n           Begin attempting to scrape: DUD                        handlers.py:27\n[10:22:34] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           Getting all files in Councillors/json...                  base.py:203\n[10:22:35] ...found 72 files in Councillors/json                     base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 72 files in Councillors/raw                      base.py:219\n           ...found 145 files in Councillors                         base.py:219\n           Deleting batch no. 1 consisting of 100 files              base.py:230\n[10:22:36] Deleting batch no. 2 consisting of 45 files               base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://cmis.dudley.gov.uk/cmis5/Councillors.aspx                     \n[10:22:38] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1272/ScreenMode/Ward/Default.aspx                \n[10:22:41] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1403/ScreenMode/Ward/Default.aspx                \n[10:22:43] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1434/ScreenMode/Ward/Default.aspx                \n[10:22:45] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1435/ScreenMode/Ward/Default.aspx                \n[10:22:47] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1436/ScreenMode/Ward/Default.aspx                \n[10:22:49] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1295/ScreenMode/Ward/Default.aspx                \n[10:22:51] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1437/ScreenMode/Ward/Default.aspx                \n[10:22:55] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1476/ScreenMode/Ward/Default.aspx                \n[10:22:57] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1006/ScreenMode/Ward/Default.aspx                \n[10:23:01] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1438/ScreenMode/Ward/Default.aspx                \n[10:23:04] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/826/ScreenMode/Ward/Default.aspx                 \n[10:23:07] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1477/ScreenMode/Ward/Default.aspx                \n[10:23:09] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/804/ScreenMode/Ward/Default.aspx                 \n[10:23:12] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1221/ScreenMode/Ward/Default.aspx                \n[10:23:14] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1478/ScreenMode/Ward/Default.aspx                \n[10:23:18] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1402/ScreenMode/Ward/Default.aspx                \n[10:23:21] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1458/ScreenMode/Ward/Default.aspx                \n[10:23:23] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/846/ScreenMode/Ward/Default.aspx                 \n[10:23:25] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/819/ScreenMode/Ward/Default.aspx                 \n[10:23:28] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1278/ScreenMode/Ward/Default.aspx                \n[10:23:30] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1495/ScreenMode/Ward/Default.aspx                \n[10:23:32] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1380/ScreenMode/Ward/Default.aspx                \n[10:23:34] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/853/ScreenMode/Ward/Default.aspx                 \n[10:23:37] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1479/ScreenMode/Ward/Default.aspx                \n[10:23:40] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1215/ScreenMode/Ward/Default.aspx                \n[10:23:43] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1285/ScreenMode/Ward/Default.aspx                \n[10:23:45] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1400/ScreenMode/Ward/Default.aspx                \n[10:23:47] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1459/ScreenMode/Ward/Default.aspx                \n[10:23:50] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1460/ScreenMode/Ward/Default.aspx                \n[10:23:52] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/879/ScreenMode/Ward/Default.aspx                 \n[10:23:55] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1401/ScreenMode/Ward/Default.aspx                \n[10:23:57] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1331/ScreenMode/Ward/Default.aspx                \n[10:24:01] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1277/ScreenMode/Ward/Default.aspx                \n[10:24:03] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1440/ScreenMode/Ward/Default.aspx                \n[10:24:05] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1322/ScreenMode/Ward/Default.aspx                \n[10:24:07] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1480/ScreenMode/Ward/Default.aspx                \n[10:24:10] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/874/ScreenMode/Ward/Default.aspx                 \n[10:24:13] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1332/ScreenMode/Ward/Default.aspx                \n[10:24:16] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/840/ScreenMode/Ward/Default.aspx                 \n[10:24:18] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1441/ScreenMode/Ward/Default.aspx                \n[10:24:22] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1005/ScreenMode/Ward/Default.aspx                \n[10:24:25] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/974/ScreenMode/Ward/Default.aspx                 \n[10:24:27] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1442/ScreenMode/Ward/Default.aspx                \n[10:24:29] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1279/ScreenMode/Ward/Default.aspx                \n[10:24:32] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1202/ScreenMode/Ward/Default.aspx                \n[10:24:34] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1299/ScreenMode/Ward/Default.aspx                \n[10:24:36] Committing batch 1 consisting of 92 files                 base.py:291\n[10:24:37] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1461/ScreenMode/Ward/Default.aspx                \n[10:24:40] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1328/ScreenMode/Ward/Default.aspx                \n[10:24:43] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1443/ScreenMode/Ward/Default.aspx                \n[10:24:45] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/834/ScreenMode/Ward/Default.aspx                 \n[10:24:48] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1334/ScreenMode/Ward/Default.aspx                \n[10:24:53] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1383/ScreenMode/Ward/Default.aspx                \n[10:24:56] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1444/ScreenMode/Ward/Default.aspx                \n[10:24:58] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1199/ScreenMode/Ward/Default.aspx                \n[10:25:01] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1481/ScreenMode/Ward/Default.aspx                \n[10:25:03] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/823/ScreenMode/Ward/Default.aspx                 \n[10:25:07] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1384/ScreenMode/Ward/Default.aspx                \n[10:25:14] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/793/ScreenMode/Ward/Default.aspx                 \n[10:25:17] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1358/ScreenMode/Ward/Default.aspx                \n[10:25:20] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1482/ScreenMode/Ward/Default.aspx                \n[10:25:22] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1321/ScreenMode/Ward/Default.aspx                \n[10:25:25] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1463/ScreenMode/Ward/Default.aspx                \n[10:25:27] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1498/ScreenMode/Ward/Default.aspx                \n[10:25:29] Scraping from                                              base.py:41\n           http://dudley.cmis.uk.com/Councillors/tabid/62/ctl/ViewCMI           \n           S_Person/mid/480/id/1197/ScreenMode/Ward/Default.aspx                \n[10:25:32] 'title'                                                handlers.py:36\n           Committing batch 2 consisting of 36 files                 base.py:291\n[10:25:33] Finished attempting to scrape: DUD                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 306, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 287, in get_party_name\n    list_page_html.find_all(\"img\")[-1][\"title\"]\n  File \"/opt/python/bs4/element.py\", line 1573, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2024-04-16 10:22:32.475925","end":"2024-04-16 10:25:33.522837","duration":181}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[09:14:04] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:14:05] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://cne-siar.gov.uk/home/your-council/council-members/           \n[09:14:08] Scraping from https://cne-siar.gov.uk/kenneth-j-maclean/   base.py:41\n[09:14:11] 'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Finished attempting to scrape: ELS                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ELS-na-h-eileanan-an-iar/councillors.py\", line 25, in get_single_councillor\n    .get_text(strip=True)\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2024-04-16 09:14:04.196870","end":"2024-04-16 09:14:11.387972","duration":7}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[09:28:01] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:28:02] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n[09:28:03] Finished attempting to scrape: ERW                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-04-16 09:28:01.049168","end":"2024-04-16 09:28:03.162719","duration":2}},{"council_id":"ESS","missing":false,"latest_run":{"status_code":1,"log_text":"[08:48:31] Fetching Scraper for: ESS                              handlers.py:23\n           Begin attempting to scrape: ESS                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:48:32] Getting all files in Councillors...                       base.py:203\n           Getting all files in Councillors/json...                  base.py:203\n           ...found 33 files in Councillors/json                     base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 33 files in Councillors/raw                      base.py:219\n           ...found 67 files in Councillors                          base.py:219\n           Deleting batch no. 1 consisting of 67 files               base.py:230\n[08:48:33] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/EssexCmis5/Councillors.aspx                 \n[08:48:35] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/655/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:48:38] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/327/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:48:40] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/100/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:48:46] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/639/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:48:49] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/882/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:48:52] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/880/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:48:56] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/879/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:48:59] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/648/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:01] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/80/ScreenMode/Ward/Default.a           \n           spx                                                                  \n[08:49:04] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/649/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:07] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/866/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:09] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/458/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:13] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/864/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:27] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/887/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:35] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/334/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:43] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/650/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:46] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/364/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:51] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/863/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:54] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/309/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:49:56] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/91/ScreenMode/Ward/Default.a           \n           spx                                                                  \n[08:49:59] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/877/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:02] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/777/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:05] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/865/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:08] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/359/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:11] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/870/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:14] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/340/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:17] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/94/ScreenMode/Ward/Default.a           \n           spx                                                                  \n[08:50:24] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/883/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:31] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/656/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:35] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/872/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:41] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/117/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:43] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/889/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:47] Scraping from                                              base.py:41\n           http://cmis.essex.gov.uk/essexcmis5/Councillors/tabid/62/c           \n           tl/ViewCMIS_Person/mid/480/id/610/ScreenMode/Ward/Default.           \n           aspx                                                                 \n[08:50:50] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 66 files                 base.py:291\n[08:50:51] Finished attempting to scrape: ESS                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 306, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 287, in get_party_name\n    list_page_html.find_all(\"img\")[-1][\"title\"]\n  File \"/opt/python/bs4/element.py\", line 1573, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2024-04-16 08:48:31.375404","end":"2024-04-16 08:50:51.836487","duration":140}},{"council_id":"GRE","missing":false,"latest_run":{"status_code":1,"log_text":"[10:09:31] Fetching Scraper for: GRE                              handlers.py:23\n           Begin attempting to scrape: GRE                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[10:09:32] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[10:09:33] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://committees.royalgreenwich.gov.uk/Councillors/tabid           \n           /63/ScreenMode/Alphabetical/Default.aspx                             \n           HTTPSConnectionPool(host='committees.royalgreenwich.go handlers.py:36\n           v.uk', port=443): Max retries exceeded with url:                     \n           /Councillors/tabid/63/ScreenMode/Alphabetical/Default.               \n           aspx (Caused by SSLError(SSLCertVerificationError(1,                 \n           '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify                 \n           failed: unable to get local issuer certificate                       \n           (_ssl.c:1007)')))                                                    \n           Finished attempting to scrape: GRE                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 277, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='committees.royalgreenwich.gov.uk', port=443): Max retries exceeded with url: /Councillors/tabid/63/ScreenMode/Alphabetical/Default.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n","start":"2024-04-16 10:09:31.389107","end":"2024-04-16 10:09:33.465040","duration":2}},{"council_id":"GRT","missing":false,"latest_run":{"status_code":1,"log_text":"[09:10:34] Fetching Scraper for: GRT                              handlers.py:23\n           Begin attempting to scrape: GRT                        handlers.py:27\n[09:10:35] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:10:36] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www2.guildford.gov.uk/councilmeetings/mgWebService           \n           .asmx/GetCouncillorsByWard                                           \n           HTTPSConnectionPool(host='www2.guildford.gov.uk',      handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /councilmeetings/mgWebService.asmx/GetCouncillorsByWar               \n           d (Caused by                                                         \n           NameResolutionError(\"<urllib3.connection.HTTPSConnecti               \n           on object at 0x7f7a877eb3d0>: Failed to resolve                      \n           'www2.guildford.gov.uk' ([Errno -2] Name or service                  \n           not known)\"))                                                        \n[09:10:37] Finished attempting to scrape: GRT                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 60, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.10/socket.py\", line 955, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 611, in connect\n    self.sock = sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 210, in _new_conn\n    raise NameResolutionError(self.host, self, e) from e\nurllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x7f7a877eb3d0>: Failed to resolve 'www2.guildford.gov.uk' ([Errno -2] Name or service not known)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www2.guildford.gov.uk', port=443): Max retries exceeded with url: /councilmeetings/mgWebService.asmx/GetCouncillorsByWard (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f7a877eb3d0>: Failed to resolve 'www2.guildford.gov.uk' ([Errno -2] Name or service not known)\"))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='www2.guildford.gov.uk', port=443): Max retries exceeded with url: /councilmeetings/mgWebService.asmx/GetCouncillorsByWard (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7f7a877eb3d0>: Failed to resolve 'www2.guildford.gov.uk' ([Errno -2] Name or service not known)\"))\n","start":"2024-04-16 09:10:34.907990","end":"2024-04-16 09:10:37.257422","duration":2}},{"council_id":"HIG","missing":false,"latest_run":{"status_code":1,"log_text":"[10:52:45] Fetching Scraper for: HIG                              handlers.py:23\n           Begin attempting to scrape: HIG                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[10:52:46] Getting all files in Councillors...                       base.py:203\n           Getting all files in Councillors/json...                  base.py:203\n           ...found 100 files in Councillors/json                    base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 100 files in Councillors/raw                     base.py:219\n           ...found 201 files in Councillors                         base.py:219\n           Deleting batch no. 1 consisting of 100 files              base.py:230\n[10:52:47] Deleting batch no. 2 consisting of 100 files              base.py:230\n[10:52:48] Deleting batch no. 3 consisting of 1 files                base.py:230\n[10:52:49] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://democracy.highpeak.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n[10:54:58] HTTPSConnectionPool(host='democracy.highpeak.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           ConnectTimeoutError(<urllib3.connection.HTTPSConnectio               \n           n object at 0x7f7a8ba79780>, 'Connection to                          \n           democracy.highpeak.gov.uk timed out. (connect                        \n           timeout=None)'))                                                     \n[10:54:59] Finished attempting to scrape: HIG                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 611, in connect\n    self.sock = sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 212, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x7f7a8ba79780>, 'Connection to democracy.highpeak.gov.uk timed out. (connect timeout=None)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.highpeak.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f7a8ba79780>, 'Connection to democracy.highpeak.gov.uk timed out. (connect timeout=None)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 507, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='democracy.highpeak.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f7a8ba79780>, 'Connection to democracy.highpeak.gov.uk timed out. (connect timeout=None)'))\n","start":"2024-04-16 10:52:45.447873","end":"2024-04-16 10:54:59.030609","duration":133}},{"council_id":"HLD","missing":false,"latest_run":{"status_code":1,"log_text":"[09:15:10] Fetching Scraper for: HLD                              handlers.py:23\n           Begin attempting to scrape: HLD                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[09:15:11] Getting all files in Councillors/json...                  base.py:203\n           ...found 41 files in Councillors/json                     base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 41 files in Councillors/raw                      base.py:219\n           ...found 83 files in Councillors                          base.py:219\n           Deleting batch no. 1 consisting of 83 files               base.py:230\n[09:15:12] ...data deleted.                                          base.py:258\n           Scraping from https://www.highland.gov.uk/councillors/name base.py:41\n[09:15:13] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/102/sarah_atkin              \n[09:15:14] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/110/michael_baird            \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/94/andrew_baldrey            \n[09:15:15] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/100/chris_ballance           \n[09:15:16] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/120/chris_birt               \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/41/bill_boyd                 \n[09:15:17] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/8/raymond_bremner            \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/30/ian_brown                 \n[09:15:18] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/79/john_bruce                \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/117/michael_camero           \n           n                                                                    \n[09:15:19] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/17/isabelle_biz_ca           \n           mpbell                                                               \n[09:15:20] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/82/glynis_campbell           \n           _sinclair                                                            \n[09:15:21] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/35/alasdair_christ           \n           ie                                                                   \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/40/muriel_cockburn           \n[09:15:22] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/124/tamala_collier           \n[09:15:23] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/101/helen_crawford           \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/83/sarah_fanet               \n[09:15:24] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/71/john_finlayson            \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/29/david_fraser              \n[09:15:25] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/47/laurie_fraser             \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/10/richard_gale              \n[09:15:26] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/51/ken_gowans                \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/95/john_grafton              \n[09:15:27] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/52/alex_graham               \n[09:15:28] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/130/michael_green            \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/128/david_gregg              \n[09:15:29] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/114/ron_gunn                 \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/123/jackie_hendry            \n[09:15:30] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/111/marianne_hutch           \n           ison                                                                 \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/68/andrew_jarvie             \n[09:15:31] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/131/barbara_jarvie           \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/103/lyndsey_johnst           \n           on                                                                   \n[09:15:32] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/109/russell_jones            \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/99/sean_kennedy              \n[09:15:33] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/86/emma_knox                 \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/121/liz_kraft                \n[09:15:34] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/61/bill_lobban               \n[09:15:35] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/122/patrick_logue            \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/78/derek_louden              \n[09:15:36] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/105/morven-may_mac           \n           callum                                                               \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/112/angus_macdonal           \n           d                                                                    \n[09:15:37] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/12/willie_mackay             \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/64/graham_mackenzi           \n           e                                                                    \n[09:15:38] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/39/isabelle_macken           \n           zie                                                                  \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/11/struan_mackie             \n[09:15:39] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/125/andrew_mackint           \n           osh                                                                  \n[09:15:40] Committing batch 1 consisting of 92 files                 base.py:291\n[09:15:41] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/98/ryan_mackintosh           \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/66/angela_maclean            \n[09:15:42] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/119/kate_maclean             \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/113/thomas_maclenn           \n           an                                                                   \n[09:15:43] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/37/duncan_macphers           \n           on                                                                   \n           Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/69/bet_mcallister            \n[09:15:45] Scraping from                                              base.py:41\n           https://www.highland.gov.uk/councillors/81/duncan_mcdonald           \n           'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Committing batch 2 consisting of 12 files                 base.py:291\n[09:15:46] Finished attempting to scrape: HLD                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/HLD-highland/councillors.py\", line 49, in get_single_councillor\n    councillor.email = soup.select_one(\"li a[href^=mailto]\").get_text(\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2024-04-16 09:15:10.161247","end":"2024-04-16 09:15:46.666567","duration":36}},{"council_id":"LEC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:31:09] Fetching Scraper for: LEC                              handlers.py:23\n           Begin attempting to scrape: LEC                        handlers.py:27\n[08:31:10] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:31:11] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://politics.leics.gov.uk//mgWebService.asmx/GetCouncil           \n           lorsByWard                                                           \n           HTTPConnectionPool(host='politics.leics.gov.uk',       handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           //mgWebService.asmx/GetCouncillorsByWard (Caused by                  \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f7451afc7f0>: Failed to establish a new                 \n           connection: [Errno 111] Connection refused'))                        \n[08:31:12] Finished attempting to scrape: LEC                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 497, in _make_request\n    conn.request(\n  File \"/opt/python/urllib3/connection.py\", line 395, in request\n    self.endheaders()\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1278, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 1038, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.10/http/client.py\", line 976, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 243, in connect\n    self.sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 218, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f7451afc7f0>: Failed to establish a new connection: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='politics.leics.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7451afc7f0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='politics.leics.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7451afc7f0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","start":"2024-04-16 08:31:09.944445","end":"2024-04-16 08:31:12.022149","duration":2}},{"council_id":"MAV","missing":false,"latest_run":{"status_code":1,"log_text":"[10:20:03] Fetching Scraper for: MAV                              handlers.py:23\n           Begin attempting to scrape: MAV                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[10:20:04] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[10:20:05] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://moderngov.malvernhills.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: MAV                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-04-16 10:20:03.495194","end":"2024-04-16 10:20:05.665313","duration":2}},{"council_id":"MOL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:24:48] Fetching Scraper for: MOL                              handlers.py:23\n           Begin attempting to scrape: MOL                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:24:49] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:24:50] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.molevalley.gov.uk/home/council/councillors/who           \n           -are-your-councillors                                                \n[08:24:55] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.molevalley.gov.uk/home/council/councillors               \n           /who-are-your-councillors                                            \n           Finished attempting to scrape: MOL                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 50, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.molevalley.gov.uk/home/council/councillors/who-are-your-councillors\n","start":"2024-04-16 08:24:48.628397","end":"2024-04-16 08:24:55.759499","duration":7}},{"council_id":"MTY","missing":false,"latest_run":{"status_code":1,"log_text":"[08:34:03] Fetching Scraper for: MTY                              handlers.py:23\n           Begin attempting to scrape: MTY                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:34:04] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://democracy.merthyr.gov.uk/mgWebService.asmx/GetCounc           \n           illorsByWard                                                         \n[08:34:05] ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: MTY                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-04-16 08:34:03.161178","end":"2024-04-16 08:34:05.538956","duration":2}},{"council_id":"NUN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:32:43] Fetching Scraper for: NUN                              handlers.py:23\n           Begin attempting to scrape: NUN                        handlers.py:27\n[08:32:44] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:32:45] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.nuneatonandbedworth.gov.uk/councillors/name              \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.nuneatonandbedworth.gov.uk/councillors/nam               \n           e                                                                    \n           Finished attempting to scrape: NUN                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 50, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.nuneatonandbedworth.gov.uk/councillors/name\n","start":"2024-04-16 08:32:43.668724","end":"2024-04-16 08:32:45.653053","duration":1}},{"council_id":"ORK","missing":false,"latest_run":{"status_code":1,"log_text":"[10:13:41] Fetching Scraper for: ORK                              handlers.py:23\n           Begin attempting to scrape: ORK                        handlers.py:27\n[10:13:42] Deleting existing data...                                 base.py:251\n[10:13:43] Getting all files in Councillors...                       base.py:203\n           Getting all files in Councillors/json...                  base.py:203\n           ...found 16 files in Councillors/json                     base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 16 files in Councillors/raw                      base.py:219\n           ...found 33 files in Councillors                          base.py:219\n           Deleting batch no. 1 consisting of 33 files               base.py:230\n[10:13:44] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/councillors.           \n           htm                                                                  \n[10:13:45] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/james-moar.h           \n           tm                                                                   \n[10:13:46] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/raymie-peace           \n           .htm                                                                 \n[10:13:47] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/gillian-skus           \n           e.htm                                                                \n[10:13:48] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/david-dawson           \n           .htm                                                                 \n           Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/Steven-Heddl           \n           e.htm                                                                \n[10:13:49] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/john-ross-sc           \n           ott.htm                                                              \n[10:13:50] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/gwenda-shear           \n           er.htm                                                               \n[10:13:51] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/sandy-cowie.           \n           htm                                                                  \n           Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/kristopher-l           \n           eask.htm                                                             \n[10:13:52] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/leslie-manso           \n           n.htm                                                                \n[10:13:53] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/ivan-taylor.           \n           htm                                                                  \n           Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/stephen-clac           \n           kson.htm                                                             \n[10:13:54] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/mellissa-tho           \n           mson.htm                                                             \n[10:13:55] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/heather-wood           \n           bridge.htm                                                           \n[10:13:56] Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/graham-bevan           \n           .htm                                                                 \n           Scraping from                                              base.py:41\n           https://www.orkney.gov.uk/Council/Councillors/lindsay-hall           \n           .htm                                                                 \n[10:13:57] 'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 32 files                 base.py:291\n[10:13:58] Finished attempting to scrape: ORK                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ORK-orkney-islands/councillors.py\", line 16, in get_single_councillor\n    url = urljoin(self.base_url, councillor_html.a[\"href\"])\nTypeError: 'NoneType' object is not subscriptable\n","start":"2024-04-16 10:13:41.050636","end":"2024-04-16 10:13:58.710571","duration":17}},{"council_id":"ROS","missing":false,"latest_run":{"status_code":1,"log_text":"[10:04:02] Fetching Scraper for: ROS                              handlers.py:23\n           Begin attempting to scrape: ROS                        handlers.py:27\n[10:04:03] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           Getting all files in Councillors/json...                  base.py:203\n           ...found 35 files in Councillors/json                     base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n[10:04:04] ...found 35 files in Councillors/raw                      base.py:219\n           ...found 71 files in Councillors                          base.py:219\n           Deleting batch no. 1 consisting of 71 files               base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from https://www.rossendale.gov.uk/councillors    base.py:41\n[10:04:06] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10079/adrian-lyt           \n           hgoe                                                                 \n[10:04:07] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10084/alan-neal            \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10093/alan-woods           \n[10:04:08] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10062/alyson-bar           \n           nes                                                                  \n[10:04:09] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10095/andrew-wal           \n           msley                                                                \n[10:04:10] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10080/andy-macna           \n           e                                                                    \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10102/ann-hodgki           \n           ss                                                                   \n[10:04:11] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10077/ann-kenyon           \n[10:04:12] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10065/anne-cartn           \n           er-cheetham                                                          \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10097/annie-mcma           \n           hon                                                                  \n[10:04:13] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10061/barbara-as           \n           hworth                                                               \n[10:04:14] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10101/caroline-s           \n           nowden                                                               \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10103/christine-           \n           gill                                                                 \n[10:04:15] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10107/danielle-a           \n           shworth                                                              \n[10:04:16] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10070/david-foxc           \n           roft                                                                 \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10096/dayne-powe           \n           ll                                                                   \n[10:04:17] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10071/gemma-rook           \n           e                                                                    \n[10:04:19] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10083/granville-           \n           morris                                                               \n[10:04:20] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10085/jacqueline           \n           -oakes                                                               \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10067/james-eato           \n           n                                                                    \n[10:04:21] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10060/janet-whit           \n           ehead                                                                \n[10:04:22] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10075/janice-joh           \n           nson                                                                 \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10064/jenny-rigb           \n           y                                                                    \n[10:04:23] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10105/judith-dri           \n           ver                                                                  \n[10:04:24] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10088/julie-adsh           \n           ead                                                                  \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10087/laura-beth           \n           -thompson                                                            \n[10:04:25] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10100/liz-mcinne           \n           s                                                                    \n[10:04:26] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10086/marilyn-pr           \n           octer                                                                \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10099/mary-cooga           \n           n                                                                    \n[10:04:27] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10106/matt-norto           \n           n                                                                    \n[10:04:28] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10098/michelle-s           \n           mith                                                                 \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10104/neil-looke           \n           r                                                                    \n[10:04:29] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10081/patrick-ma           \n           rriott                                                               \n[10:04:30] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10072/samara-bar           \n           nes                                                                  \n           Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10063/scott-smit           \n           h                                                                    \n[10:04:31] Scraping from                                              base.py:41\n           https://www.rossendale.gov.uk/councillors/10074/vacant-vac           \n           ant                                                                  \n[10:04:32] 'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 70 files                 base.py:291\n[10:04:33] Finished attempting to scrape: ROS                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 58, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ROS-rossendale/councillors.py\", line 48, in get_single_councillor\n    councillor.email = soup.select_one(\".page-meta a[href^=mailto]\")[\nTypeError: 'NoneType' object is not subscriptable\n","start":"2024-04-16 10:04:02.766712","end":"2024-04-16 10:04:33.599899","duration":30}},{"council_id":"SAY","missing":false,"latest_run":{"status_code":1,"log_text":"[09:43:31] Fetching Scraper for: SAY                              handlers.py:23\n           Begin attempting to scrape: SAY                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[09:43:32] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:43:33] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.south-ayrshire.gov.uk/councillors/                       \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: SAY                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-04-16 09:43:31.328298","end":"2024-04-16 09:43:33.439227","duration":2}},{"council_id":"SHE","missing":false,"latest_run":{"status_code":null,"log_text":"[11:28:20] Fetching Scraper for: SHE                              handlers.py:22\n           Begin attempting to scrape: SHE                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in SHE...                               base.py:186\n[11:28:21] Getting all files in SHE/json...                          base.py:186\n           ...found 30 files in SHE/json                             base.py:202\n           Getting all files in SHE/raw...                           base.py:186\n           ...found 30 files in SHE/raw                              base.py:202\n           ...found 61 files in SHE                                  base.py:202\n           Deleting batch no. 1 consisting of 61 files               base.py:211\n[11:28:32] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n           Finished attempting to scrape: SHE                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:28:20.509898","end":"2022-04-04 11:28:32.871624","duration":12}},{"council_id":"SHO","missing":false,"latest_run":{"status_code":1,"log_text":"[10:01:51] Fetching Scraper for: SHO                              handlers.py:23\n           Begin attempting to scrape: SHO                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[10:01:52] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[10:01:53] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://democracy.sholland.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           HTTPSConnectionPool(host='democracy.sholland.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1007)')))                                                    \n           Finished attempting to scrape: SHO                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\nurllib3.exceptions.SSLError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.sholland.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.sholland.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1007)')))\n","start":"2024-04-16 10:01:51.391754","end":"2024-04-16 10:01:53.609995","duration":2}},{"council_id":"SOL","missing":false,"latest_run":{"status_code":1,"log_text":"[09:37:57] Fetching Scraper for: SOL                              handlers.py:23\n           Begin attempting to scrape: SOL                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[09:37:58] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://eservices.solihull.gov.uk/mgInternet/mgWebService.           \n           asmx/GetCouncillorsByWard                                            \n           HTTPSConnectionPool(host='eservices.solihull.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgInternet/mgWebService.asmx/GetCouncillorsByWard                   \n           (Caused by                                                           \n           NewConnectionError('<urllib3.connection.HTTPSConnectio               \n           n object at 0x7f7a8babeef0>: Failed to establish a new               \n           connection: [Errno 111] Connection refused'))                        \n[09:37:59] Finished attempting to scrape: SOL                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 611, in connect\n    self.sock = sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 218, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f7a8babeef0>: Failed to establish a new connection: [Errno 111] Connection refused\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='eservices.solihull.gov.uk', port=443): Max retries exceeded with url: /mgInternet/mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7a8babeef0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='eservices.solihull.gov.uk', port=443): Max retries exceeded with url: /mgInternet/mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f7a8babeef0>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","start":"2024-04-16 09:37:57.252948","end":"2024-04-16 09:37:59.184418","duration":1}},{"council_id":"STF","missing":false,"latest_run":{"status_code":1,"log_text":"[10:05:49] Fetching Scraper for: STF                              handlers.py:23\n           Begin attempting to scrape: STF                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[10:05:50] Getting all files in Councillors...                       base.py:203\n           Getting all files in Councillors/json...                  base.py:203\n           ...found 100 files in Councillors/json                    base.py:219\n           Getting all files in Councillors/raw...                   base.py:203\n           ...found 100 files in Councillors/raw                     base.py:219\n           ...found 201 files in Councillors                         base.py:219\n           Deleting batch no. 1 consisting of 100 files              base.py:230\n[10:05:51] Deleting batch no. 2 consisting of 100 files              base.py:230\n[10:05:52] Deleting batch no. 3 consisting of 1 files                base.py:230\n[10:05:53] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://democracy.staffsmoorlands.gov.uk/mgWebService.asmx           \n           /GetCouncillorsByWard                                                \n[10:08:02] HTTPSConnectionPool(host='democracy.staffsmoorlands.go handlers.py:36\n           v.uk', port=443): Max retries exceeded with url:                     \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           ConnectTimeoutError(<urllib3.connection.HTTPSConnectio               \n           n object at 0x7f7a87354f10>, 'Connection to                          \n           democracy.staffsmoorlands.gov.uk timed out. (connect                 \n           timeout=None)'))                                                     \n           Finished attempting to scrape: STF                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 203, in _new_conn\n    sock = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 73, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 611, in connect\n    self.sock = sock = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 212, in _new_conn\n    raise ConnectTimeoutError(\nurllib3.exceptions.ConnectTimeoutError: (<urllib3.connection.HTTPSConnection object at 0x7f7a87354f10>, 'Connection to democracy.staffsmoorlands.gov.uk timed out. (connect timeout=None)')\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 515, in increment\n    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.staffsmoorlands.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f7a87354f10>, 'Connection to democracy.staffsmoorlands.gov.uk timed out. (connect timeout=None)'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 507, in send\n    raise ConnectTimeout(e, request=request)\nrequests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='democracy.staffsmoorlands.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f7a87354f10>, 'Connection to democracy.staffsmoorlands.gov.uk timed out. (connect timeout=None)'))\n","start":"2024-04-16 10:05:49.319653","end":"2024-04-16 10:08:03.008082","duration":133}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[08:46:14] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n[08:46:15] ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n           ...data deleted.                                          base.py:258\n           Scraping from https://www.stirling.gov.uk/councillors      base.py:41\n[08:46:17] list index out of range                                handlers.py:36\n           Finished attempting to scrape: STG                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 161, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2024-04-16 08:46:14.093552","end":"2024-04-16 08:46:17.656543","duration":3}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[09:14:13] Fetching Scraper for: THE                              handlers.py:23\n[09:14:14] Begin attempting to scrape: THE                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[09:14:15] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           https://www.threerivers.gov.uk/listing/councillors                   \n[09:14:17] 'NoneType' object has no attribute 'findNext'          handlers.py:36\n           Finished attempting to scrape: THE                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 164, in get_councillors\n    container = self.get_list_container()\n  File \"scrapers/THE-three-rivers/councillors.py\", line 15, in get_list_container\n    return soup.find(\"h3\", text=\"District Councillor\").findNext(\"ul\")\nAttributeError: 'NoneType' object has no attribute 'findNext'\n","start":"2024-04-16 09:14:13.979292","end":"2024-04-16 09:14:17.524773","duration":3}},{"council_id":"WLN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:36:42] Fetching Scraper for: WLN                              handlers.py:23\n           Begin attempting to scrape: WLN                        handlers.py:27\n[08:36:43] Deleting existing data...                                 base.py:251\n           Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:36:44] ...data deleted.                                          base.py:258\n           Scraping from https://westlothian.gov.uk/councillors       base.py:41\n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n[08:36:45] Finished attempting to scrape: WLN                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 56, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/WLN-west-lothian/councillors.py\", line 12, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 155, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 142, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-04-16 08:36:42.793622","end":"2024-04-16 08:36:45.078091","duration":2}},{"council_id":"WOC","missing":false,"latest_run":{"status_code":1,"log_text":"[10:03:55] Fetching Scraper for: WOC                              handlers.py:23\n           Begin attempting to scrape: WOC                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[10:03:56] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[10:03:57] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://committee.worcester.gov.uk/mgWebService.asmx/GetCou           \n           ncillorsByWard                                                       \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: WOC                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-04-16 10:03:55.532817","end":"2024-04-16 10:03:57.809795","duration":2}},{"council_id":"WYC","missing":false,"latest_run":{"status_code":1,"log_text":"[08:18:34] Fetching Scraper for: WYC                              handlers.py:23\n           Begin attempting to scrape: WYC                        handlers.py:27\n           Deleting existing data...                                 base.py:251\n[08:18:35] Getting all files in Councillors...                       base.py:203\n           ...found 1 files in Councillors                           base.py:219\n           Deleting batch no. 1 consisting of 1 files                base.py:230\n[08:18:36] ...data deleted.                                          base.py:258\n           Scraping from                                              base.py:41\n           http://mgov.wychavon.gov.uk/mgWebService.asmx/GetCouncillo           \n           rsByWard                                                             \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: WYC                        base.py:339\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 845, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 470, in increment\n    raise reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/util/util.py\", line 38, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 791, in urlopen\n    response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 492, in _make_request\n    raise new_e\n  File \"/opt/python/urllib3/connectionpool.py\", line 468, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1097, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 642, in connect\n    sock_and_verified = _ssl_wrap_socket_and_match_hostname(\n  File \"/opt/python/urllib3/connection.py\", line 783, in _ssl_wrap_socket_and_match_hostname\n    ssl_sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 471, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(sock, context, tls_in_tls, server_hostname)\n  File \"/opt/python/urllib3/util/ssl_.py\", line 515, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.10/ssl.py\", line 513, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1104, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.10/ssl.py\", line 1375, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 200, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 219, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 47, in get\n    response = self.requests_session.get(\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2024-04-16 08:18:34.342537","end":"2024-04-16 08:18:36.646585","duration":2}}]
