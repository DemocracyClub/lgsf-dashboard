[{"council_id":"AGB","missing":false,"latest_run":{"status_code":1,"log_text":"[10:04:38] Fetching Scraper for: AGB                              handlers.py:23\n           Begin attempting to scrape: AGB                        handlers.py:27\n[10:04:39] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n[10:04:40] ...found 16 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 16 files in Councillors/raw                      base.py:207\n           ...found 33 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 33 files               base.py:216\n[10:04:41] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillor_list                       \n[10:04:44] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/argyll-and-bute-council/cou           \n           ncillors-directory/south-kintyre/councillor/john-armour              \n[10:04:45] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/argyll-and-bute-council/cou           \n           ncillors-directory/cowal/councillor/gordon-blair                     \n[10:04:47] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/mid-argyll/councillor/jan-brown                                \n[10:04:48] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/helensburgh-and-lomond-south/councillor/math-campbel           \n           l-sturgess                                                           \n[10:04:49] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/argyll-and-bute-council/cou           \n           ncillors-directory/mid-argyll/councillor/garret-corner               \n[10:04:50] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/lomond-north/councillor/maurice-corry                          \n[10:04:51] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/kintyre-and-islands/councillor/robin-currie                    \n[10:04:52] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/dunoon/councillor/audrey-e-forrest                             \n[10:04:54] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/oban-north-and-lorn/councillor/kieron-green                    \n[10:04:55] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/oban-south-and-isles/councillor/amanda-hampsey                 \n[10:04:56] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/dunoon/councillor/daniel-hampsey                               \n[10:04:57] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/helensburgh-central/councillor/graham-archibald-hard           \n           ie                                                                   \n[10:04:58] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillor_list?page=1                \n[10:05:00] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/helensburgh-central/councillor/fiona-howard                    \n[10:05:02] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/oban-south-and-isles/councillor/willie-hume                    \n[10:05:03] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/lomond-north/councillor/mark-irvine                            \n[10:05:05] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/oban-south-and-isles/councillor/andrew-kain                    \n[10:05:06] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/my-council/councillors-dire           \n           ctory/south-kintyre/councillor/jennifer-kelly                        \n[10:05:07] 'NoneType' object has no attribute 'img'               handlers.py:36\n           Committing batch 1 consisting of 32 files                 base.py:274\n[10:05:09] Finished attempting to scrape: AGB                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 52, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/AGB-argyll-and-bute/councillors.py\", line 37, in get_single_councillor\n    councillor.photo_url = soup.select_one(\".field--name-field-photo\").img[\"src\"]\nAttributeError: 'NoneType' object has no attribute 'img'\n","start":"2023-11-09 10:04:38.728115","end":"2023-11-09 10:05:09.038290","duration":30}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[09:10:09] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n[09:10:10] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[09:10:11] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.cne-siar.gov.uk/your-council/wards-and-council           \n           lors/council-members/                                                \n           HTTPSConnectionPool(host='www.cne-siar.gov.uk',        handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /your-council/wards-and-councillors/council-members/                 \n           (Caused by                                                           \n           NewConnectionError('<urllib3.connection.HTTPSConnectio               \n           n object at 0x7fc36550f8e0>: Failed to establish a new               \n           connection: [Errno -2] Name or service not known'))                  \n           Finished attempting to scrape: ELS                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 404, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1058, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 363, in connect\n    self.sock = conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc36550f8e0>: Failed to establish a new connection: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.cne-siar.gov.uk', port=443): Max retries exceeded with url: /your-council/wards-and-councillors/council-members/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc36550f8e0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 151, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 144, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 131, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='www.cne-siar.gov.uk', port=443): Max retries exceeded with url: /your-council/wards-and-councillors/council-members/ (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc36550f8e0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n","start":"2023-11-09 09:10:09.664079","end":"2023-11-09 09:10:11.836052","duration":2}},{"council_id":"HIN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:56:21] Fetching Scraper for: HIN                              handlers.py:23\n           Begin attempting to scrape: HIN                        handlers.py:27\n[08:56:22] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n[08:56:23] ...found 34 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 34 files in Councillors/raw                      base.py:207\n           ...found 69 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 69 files               base.py:216\n[08:56:24] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.hinckley-bosworth.gov.uk/mgWebService.asm           \n           x/GetCouncillorsByWard                                               \n[08:56:55] HTTPConnectionPool(host='moderngov.hinckley-bosworth.g handlers.py:36\n           ov.uk', port=80): Max retries exceeded with url:                     \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fc364ff2e80>: Failed to establish a new                 \n           connection: [Errno 111] Connection refused'))                        \n[08:56:56] Finished attempting to scrape: HIN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nConnectionRefusedError: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 416, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 244, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fc364ff2e80>: Failed to establish a new connection: [Errno 111] Connection refused\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='moderngov.hinckley-bosworth.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc364ff2e80>: Failed to establish a new connection: [Errno 111] Connection refused'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='moderngov.hinckley-bosworth.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc364ff2e80>: Failed to establish a new connection: [Errno 111] Connection refused'))\n","start":"2023-11-09 08:56:21.780698","end":"2023-11-09 08:56:56.246738","duration":34}},{"council_id":"MEL","missing":false,"latest_run":{"status_code":1,"log_text":"[08:25:51] Fetching Scraper for: MEL                              handlers.py:23\n           Begin attempting to scrape: MEL                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[08:25:52] Getting all files in Councillors/json...                  base.py:191\n           ...found 28 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 28 files in Councillors/raw                      base.py:207\n           ...found 57 files in Councillors                          base.py:207\n           Deleting batch no. 1 consisting of 57 files               base.py:216\n[08:25:53] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://democracy.melton.gov.uk/mgWebService.asmx/GetCounc           \n           illorsByWard                                                         \n[08:27:48] ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n[08:27:49] Finished attempting to scrape: MEL                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 404, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1058, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1073, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1342, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 404, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1058, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1073, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1342, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2023-11-09 08:25:51.104931","end":"2023-11-09 08:27:49.077352","duration":117}},{"council_id":"NGM","missing":false,"latest_run":{"status_code":1,"log_text":"[10:03:56] Fetching Scraper for: NGM                              handlers.py:23\n           Begin attempting to scrape: NGM                        handlers.py:27\n[10:03:57] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 55 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n[10:03:58] ...found 55 files in Councillors/raw                      base.py:207\n           ...found 111 files in Councillors                         base.py:207\n           Deleting batch no. 1 consisting of 100 files              base.py:216\n[10:03:59] Deleting batch no. 2 consisting of 11 files               base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://committee.nottinghamcity.gov.uk/mgWebService.asmx/G           \n           etCouncillorsByWard                                                  \n[10:04:12] ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: NGM                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 467, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 462, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1274, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1132, in read\n    return self._sslobj.read(len, buffer)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 467, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 462, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1274, in recv_into\n    return self.read(nbytes, buffer)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1132, in read\n    return self._sslobj.read(len, buffer)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2023-11-09 10:03:56.687068","end":"2023-11-09 10:04:13.000608","duration":16}},{"council_id":"OAD","missing":false,"latest_run":{"status_code":1,"log_text":"[10:30:18] Fetching Scraper for: OAD                              handlers.py:23\n           Begin attempting to scrape: OAD                        handlers.py:27\n[10:30:19] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[10:30:20] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.oadby-wigston.gov.uk/mgWebService.asmx/Ge           \n           tCouncillorsByWard                                                   \n[10:30:21] ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: OAD                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 467, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 462, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 467, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 462, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2023-11-09 10:30:18.960722","end":"2023-11-09 10:30:21.785348","duration":2}},{"council_id":"ORK","missing":false,"latest_run":{"status_code":1,"log_text":"[09:24:00] Fetching Scraper for: ORK                              handlers.py:23\n           Begin attempting to scrape: ORK                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[09:24:01] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[09:24:02] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.orkney.gov.uk/Council/Councillors/councillor-p           \n           rofiles.htm                                                          \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.orkney.gov.uk/Council/Councillors/councill               \n           or-profiles.htm                                                      \n           Finished attempting to scrape: ORK                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 151, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 144, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 131, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.orkney.gov.uk/Council/Councillors/councillor-profiles.htm\n","start":"2023-11-09 09:24:00.611523","end":"2023-11-09 09:24:02.932748","duration":2}},{"council_id":"OXF","missing":false,"latest_run":{"status_code":1,"log_text":"[09:55:32] Fetching Scraper for: OXF                              handlers.py:23\n           Begin attempting to scrape: OXF                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[09:55:33] Getting all files in Councillors...                       base.py:191\n           Getting all files in Councillors/json...                  base.py:191\n           ...found 63 files in Councillors/json                     base.py:207\n           Getting all files in Councillors/raw...                   base.py:191\n           ...found 63 files in Councillors/raw                      base.py:207\n           ...found 127 files in Councillors                         base.py:207\n           Deleting batch no. 1 consisting of 100 files              base.py:216\n[09:55:34] Deleting batch no. 2 consisting of 27 files               base.py:216\n[09:55:35] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://mycouncil.oxfordshire.gov.uk/mgWebService.asmx/Get           \n           CouncillorsByWard                                                    \n[09:55:45] HTTPSConnectionPool(host='mycouncil.oxfordshire.gov.uk handlers.py:36\n           ', port=443): Max retries exceeded with url:                         \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPSConnectio               \n           n object at 0x7fc3654129a0>: Failed to establish a new               \n           connection: [Errno -2] Name or service not known'))                  \n           Finished attempting to scrape: OXF                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 404, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1058, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 363, in connect\n    self.sock = conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fc3654129a0>: Failed to establish a new connection: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mycouncil.oxfordshire.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc3654129a0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='mycouncil.oxfordshire.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fc3654129a0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n","start":"2023-11-09 09:55:32.273130","end":"2023-11-09 09:55:45.845427","duration":13}},{"council_id":"SFT","missing":false,"latest_run":{"status_code":1,"log_text":"[10:07:37] Fetching Scraper for: SFT                              handlers.py:23\n           Begin attempting to scrape: SFT                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[10:07:38] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://modgov.sefton.gov.uk/mgWebService.asmx/GetCouncillo           \n           rsByWard                                                             \n[10:07:39] HTTPSConnectionPool(host='modgov.sefton.gov.uk',       handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: SFT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 404, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1058, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 419, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1073, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1342, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='modgov.sefton.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 725, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 725, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 266, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='modgov.sefton.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2023-11-09 10:07:37.016583","end":"2023-11-09 10:07:39.421585","duration":2}},{"council_id":"SHE","missing":false,"latest_run":{"status_code":null,"log_text":"[11:28:20] Fetching Scraper for: SHE                              handlers.py:22\n           Begin attempting to scrape: SHE                        handlers.py:25\n           Deleting existing data...                                 base.py:234\n           Getting all files in SHE...                               base.py:186\n[11:28:21] Getting all files in SHE/json...                          base.py:186\n           ...found 30 files in SHE/json                             base.py:202\n           Getting all files in SHE/raw...                           base.py:186\n           ...found 30 files in SHE/raw                              base.py:202\n           ...found 61 files in SHE                                  base.py:202\n           Deleting batch no. 1 consisting of 61 files               base.py:211\n[11:28:32] An error occurred (ThrottlingException) when calling   handlers.py:34\n           the CreateCommit operation (reached max retries: 4):                 \n           Rate exceeded                                                        \n           Finished attempting to scrape: SHE                        base.py:319\n","errors":"An error occurred (ThrottlingException) when calling the CreateCommit operation (reached max retries: 4): Rate exceeded","start":"2022-04-04 11:28:20.509898","end":"2022-04-04 11:28:32.871624","duration":12}},{"council_id":"SHN","missing":false,"latest_run":{"status_code":1,"log_text":"[08:54:38] Fetching Scraper for: SHN                              handlers.py:23\n           Begin attempting to scrape: SHN                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n[08:54:39] ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n           ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://moderngov.sthelens.gov.uk/mgWebService.asmx/GetCoun           \n           cillorsByWard                                                        \n[08:54:43] HTTPConnectionPool(host='moderngov.sthelens.gov.uk',   handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7fc36588e400>: Failed to establish a new                 \n           connection: [Errno 113] No route to host'))                          \n           Finished attempting to scrape: SHN                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nOSError: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 715, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 416, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 244, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7fc36588e400>: Failed to establish a new connection: [Errno 113] No route to host\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 486, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 799, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='moderngov.sthelens.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc36588e400>: Failed to establish a new connection: [Errno 113] No route to host'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 182, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 199, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 602, in get\n    return self.request(\"GET\", url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 589, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 703, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='moderngov.sthelens.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fc36588e400>: Failed to establish a new connection: [Errno 113] No route to host'))\n","start":"2023-11-09 08:54:38.190384","end":"2023-11-09 08:54:43.385892","duration":5}},{"council_id":"SNO","missing":false,"latest_run":{"status_code":1,"log_text":"[10:12:58] Fetching Scraper for: SNO                              handlers.py:23\n           Begin attempting to scrape: SNO                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[10:12:59] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[10:13:00] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.southnorfolkandbroadland.gov.uk/directory/3/so           \n           uth-norfolk-councillor-directory/category/11                         \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.southnorfolkandbroadland.gov.uk/directory/               \n           3/south-norfolk-councillor-directory/category/11                     \n           Finished attempting to scrape: SNO                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 151, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 144, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 131, in get_page\n    page = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.southnorfolkandbroadland.gov.uk/directory/3/south-norfolk-councillor-directory/category/11\n","start":"2023-11-09 10:12:58.576916","end":"2023-11-09 10:13:00.893343","duration":2}},{"council_id":"SST","missing":false,"latest_run":{"status_code":1,"log_text":"[08:35:18] Fetching Scraper for: SST                              handlers.py:23\n           Begin attempting to scrape: SST                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[08:35:19] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[08:35:20] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://services.sstaffs.gov.uk/cmis/Councillors.aspx                \n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://services.sstaffs.gov.uk/cmis/Councillors.aspx                \n           Finished attempting to scrape: SST                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 251, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://services.sstaffs.gov.uk/cmis/Councillors.aspx\n","start":"2023-11-09 08:35:18.152574","end":"2023-11-09 08:35:20.586325","duration":2}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[08:25:43] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[08:25:44] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[08:25:45] ...data deleted.                                          base.py:246\n           Scraping from https://www.stirling.gov.uk/councillors      base.py:42\n[08:25:47] list index out of range                                handlers.py:36\n           Finished attempting to scrape: STG                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 151, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 148, in get_list_container\n    return selected[0]\nIndexError: list index out of range\n","start":"2023-11-09 08:25:43.371127","end":"2023-11-09 08:25:47.580276","duration":4}},{"council_id":"TES","missing":false,"latest_run":{"status_code":1,"log_text":"[10:08:09] Fetching Scraper for: TES                              handlers.py:23\n           Begin attempting to scrape: TES                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[10:08:10] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[10:08:11] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepr           \n           esentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#           \n           MemberSectionA                                                       \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://testvalley.cmis.uk.com/testvalleypublic/Elected               \n           Representatives/tabid/63/ScreenMode/Alphabetical/Defau               \n           lt.aspx#MemberSectionA                                               \n           Finished attempting to scrape: TES                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 251, in get_councillors\n    req = self.get(\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 1021, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepresentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#MemberSectionA\n","start":"2023-11-09 10:08:09.463040","end":"2023-11-09 10:08:11.501422","duration":2}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[08:57:00] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n           Deleting existing data...                                 base.py:239\n[08:57:01] Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[08:57:02] ...data deleted.                                          base.py:246\n           Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/listing/councillors                   \n[08:57:04] 'NoneType' object has no attribute 'findNext'          handlers.py:36\n[08:57:05] Finished attempting to scrape: THE                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 151, in get_councillors\n    container = self.get_list_container()\n  File \"scrapers/THE-three-rivers/councillors.py\", line 13, in get_list_container\n    return soup.find(\"h3\", text=\"District Councillor\").findNext(\"ul\")\nAttributeError: 'NoneType' object has no attribute 'findNext'\n","start":"2023-11-09 08:57:00.624258","end":"2023-11-09 08:57:05.178967","duration":4}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[09:24:14] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n[09:24:15] Deleting existing data...                                 base.py:239\n           Getting all files in Councillors...                       base.py:191\n           ...found 1 files in Councillors                           base.py:207\n           Deleting batch no. 1 consisting of 1 files                base.py:216\n[09:24:16] ...data deleted.                                          base.py:246\n           Scraping from https://www.warrington.gov.uk/councillors    base.py:42\n[09:24:18] More than one element selected                         handlers.py:36\n[09:24:19] Finished attempting to scrape: WRT                        base.py:324\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 50, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 151, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 147, in get_list_container\n    raise ValueError(\"More than one element selected\")\nValueError: More than one element selected\n","start":"2023-11-09 09:24:14.727951","end":"2023-11-09 09:24:19.247221","duration":4}}]
