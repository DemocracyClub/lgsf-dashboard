[{"council_id":"AGB","missing":false,"latest_run":{"status_code":1,"log_text":"[11:44:09] Fetching Scraper for: AGB                              handlers.py:23\n           Begin attempting to scrape: AGB                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:44:10] Getting all files in AGB...                               base.py:182\n           Getting all files in AGB/json...                          base.py:182\n           ...found 8 files in AGB/json                              base.py:198\n           Getting all files in AGB/raw...                           base.py:182\n[11:44:11] ...found 8 files in AGB/raw                               base.py:198\n           ...found 17 files in AGB                                  base.py:198\n           Deleting batch no. 1 consisting of 17 files               base.py:207\n[11:44:12] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillor_list                       \n[11:44:14] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillors/yvonne-mcneilly           \n[11:44:15] Scraping from https://www.argyll-bute.gov.uk/councillors/w base.py:42\n           illiam-sinclair                                                      \n[11:44:16] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillors/gordon-blair              \n           Scraping from https://www.argyll-bute.gov.uk/councillors/a base.py:42\n           udrey-e-forrest                                                      \n[11:44:17] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillors/ross-moreland             \n           Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillors/daniel-hampsey            \n[11:44:18] Scraping from https://www.argyll-bute.gov.uk/councillors/m base.py:42\n           ath-campbell-sturgess                                                \n[11:44:19] Scraping from https://www.argyll-bute.gov.uk/councillors/p base.py:42\n           aul-donald-kennedy                                                   \n           list index out of range                                handlers.py:36\n           Committing batch 1 consisting of 14 files                 base.py:265\n[11:44:21] Finished attempting to scrape: AGB                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/AGB-argyll-and-bute/councillors.py\", line 33, in get_single_councillor\n    councillor.email = soup.select(\"a[href^=mailto]\")[0].get_text(strip=True)\nIndexError: list index out of range\n","start":"2022-05-30 11:44:09.713463","end":"2022-05-30 11:44:21.185030","duration":11}},{"council_id":"BIR","missing":false,"latest_run":{"status_code":1,"log_text":"[12:18:13] Fetching Scraper for: BIR                              handlers.py:23\n           Begin attempting to scrape: BIR                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:18:14] Getting all files in BIR...                               base.py:182\n           Getting all files in BIR/json...                          base.py:182\n           ...found 134 files in BIR/json                            base.py:198\n           Getting all files in BIR/raw...                           base.py:182\n           ...found 134 files in BIR/raw                             base.py:198\n           ...found 269 files in BIR                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[12:18:15] Deleting batch no. 2 consisting of 100 files              base.py:207\n[12:18:16] Deleting batch no. 3 consisting of 69 files               base.py:207\n[12:18:17] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://birmingham.cmis.uk.com/birmingham/Councillors.aspx           \n[12:18:20] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/122/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:18:22] Scraping from https://www.birmingham.gov.uk/councillors/1/ base.py:42\n           muhammad_afzal;                                                      \n[12:18:23] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/122/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:18:24] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1005/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:25] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1005/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:26] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/124/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:18:28] Scraping from https://www.birmingham.gov.uk/councillors/4/ base.py:42\n           mohammed_aikhlaq ;                                                   \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/124/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:18:30] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1006/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:31] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1006/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:32] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1007/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:34] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1007/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:35] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/80/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:18:38] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/5/deirdre_alden            \n           ;                                                                    \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/126/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:18:40] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/7/robert_alden ;           \n[12:18:42] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/24/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:18:43] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/9/tahir_ali ;              \n[12:18:44] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/24/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:18:45] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1008/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:46] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1008/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:48] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/73/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:18:49] Scraping from https://www.birmingham.gov.uk/councillors/11 base.py:42\n           /gurdial_singh_atwal ;                                               \n[12:18:50] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/127/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:18:51] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/12/mohammed_azim           \n           ;                                                                    \n[12:18:52] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1228/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:53] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1228/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1009/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:55] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1009/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:56] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1245/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:57] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1245/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:18:59] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1229/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:00] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1229/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:01] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/128/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:03] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/14/david_barrie            \n           ;                                                                    \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/39/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:05] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/15/bob_beauchamp           \n           ;                                                                    \n[12:19:06] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/39/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:08] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/180/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:10] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/16/matt_bennett            \n           ;                                                                    \n[12:19:11] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1240/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:12] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1240/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:13] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1238/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:14] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1238/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:15] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1251/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:16] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1251/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:17] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/939/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:20] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/17/kate_booth ;            \n[12:19:21] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/939/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:22] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/14/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:23] Scraping from https://www.birmingham.gov.uk/councillors/19 base.py:42\n           /sir_albert_bore ;                                                   \n[12:19:24] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1023/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:25] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1023/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:27] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1227/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:28] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1227/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:29] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/15/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:32] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/22/marje_bridle            \n           ;                                                                    \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1231/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:34] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1231/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:35] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/118/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:37] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/23/mick_brown ;            \n[12:19:38] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/93/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:41] Scraping from https://www.birmingham.gov.uk/councillors/27 base.py:42\n           /tristan_chatfield ;                                                 \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/93/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:43] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/130/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:44] Scraping from https://www.birmingham.gov.uk/councillors/28 base.py:42\n           /zaker_choudhry ;                                                    \n[12:19:45] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/182/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:47] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/29/debbie_clancy           \n           ;                                                                    \n[12:19:48] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/967/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:49] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/123/liz_clements           \n[12:19:50] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/30/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:51] Scraping from https://www.birmingham.gov.uk/councillors/33 base.py:42\n           /maureen_cornish ;                                                   \n[12:19:52] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/21/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:19:54] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/34/john_cotton ;           \n[12:19:55] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/132/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:19:56] Scraping from https://www.birmingham.gov.uk/councillors/37 base.py:42\n           /philip_davis_ba_ma ;                                                \n[12:19:57] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1223/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:58] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/1223/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:19:59] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/106/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:20:02] Scraping from http://www.birmingham.gov.uk/cs/Satellite?c= base.py:42\n           Page&childpagename=Member-Services%2FPageLayout&cid=122309           \n           2734506&pagename=BCC%2FCommon%2FWrapper%2FWrapper                    \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/106/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:20:03] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/940/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:20:05] Scraping from https://www.birmingham.gov.uk/councillors/38 base.py:42\n           /diane_donaldson ;                                                   \n           Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/38/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:20:07] Scraping from                                              base.py:42\n           https://www.birmingham.gov.uk/councillors/40/barbara_dring           \n           ;                                                                    \n[12:20:08] Scraping from https://birmingham.cmis.uk.com/birmingham/Co base.py:42\n           uncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/135/Scre           \n           enMode/Alphabetical/Default.aspx                                     \n[12:20:09] Scraping from https://www.birmingham.gov.uk/councillors/42 base.py:42\n           /mohammed_fazal ;                                                    \n[12:20:10] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 78 files                 base.py:265\n[12:20:11] Finished attempting to scrape: BIR                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"scrapers/BIR-birmingham/councillors.py\", line 45, in get_single_councillor\n    councillor = self.get_from_profile_page(profile_url)\n  File \"scrapers/BIR-birmingham/councillors.py\", line 28, in get_from_profile_page\n    councillor.email = soup.find(text=re.compile(\"Email:\")).next.getText(strip=True)\nAttributeError: 'NoneType' object has no attribute 'next'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/BIR-birmingham/councillors.py\", line 49, in get_single_councillor\n    councillor = super().get_single_councillor(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1486, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2022-05-30 12:18:13.330369","end":"2022-05-30 12:20:11.841483","duration":118}},{"council_id":"BOL","missing":false,"latest_run":{"status_code":1,"log_text":"[11:54:09] Fetching Scraper for: BOL                              handlers.py:23\n           Begin attempting to scrape: BOL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:54:10] Getting all files in BOL...                               base.py:182\n           ...found 1 files in BOL                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:54:11] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People.aspx                \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n           Finished attempting to scrape: BOL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2022-05-30 11:54:09.603832","end":"2022-05-30 11:54:11.628883","duration":2}},{"council_id":"COT","missing":false,"latest_run":{"status_code":1,"log_text":"[11:54:46] Fetching Scraper for: COT                              handlers.py:23\n           Begin attempting to scrape: COT                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:54:47] Getting all files in COT...                               base.py:182\n           ...found 1 files in COT                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:54:48] ...data deleted.                                          base.py:237\n           Scraping from http://www.cmis.cotswold.gov.uk/cmis5/People base.py:42\n           /tabid/62/ScreenMode/Alphabetical/Default.aspx                       \n           HTTPConnectionPool(host='www.cmis.cotswold.gov.uk',    handlers.py:36\n           port=80): Max retries exceeded with url: /cmis5/People               \n           /tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused               \n           by                                                                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f2a726e60a0>: Failed to establish a new                 \n           connection: [Errno -2] Name or service not known'))                  \n[11:54:49] Finished attempting to scrape: COT                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f2a726e60a0>: Failed to establish a new connection: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2a726e60a0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2a726e60a0>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n","start":"2022-05-30 11:54:46.728183","end":"2022-05-30 11:54:49.134303","duration":2}},{"council_id":"DEB","missing":false,"latest_run":{"status_code":1,"log_text":"[11:18:18] Fetching Scraper for: DEB                              handlers.py:23\n           Begin attempting to scrape: DEB                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:18:19] Getting all files in DEB...                               base.py:182\n           ...found 1 files in DEB                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:18:20] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://derbyshiredales.gov.uk/your-council/councillors              \n[11:18:21] 404 Client Error: Not Found for url: https://derbyshir handlers.py:36\n           edales.gov.uk/your-council/councillors                               \n           Finished attempting to scrape: DEB                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://derbyshiredales.gov.uk/your-council/councillors\n","start":"2022-05-30 11:18:18.714600","end":"2022-05-30 11:18:21.299987","duration":2}},{"council_id":"ELN","missing":false,"latest_run":{"status_code":1,"log_text":"[13:08:42] Fetching Scraper for: ELN                              handlers.py:23\n           Begin attempting to scrape: ELN                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in ELN...                               base.py:182\n[13:08:43] Getting all files in ELN/json...                          base.py:182\n           ...found 14 files in ELN/json                             base.py:198\n           Getting all files in ELN/raw...                           base.py:182\n           ...found 14 files in ELN/raw                              base.py:198\n           ...found 29 files in ELN                                  base.py:198\n           Deleting batch no. 1 consisting of 29 files               base.py:207\n[13:08:44] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.eastlothian.gov.uk/councillors/name                      \n[13:08:46] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0040/shamin_akhtar                                                   \n[13:08:47] Scraping from                                              base.py:42\n           https://www.eastlothian.gov.uk/councillors/10064/liz_allan           \n[13:08:48] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0066/ruaridh_bennett                                                 \n[13:08:49] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0041/lachlan_bruce                                                   \n[13:08:50] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0065/cher_cassini                                                    \n[13:08:51] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0072/donna_collins                                                   \n[13:08:52] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0043/fiona_dugdale                                                   \n[13:08:53] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0044/jeremy_findlay                                                  \n[13:08:54] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0045/andrew_forrest                                                  \n[13:08:55] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0046/neil_gilbert                                                    \n[13:08:56] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0048/norman_hampshire                                                \n[13:08:57] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0073/lyn_jardine                                                     \n[13:08:58] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0069/carol_mcfarlane                                                 \n[13:08:59] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0054/colin_mcginn                                                    \n[13:09:01] Scraping from https://www.eastlothian.gov.uk/councillors/1 base.py:42\n           0068/george_mcguire                                                  \n[13:09:03] 'NoneType' object has no attribute 'getText'           handlers.py:36\n           Committing batch 1 consisting of 28 files                 base.py:265\n[13:09:04] Finished attempting to scrape: ELN                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ELN-east-lothian/councillors.py\", line 41, in get_single_councillor\n    councillor.email = soup.select_one(\nAttributeError: 'NoneType' object has no attribute 'getText'\n","start":"2022-05-30 13:08:42.109526","end":"2022-05-30 13:09:04.569676","duration":22}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[13:39:13] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:39:14] Getting all files in ERW...                               base.py:182\n           ...found 1 files in ERW                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:39:15] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n[13:39:16] Scraping from https://www.eastrenfrewshire.gov.uk/councill base.py:42\n           or-angela-convery                                                    \n[13:39:17] 'NoneType' object is not subscriptable                 handlers.py:36\n           Finished attempting to scrape: ERW                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ERW-east-renfrewshire/councillors.py\", line 51, in get_single_councillor\n    contact_url = soup.select_one(\".panel__list--relarticles a.panel__link\")[\"href\"]\nTypeError: 'NoneType' object is not subscriptable\n","start":"2022-05-30 13:39:13.300359","end":"2022-05-30 13:39:17.556750","duration":4}},{"council_id":"ESS","missing":false,"latest_run":{"status_code":1,"log_text":"[13:43:58] Fetching Scraper for: ESS                              handlers.py:23\n           Begin attempting to scrape: ESS                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:43:59] Getting all files in ESS...                               base.py:182\n           ...found 1 files in ESS                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:44:00] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx               \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx               \n           Finished attempting to scrape: ESS                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx\n","start":"2022-05-30 13:43:58.711433","end":"2022-05-30 13:44:00.945764","duration":2}},{"council_id":"FIF","missing":false,"latest_run":{"status_code":1,"log_text":"[14:05:39] Fetching Scraper for: FIF                              handlers.py:23\n           Begin attempting to scrape: FIF                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[14:05:40] Getting all files in FIF...                               base.py:182\n           Getting all files in FIF/json...                          base.py:182\n[14:05:41] ...found 8 files in FIF/json                              base.py:198\n           Getting all files in FIF/raw...                           base.py:182\n           ...found 8 files in FIF/raw                               base.py:198\n           ...found 17 files in FIF                                  base.py:198\n           Deleting batch no. 1 consisting of 17 files               base.py:207\n[14:05:42] ...data deleted.                                          base.py:237\n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/                                                 \n[14:05:43] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages            \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr-ken-caldwell                                                    \n[14:05:44] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr-david-graham                                                    \n[14:05:45] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr-john-obrien                                                     \n[14:05:46] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr.-tom-adams                                                      \n[14:05:47] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy                                                                \n[14:05:49] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy/cllr-lesley-backhouse                                          \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy/cllr-kathleen-leslie                                           \n[14:05:50] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy/cllr.-julie-macdougall                                         \n[14:05:51] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cowdenbeath                                      \n[14:05:52] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cowdenbeath/cllr-alistair-bain                   \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cowdenbeath/cllr-alex-campbell                   \n[14:05:53] 'NoneType' object has no attribute 'find_parent'       handlers.py:36\n           Committing batch 1 consisting of 16 files                 base.py:265\n[14:05:57] Finished attempting to scrape: FIF                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/FIF-fife/councillors.py\", line 33, in get_single_councillor\n    soup.find(\"b\", text=re.compile(\"Ward:\"))\nAttributeError: 'NoneType' object has no attribute 'find_parent'\n","start":"2022-05-30 14:05:39.828446","end":"2022-05-30 14:05:57.759844","duration":17}},{"council_id":"HER","missing":false,"latest_run":{"status_code":1,"log_text":"[11:23:40] Fetching Scraper for: HER                              handlers.py:23\n           Begin attempting to scrape: HER                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:23:41] Getting all files in HER...                               base.py:182\n           ...found 1 files in HER                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:23:42] ...data deleted.                                          base.py:237\n           Scraping from https://www5.hertsmere.gov.uk/democracy//mgW base.py:42\n           ebService.asmx/GetCouncillorsByWard                                  \n[11:23:43] 404 Client Error: Not Found for url: https://www5.hert handlers.py:36\n           smere.gov.uk/democracy//mgWebService.asmx/GetCouncillo               \n           rsByWard                                                             \n           Finished attempting to scrape: HER                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www5.hertsmere.gov.uk/democracy//mgWebService.asmx/GetCouncillorsByWard\n","start":"2022-05-30 11:23:40.634293","end":"2022-05-30 11:23:43.210975","duration":2}},{"council_id":"MOL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:35:53] Fetching Scraper for: MOL                              handlers.py:23\n           Begin attempting to scrape: MOL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[14:35:54] Getting all files in MOL...                               base.py:182\n           Getting all files in MOL/json...                          base.py:182\n           ...found 19 files in MOL/json                             base.py:198\n           Getting all files in MOL/raw...                           base.py:182\n[14:35:55] ...found 19 files in MOL/raw                              base.py:198\n           ...found 39 files in MOL                                  base.py:198\n           Deleting batch no. 1 consisting of 39 files               base.py:207\n           ...data deleted.                                          base.py:237\n           Scraping from https://www.molevalley.gov.uk/home/council/c base.py:42\n           ouncillors/who-are-your-councillors                                  \n[14:35:58] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-david-hawksworth-cbe                                               \n[14:35:59] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-patricia-wiltshire                                                 \n[14:36:01] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-david-harper           \n[14:36:05] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-garry-stansfield                                                   \n[14:36:07] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-mary-cooper            \n[14:36:12] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-chris-hunt             \n[14:36:13] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-alan-reilly            \n[14:36:15] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-caroline-salmon                                                    \n[14:36:17] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-sarah-chambers                                                     \n[14:36:19] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-monica-weller                                                      \n[14:36:21] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-roger-adams            \n[14:36:22] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-james-chambers                                                     \n[14:36:24] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-elizabeth-daly                                                     \n[14:36:25] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-nancy-goodacre                                                     \n[14:36:27] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-david-preedy           \n[14:36:28] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-paul-potter            \n[14:36:30] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/cllr-simon-budd             \n[14:36:31] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-charles-engel                                                      \n[14:36:33] Scraping from https://www.molevalley.gov.uk/councillor/cll base.py:42\n           r-lesley-bushnell                                                    \n[14:36:34] Scraping from                                              base.py:42\n           https://www.molevalley.gov.uk/councillor/vacancy                     \n[14:36:36] 'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Committing batch 1 consisting of 38 files                 base.py:265\n[14:36:37] Finished attempting to scrape: MOL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/MOL-mole-valley/councillors.py\", line 30, in get_single_councillor\n    soup.select_one(\".field--name-field-political-party\")\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2022-05-30 14:35:53.658901","end":"2022-05-30 14:36:37.901571","duration":44}},{"council_id":"NEL","missing":false,"latest_run":{"status_code":1,"log_text":"[12:47:41] Fetching Scraper for: NEL                              handlers.py:23\n           Begin attempting to scrape: NEL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:47:42] Getting all files in NEL...                               base.py:182\n           ...found 1 files in NEL                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:47:43] ...data deleted.                                          base.py:237\n           Scraping from https://www.nelincs.gov.uk/your-council/coun base.py:42\n           cillors-mps-and-meps/find-your-councillor/councillors-by-p           \n           arty/                                                                \n[12:47:47] More than one element selected                         handlers.py:36\n           Finished attempting to scrape: NEL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 137, in get_list_container\n    raise ValueError(\"More than one element selected\")\nValueError: More than one element selected\n","start":"2022-05-30 12:47:41.359312","end":"2022-05-30 12:47:47.735278","duration":6}},{"council_id":"NFK","missing":false,"latest_run":{"status_code":1,"log_text":"[11:58:12] Fetching Scraper for: NFK                              handlers.py:23\n           Begin attempting to scrape: NFK                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:58:13] Getting all files in NFK...                               base.py:182\n           ...found 1 files in NFK                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:58:14] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://norfolkcc.cmis.uk.com/norfolkcc/Councillors.aspx             \n[11:58:16] 'NoneType' object has no attribute 'next'              handlers.py:36\n           Finished attempting to scrape: NFK                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 258, in get_single_councillor\n    division = list_page_html.find(text=self.division_text).next.strip()\nAttributeError: 'NoneType' object has no attribute 'next'\n","start":"2022-05-30 11:58:12.275777","end":"2022-05-30 11:58:16.791671","duration":4}},{"council_id":"NNO","missing":false,"latest_run":{"status_code":1,"log_text":"[12:03:26] Fetching Scraper for: NNO                              handlers.py:23\n           Begin attempting to scrape: NNO                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in NNO...                               base.py:182\n[12:03:27] ...found 1 files in NNO                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:03:28] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.north-norfolk.gov.uk/members/#filter-form                \n           HTTPSConnectionPool(host='modgov.north-norfolk.gov.uk' handlers.py:36\n           , port=443): Max retries exceeded with url:                          \n           /mgMemberIndex.aspx?bcr=1 (Caused by                                 \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: NNO                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='modgov.north-norfolk.gov.uk', port=443): Max retries exceeded with url: /mgMemberIndex.aspx?bcr=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/NNO-north-norfolk/councillors.py\", line 15, in get_councillors\n    return super().get_councillors()[1:]\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 667, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 667, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 237, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='modgov.north-norfolk.gov.uk', port=443): Max retries exceeded with url: /mgMemberIndex.aspx?bcr=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2022-05-30 12:03:26.037065","end":"2022-05-30 12:03:28.823835","duration":2}},{"council_id":"OAD","missing":false,"latest_run":{"status_code":1,"log_text":"[11:16:33] Fetching Scraper for: OAD                              handlers.py:23\n           Begin attempting to scrape: OAD                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:16:34] Getting all files in OAD...                               base.py:182\n           ...found 1 files in OAD                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:16:35] ...data deleted.                                          base.py:237\n           Scraping from http://moderngov.oadby-wigston.gov.uk/mgWebS base.py:42\n           ervice.asmx/GetCouncillorsByWard                                     \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n[11:16:36] Finished attempting to scrape: OAD                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 449, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 444, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 449, in _make_request\n    six.raise_from(e, None)\n  File \"<string>\", line 3, in raise_from\n  File \"/opt/python/urllib3/connectionpool.py\", line 444, in _make_request\n    httplib_response = conn.getresponse()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1348, in getresponse\n    response.begin()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 316, in begin\n    version, status, reason = self._read_status()\n  File \"/var/lang/lib/python3.8/http/client.py\", line 277, in _read_status\n    line = str(self.fp.readline(_MAXLINE + 1), \"iso-8859-1\")\n  File \"/var/lang/lib/python3.8/socket.py\", line 669, in readinto\n    return self._sock.recv_into(b)\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2022-05-30 11:16:33.442315","end":"2022-05-30 11:16:36.286485","duration":2}},{"council_id":"ORK","missing":false,"latest_run":{"status_code":1,"log_text":"[12:00:02] Fetching Scraper for: ORK                              handlers.py:23\n           Begin attempting to scrape: ORK                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in ORK...                               base.py:182\n[12:00:03] ...found 1 files in ORK                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:00:05] ...data deleted.                                          base.py:237\n           Scraping from https://www.orkney.gov.uk/Council/Councillor base.py:42\n           s/councillor-profiles.htm                                            \n[12:00:06] 404 Client Error: Not Found for url: https://www.orkne handlers.py:36\n           y.gov.uk/Council/Councillors/councillor-profiles.htm                 \n           Finished attempting to scrape: ORK                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.orkney.gov.uk/Council/Councillors/councillor-profiles.htm\n","start":"2022-05-30 12:00:02.181299","end":"2022-05-30 12:00:06.346152","duration":4}},{"council_id":"PEN","missing":false,"latest_run":{"status_code":1,"log_text":"[11:55:40] Fetching Scraper for: PEN                              handlers.py:23\n           Begin attempting to scrape: PEN                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:55:41] Getting all files in PEN...                               base.py:182\n           Getting all files in PEN/json...                          base.py:182\n           ...found 3 files in PEN/json                              base.py:198\n           Getting all files in PEN/raw...                           base.py:182\n[11:55:42] ...found 3 files in PEN/raw                               base.py:198\n           ...found 7 files in PEN                                   base.py:198\n           Deleting batch no. 1 consisting of 7 files                base.py:207\n[11:55:43] ...data deleted.                                          base.py:237\n           Scraping from https://www.pendle.gov.uk/councillors/name   base.py:42\n[11:55:44] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/76/mohammed_adnan              \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/83/faraz_ahmad                 \n[11:55:45] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/2/nadeem_ahmed                 \n[11:55:46] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/91/sajjad_ahmed                \n           'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 6 files                  base.py:265\n[11:55:48] Finished attempting to scrape: PEN                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/PEN-pendle/councillors.py\", line 48, in get_single_councillor\n    councillor.email = soup.select_one(\"li a[href^=mailto]\")[\"href\"].replace(\nTypeError: 'NoneType' object is not subscriptable\n","start":"2022-05-30 11:55:40.712387","end":"2022-05-30 11:55:48.345766","duration":7}},{"council_id":"SFK","missing":false,"latest_run":{"status_code":1,"log_text":"[12:05:22] Fetching Scraper for: SFK                              handlers.py:23\n           Begin attempting to scrape: SFK                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:05:23] Getting all files in SFK...                               base.py:182\n           Getting all files in SFK/json...                          base.py:182\n[12:05:24] ...found 75 files in SFK/json                             base.py:198\n           Getting all files in SFK/raw...                           base.py:182\n           ...found 75 files in SFK/raw                              base.py:198\n           ...found 151 files in SFK                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[12:05:25] Deleting batch no. 2 consisting of 51 files               base.py:207\n[12:05:26] ...data deleted.                                          base.py:237\n           Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/?ward=&action=SEARCH&party=&name=                            \n[12:05:32] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/alexander-nicoll/                                            \n[12:05:35] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/andrew-reid/                                                 \n[12:05:38] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/andrew-stringer/                                             \n[12:05:41] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/andy-drummond/                                               \n[12:05:45] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/andy-mellen/                                                 \n[12:05:48] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/annette-dunning/                                             \n[12:05:51] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/beccy-hopfensperger/                                         \n[12:05:54] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/bill-quinton/                                                \n[12:05:57] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/bobby-bennett/                                               \n[12:06:00] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/caroline-page/                                               \n[12:06:03] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/caroline-topping/                                            \n[12:06:06] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/christopher-hudson/                                          \n[12:06:10] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/colin-noble/                                                 \n[12:06:13] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/craig-rivett/                                                \n[12:06:16] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/chris-chambers/                                              \n[12:06:19] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/david-goldsmith/                                             \n[12:06:22] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/debbie-mccallum/                                             \n[12:06:25] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/david-nettleton/                                             \n[12:06:28] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/david-roach/                                                 \n[12:06:32] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/debbie-richards/                                             \n[12:06:35] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/elaine-bryce/                                                \n[12:06:38] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/elizabeth-johnson/                                           \n[12:06:41] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/edward-back/                                                 \n[12:06:44] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/georgia-hall/                                                \n[12:06:47] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/graham-newman/                                               \n[12:06:50] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/heike-sowa/                                                  \n[12:06:53] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/inga-lockington/                                             \n[12:06:57] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/judy-cloke/                                                  \n[12:07:00] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/jessie-carter/                                               \n[12:07:03] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/james-finch/                                                 \n[12:07:06] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/james-reeder/                                                \n[12:07:09] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/jamie-starling/                                              \n[12:07:12] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/jenny-ceresa/                                                \n[12:07:15] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/jessica-fleming/                                             \n[12:07:18] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/joe-mason/                                                   \n[12:07:22] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/joanna-spicer/                                               \n[12:07:25] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/karen-soons/                                                 \n[12:07:28] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/kay-oakes/                                                   \n[12:07:32] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/keith-welham/                                                \n[12:07:35] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/keith-robinson/                                              \n[12:07:38] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/keith-scarff/                                                \n[12:07:41] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/lance-stanbury/                                              \n[12:07:44] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/liz-harsant/                                                 \n[12:07:48] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/matthew-hicks/                                               \n[12:07:51] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/melanie-vigo-di-gallidoro/                                   \n[12:07:54] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/michael-ladd/                                                \n[12:07:57] Committing batch 1 consisting of 92 files                 base.py:265\n[12:07:59] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/mick-fraser/                                                 \n[12:08:02] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/nadia-cenci/                                                 \n[12:08:05] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/nathan-wilson/                                               \n[12:08:07] Scraping from https://www.suffolk.gov.uk/council-and-democ base.py:42\n           racy/councillors-and-elected-representatives/find-your-cou           \n           ncillor/patti-mulcahy/                                               \n           502 Server Error: Bad Gateway for url: https://www.suf handlers.py:36\n           folk.gov.uk/council-and-democracy/councillors-and-elec               \n           ted-representatives/find-your-councillor/patti-mulcahy               \n           /                                                                    \n           Committing batch 2 consisting of 6 files                  base.py:265\n[12:08:09] Finished attempting to scrape: SFK                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/SFK-suffolk/councillors.py\", line 32, in get_single_councillor\n    req = self.get(url)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 502 Server Error: Bad Gateway for url: https://www.suffolk.gov.uk/council-and-democracy/councillors-and-elected-representatives/find-your-councillor/patti-mulcahy/\n","start":"2022-05-30 12:05:22.805188","end":"2022-05-30 12:08:09.927550","duration":167}},{"council_id":"SHO","missing":false,"latest_run":{"status_code":1,"log_text":"[11:17:49] Fetching Scraper for: SHO                              handlers.py:23\n           Begin attempting to scrape: SHO                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:17:50] Getting all files in SHO...                               base.py:182\n           ...found 1 files in SHO                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:17:51] ...data deleted.                                          base.py:237\n           Scraping from https://democracy.sholland.gov.uk/mgWebServi base.py:42\n           ce.asmx/GetCouncillorsByWard                                         \n[11:17:57] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://democracy.sholland.gov.uk/mgError.aspx                       \n[11:17:58] Finished attempting to scrape: SHO                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://democracy.sholland.gov.uk/mgError.aspx\n","start":"2022-05-30 11:17:49.338330","end":"2022-05-30 11:17:58.117941","duration":8}},{"council_id":"STS","missing":false,"latest_run":{"status_code":1,"log_text":"[12:10:03] Fetching Scraper for: STS                              handlers.py:23\n           Begin attempting to scrape: STS                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:10:04] Getting all files in STS...                               base.py:182\n           ...found 1 files in STS                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:10:05] ...data deleted.                                          base.py:237\n           Scraping from http://moderngov.staffordshire.gov.uk//mgWeb base.py:42\n           Service.asmx/GetCouncillorsByWard                                    \n[12:12:14] HTTPConnectionPool(host='moderngov.staffordshire.gov.u handlers.py:36\n           k', port=80): Max retries exceeded with url:                         \n           //mgWebService.asmx/GetCouncillorsByWard (Caused by                  \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f2a7307cfa0>: Failed to establish a new                 \n           connection: [Errno 110] Connection timed out'))                      \n[12:12:15] Finished attempting to scrape: STS                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f2a7307cfa0>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='moderngov.staffordshire.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2a7307cfa0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='moderngov.staffordshire.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f2a7307cfa0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2022-05-30 12:10:03.656984","end":"2022-05-30 12:12:15.351618","duration":131}},{"council_id":"TEN","missing":false,"latest_run":{"status_code":1,"log_text":"[12:25:17] Fetching Scraper for: TEN                              handlers.py:23\n           Begin attempting to scrape: TEN                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:25:18] Getting all files in TEN...                               base.py:182\n           ...found 1 files in TEN                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:25:19] ...data deleted.                                          base.py:237\n           Scraping from http://tdcdemocracy.tendringdc.gov.uk/mgWebS base.py:42\n           ervice.asmx/GetCouncillorsByWard                                     \n           HTTPSConnectionPool(host='tdcdemocracy.tendringdc.gov. handlers.py:36\n           uk', port=443): Max retries exceeded with url:                       \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: TEN                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='tdcdemocracy.tendringdc.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 667, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 667, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 237, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='tdcdemocracy.tendringdc.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2022-05-30 12:25:17.554375","end":"2022-05-30 12:25:19.967001","duration":2}},{"council_id":"TES","missing":false,"latest_run":{"status_code":1,"log_text":"[11:48:45] Fetching Scraper for: TES                              handlers.py:23\n           Begin attempting to scrape: TES                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in TES...                               base.py:182\n           ...found 1 files in TES                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:48:46] ...data deleted.                                          base.py:237\n           Scraping from http://testvalley.cmis.uk.com/testvalleypubl base.py:42\n           ic/ElectedRepresentatives/tabid/63/ScreenMode/Alphabetical           \n           /Default.aspx#MemberSectionA                                         \n           404 Client Error: Not Found for url: http://testvalley handlers.py:36\n           .cmis.uk.com/testvalleypublic/ElectedRepresentatives/t               \n           abid/63/ScreenMode/Alphabetical/Default.aspx#MemberSec               \n           tionA                                                                \n[11:48:47] Finished attempting to scrape: TES                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepresentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#MemberSectionA\n","start":"2022-05-30 11:48:45.042175","end":"2022-05-30 11:48:47.158427","duration":2}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[11:33:37] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:33:38] Getting all files in THE...                               base.py:182\n           Getting all files in THE/json...                          base.py:182\n           ...found 6 files in THE/json                              base.py:198\n           Getting all files in THE/raw...                           base.py:182\n           ...found 6 files in THE/raw                               base.py:198\n           ...found 13 files in THE                                  base.py:198\n           Deleting batch no. 1 consisting of 13 files               base.py:207\n[11:33:39] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/listing/councillors                   \n[11:33:40] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/matthew-bedford            \n[11:33:41] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/sara-bedford               \n[11:33:42] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/ruth-clark                 \n[11:33:43] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/david-coltman              \n[11:33:45] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/stephen-cox                \n[11:33:46] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/steve-drury                \n[11:33:47] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/andrea-fraser              \n[11:33:48] list index out of range                                handlers.py:36\n           Committing batch 1 consisting of 12 files                 base.py:265\n[11:33:49] Finished attempting to scrape: THE                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/THE-three-rivers/councillors.py\", line 30, in get_single_councillor\n    councillor.email = soup.select(\"a[href^=mailto]\")[0].get_text(strip=True)\nIndexError: list index out of range\n","start":"2022-05-30 11:33:37.292382","end":"2022-05-30 11:33:49.828076","duration":12}},{"council_id":"VGL","missing":false,"latest_run":{"status_code":1,"log_text":"[13:11:57] Fetching Scraper for: VGL                              handlers.py:23\n           Begin attempting to scrape: VGL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:11:58] Getting all files in VGL...                               base.py:182\n           ...found 1 files in VGL                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:11:59] ...data deleted.                                          base.py:237\n           Scraping from https://www.valeofglamorgan.gov.uk/en/our_co base.py:42\n           uncil/Council-Structure/councillors/Councillors.aspx                 \n           HTTPSConnectionPool(host='www.valeofglamorgan.gov.uk', handlers.py:36\n           port=443): Max retries exceeded with url: /en/our_coun               \n           cil/Council-Structure/councillors/Councillors.aspx                   \n           (Caused by SSLError(SSLCertVerificationError(1, '[SSL:               \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: VGL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.valeofglamorgan.gov.uk', port=443): Max retries exceeded with url: /en/our_council/Council-Structure/councillors/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='www.valeofglamorgan.gov.uk', port=443): Max retries exceeded with url: /en/our_council/Council-Structure/councillors/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2022-05-30 13:11:57.760678","end":"2022-05-30 13:11:59.934572","duration":2}},{"council_id":"WAW","missing":false,"latest_run":{"status_code":1,"log_text":"[12:50:27] Fetching Scraper for: WAW                              handlers.py:23\n           Begin attempting to scrape: WAW                        handlers.py:27\n[12:50:28] Deleting existing data...                                 base.py:230\n           Getting all files in WAW...                               base.py:182\n           ...found 1 files in WAW                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:50:29] ...data deleted.                                          base.py:237\n           Scraping from https://estates4.warwickdc.gov.uk/cmis/Counc base.py:42\n           illorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx                     \n           HTTPSConnectionPool(host='estates4.warwickdc.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url: /cmis/Counci               \n           llorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx                      \n           (Caused by NewConnectionError('<urllib3.connection.HTT               \n           PSConnection object at 0x7f2a7269e100>: Failed to                    \n           establish a new connection: [Errno -5] No address                    \n           associated with hostname'))                                          \n[12:50:30] Finished attempting to scrape: WAW                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -5] No address associated with hostname\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 358, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f2a7269e100>: Failed to establish a new connection: [Errno -5] No address associated with hostname\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='estates4.warwickdc.gov.uk', port=443): Max retries exceeded with url: /cmis/CouncillorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2a7269e100>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='estates4.warwickdc.gov.uk', port=443): Max retries exceeded with url: /cmis/CouncillorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f2a7269e100>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n","start":"2022-05-30 12:50:27.934485","end":"2022-05-30 12:50:30.064859","duration":2}},{"council_id":"WEA","missing":false,"latest_run":{"status_code":1,"log_text":"[11:37:33] Fetching Scraper for: WEA                              handlers.py:23\n           Begin attempting to scrape: WEA                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:37:34] Getting all files in WEA...                               base.py:182\n           ...found 1 files in WEA                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:37:35] ...data deleted.                                          base.py:237\n           Scraping from http://council.wealden.gov.uk/mgWebService.a base.py:42\n           smx/GetCouncillorsByWard                                             \n[11:39:46] HTTPConnectionPool(host='council.wealden.gov.uk',      handlers.py:36\n           port=80): Max retries exceeded with url:                             \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f9be4659d60>: Failed to establish a new                 \n           connection: [Errno 110] Connection timed out'))                      \n[11:39:47] Finished attempting to scrape: WEA                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f9be4659d60>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='council.wealden.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9be4659d60>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='council.wealden.gov.uk', port=80): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9be4659d60>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2022-05-30 11:37:33.861786","end":"2022-05-30 11:39:47.441726","duration":133}},{"council_id":"WLL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:06:33] Fetching Scraper for: WLL                              handlers.py:23\n           Begin attempting to scrape: WLL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in WLL...                               base.py:182\n[14:06:34] Getting all files in WLL/json...                          base.py:182\n           ...found 60 files in WLL/json                             base.py:198\n           Getting all files in WLL/raw...                           base.py:182\n           ...found 60 files in WLL/raw                              base.py:198\n           ...found 121 files in WLL                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[14:06:35] Deleting batch no. 2 consisting of 21 files               base.py:207\n[14:06:36] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://cmispublic.walsall.gov.uk/cmis/Councillors.aspx              \n[14:08:45] HTTPSConnectionPool(host='cmispublic.walsall.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /cmis/Councillors.aspx (Caused by NewConnectionError('               \n           <urllib3.connection.HTTPSConnection object at                        \n           0x7fe8fee67be0>: Failed to establish a new connection:               \n           [Errno 110] Connection timed out'))                                  \n[14:08:46] Finished attempting to scrape: WLL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 358, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7fe8fee67be0>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cmispublic.walsall.gov.uk', port=443): Max retries exceeded with url: /cmis/Councillors.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe8fee67be0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='cmispublic.walsall.gov.uk', port=443): Max retries exceeded with url: /cmis/Councillors.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7fe8fee67be0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2022-05-30 14:06:33.233298","end":"2022-05-30 14:08:46.401682","duration":133}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[12:29:33] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n[12:29:34] Deleting existing data...                                 base.py:230\n           Getting all files in WRT...                               base.py:182\n           ...found 1 files in WRT                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:29:35] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.warrington.gov.uk/councillors/name                       \n[12:29:36] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.warrington.gov.uk/councillors/name                       \n           Finished attempting to scrape: WRT                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.warrington.gov.uk/councillors/name\n","start":"2022-05-30 12:29:33.926559","end":"2022-05-30 12:29:36.959754","duration":3}},{"council_id":"WYE","missing":false,"latest_run":{"status_code":1,"log_text":"[13:47:42] Fetching Scraper for: WYE                              handlers.py:23\n           Begin attempting to scrape: WYE                        handlers.py:27\n[13:47:43] Deleting existing data...                                 base.py:230\n           Getting all files in WYE...                               base.py:182\n           ...found 1 files in WYE                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:47:44] ...data deleted.                                          base.py:237\n           Scraping from http://www.wyreforestdc.gov.uk/the-council/c base.py:42\n           ouncillors-committees-and-meetings/your-district-councillo           \n           r.aspx                                                               \n[13:47:45] 404 Client Error: Not Found for url: https://www.wyref handlers.py:36\n           orestdc.gov.uk/the-council/councillors-committees-and-               \n           meetings/your-district-councillor.aspx                               \n           Finished attempting to scrape: WYE                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.wyreforestdc.gov.uk/the-council/councillors-committees-and-meetings/your-district-councillor.aspx\n","start":"2022-05-30 13:47:42.926486","end":"2022-05-30 13:47:45.379295","duration":2}}]
