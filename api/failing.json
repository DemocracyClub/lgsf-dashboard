[{"council_id":"AGB","missing":false,"latest_run":{"status_code":1,"log_text":"[11:38:34] Fetching Scraper for: AGB                              handlers.py:23\n           Begin attempting to scrape: AGB                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in AGB...                               base.py:182\n[11:38:35] Getting all files in AGB/json...                          base.py:182\n           ...found 2 files in AGB/json                              base.py:198\n           Getting all files in AGB/raw...                           base.py:182\n           ...found 2 files in AGB/raw                               base.py:198\n           ...found 5 files in AGB                                   base.py:198\n           Deleting batch no. 1 consisting of 5 files                base.py:207\n[11:38:36] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillor_list                       \n[11:38:39] Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillors/gordon-blair              \n           Scraping from                                              base.py:42\n           https://www.argyll-bute.gov.uk/councillors/yvonne-mcneilly           \n[11:38:40] Scraping from https://www.argyll-bute.gov.uk/councillors/w base.py:42\n           illiam-sinclair                                                      \n           list index out of range                                handlers.py:36\n           Committing batch 1 consisting of 4 files                  base.py:265\n[11:38:41] Finished attempting to scrape: AGB                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/AGB-argyll-and-bute/councillors.py\", line 33, in get_single_councillor\n    councillor.email = soup.select(\"a[href^=mailto]\")[0].get_text(strip=True)\nIndexError: list index out of range\n","start":"2022-05-09 11:38:34.147689","end":"2022-05-09 11:38:42.004745","duration":7}},{"council_id":"BOL","missing":false,"latest_run":{"status_code":1,"log_text":"[12:56:03] Fetching Scraper for: BOL                              handlers.py:23\n           Begin attempting to scrape: BOL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in BOL...                               base.py:182\n           ...found 1 files in BOL                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:56:04] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.democracy.bolton.gov.uk/cmis5/People.aspx                \n           ('Connection aborted.', ConnectionResetError(104,      handlers.py:36\n           'Connection reset by peer'))                                         \n[12:56:05] Finished attempting to scrape: BOL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nConnectionResetError: [Errno 104] Connection reset by peer\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 550, in increment\n    raise six.reraise(type(error), error, _stacktrace)\n  File \"/opt/python/urllib3/packages/six.py\", line 769, in reraise\n    raise value.with_traceback(tb)\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nurllib3.exceptions.ProtocolError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 501, in send\n    raise ConnectionError(err, request=request)\nrequests.exceptions.ConnectionError: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n","start":"2022-05-09 12:56:03.004290","end":"2022-05-09 12:56:05.222128","duration":2}},{"council_id":"COL","missing":false,"latest_run":{"status_code":1,"log_text":"[12:20:13] Fetching Scraper for: COL                              handlers.py:23\n           Begin attempting to scrape: COL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:20:14] Getting all files in COL...                               base.py:182\n           Getting all files in COL/json...                          base.py:182\n[12:20:15] ...found 50 files in COL/json                             base.py:198\n           Getting all files in COL/raw...                           base.py:182\n           ...found 50 files in COL/raw                              base.py:198\n           ...found 101 files in COL                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[12:20:16] Deleting batch no. 2 consisting of 1 files                base.py:207\n[12:20:17] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           http://colchester.cmis.uk.com/colchester/Councillors.aspx            \n[12:20:19] Scraping from http://colchester.cmis.uk.com/colchester/Cou base.py:42\n           ncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/111/Scree           \n           nMode/Alphabetical/Default.aspx                                      \n[12:20:20] Scraping from http://colchester.cmis.uk.com/colchester/Cou base.py:42\n           ncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/43/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[12:20:22] Scraping from http://colchester.cmis.uk.com/colchester/Cou base.py:42\n           ncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/18/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[12:20:25] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 6 files                  base.py:265\n[12:20:26] Finished attempting to scrape: COL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1486, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2022-05-09 12:20:13.850408","end":"2022-05-09 12:20:26.936024","duration":13}},{"council_id":"COT","missing":false,"latest_run":{"status_code":1,"log_text":"[14:21:01] Fetching Scraper for: COT                              handlers.py:23\n           Begin attempting to scrape: COT                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[14:21:02] Getting all files in COT...                               base.py:182\n           ...found 1 files in COT                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[14:21:03] ...data deleted.                                          base.py:237\n           Scraping from http://www.cmis.cotswold.gov.uk/cmis5/People base.py:42\n           /tabid/62/ScreenMode/Alphabetical/Default.aspx                       \n           HTTPConnectionPool(host='www.cmis.cotswold.gov.uk',    handlers.py:36\n           port=80): Max retries exceeded with url: /cmis5/People               \n           /tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused               \n           by                                                                   \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f7fb0442220>: Failed to establish a new                 \n           connection: [Errno -2] Name or service not known'))                  \n           Finished attempting to scrape: COT                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f7fb0442220>: Failed to establish a new connection: [Errno -2] Name or service not known\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7fb0442220>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='www.cmis.cotswold.gov.uk', port=80): Max retries exceeded with url: /cmis5/People/tabid/62/ScreenMode/Alphabetical/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7fb0442220>: Failed to establish a new connection: [Errno -2] Name or service not known'))\n","start":"2022-05-09 14:21:01.413582","end":"2022-05-09 14:21:03.821011","duration":2}},{"council_id":"ELS","missing":false,"latest_run":{"status_code":1,"log_text":"[12:53:20] Fetching Scraper for: ELS                              handlers.py:23\n           Begin attempting to scrape: ELS                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:53:21] Getting all files in ELS...                               base.py:182\n           ...found 1 files in ELS                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:53:22] ...data deleted.                                          base.py:237\n           Scraping from https://www.cne-siar.gov.uk/your-council/war base.py:42\n           ds-and-councillors/council-members/                                  \n[12:53:24] Scraping from https://www.cne-siar.gov.uk/your-council/war base.py:42\n           ds-and-councillors/council-members/kenneth-j-maclean/                \n[12:53:25] 'NoneType' object has no attribute 'getText'           handlers.py:36\n           Finished attempting to scrape: ELS                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/ELS-na-h-eileanan-an-iar/councillors.py\", line 42, in get_single_councillor\n    councillor.email = soup.select_one(\"article a[href^=mailto]\").getText(\nAttributeError: 'NoneType' object has no attribute 'getText'\n","start":"2022-05-09 12:53:20.294501","end":"2022-05-09 12:53:25.974195","duration":5}},{"council_id":"ERW","missing":false,"latest_run":{"status_code":1,"log_text":"[11:38:47] Fetching Scraper for: ERW                              handlers.py:23\n           Begin attempting to scrape: ERW                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:38:48] Getting all files in ERW...                               base.py:182\n           ...found 1 files in ERW                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:38:49] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n[11:38:50] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.eastrenfrewshire.gov.uk/Find-my-councillor               \n           Finished attempting to scrape: ERW                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.eastrenfrewshire.gov.uk/Find-my-councillor\n","start":"2022-05-09 11:38:47.703002","end":"2022-05-09 11:38:50.948322","duration":3}},{"council_id":"ESS","missing":false,"latest_run":{"status_code":1,"log_text":"[13:31:39] Fetching Scraper for: ESS                              handlers.py:23\n           Begin attempting to scrape: ESS                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:31:40] Getting all files in ESS...                               base.py:182\n           ...found 1 files in ESS                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:31:41] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx               \n           404 Client Error: Not Found for url:                   handlers.py:36\n           http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx               \n           Finished attempting to scrape: ESS                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://cmis.essexcc.gov.uk/EssexCmis5/Councillors.aspx\n","start":"2022-05-09 13:31:39.189453","end":"2022-05-09 13:31:41.524473","duration":2}},{"council_id":"FIF","missing":false,"latest_run":{"status_code":1,"log_text":"[12:08:49] Fetching Scraper for: FIF                              handlers.py:23\n           Begin attempting to scrape: FIF                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:08:50] Getting all files in FIF...                               base.py:182\n           Getting all files in FIF/json...                          base.py:182\n[12:08:51] ...found 66 files in FIF/json                             base.py:198\n           Getting all files in FIF/raw...                           base.py:182\n           ...found 66 files in FIF/raw                              base.py:198\n           ...found 133 files in FIF                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[12:08:52] Deleting batch no. 2 consisting of 33 files               base.py:207\n[12:08:53] ...data deleted.                                          base.py:237\n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/                                                 \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages            \n[12:08:54] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr-ken-caldwell                                                    \n[12:08:55] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr-david-graham                                                    \n[12:08:56] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr-john-obrien                                                     \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/buckhaven,-methil-and-wemyss-villages/           \n           cllr-ryan-smart                                                      \n[12:08:57] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy                                                                \n[12:08:58] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy/cllr-lesley-backhouse                                          \n[12:08:59] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy/cllr-gordon-langlands                                          \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/burntisland,-kinghorn-and-western-kirk           \n           caldy/cllr-kathleen-leslie                                           \n[12:09:00] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cowdenbeath                                      \n[12:09:01] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cowdenbeath/cllr-alistair-bain                   \n[12:09:02] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cowdenbeath/cllr-alex-campbell                   \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cowdenbeath/cllr-darren-watt                     \n[12:09:03] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cupar                                            \n[12:09:04] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cupar/cllr-margaret-kennedy                      \n[12:09:05] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/cupar/cllr-karen-marjoram                        \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-central                              \n[12:09:06] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-central/cllr-garry-haldane           \n[12:09:07] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-central/cllr-jim-leishman            \n[12:09:09] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-central/cllr.-jean-hall-mu           \n           ir                                                                   \n[12:09:10] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-central/derek-glen                   \n[12:09:11] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-north                                \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-north/cllr.-gavin-ellis              \n[12:09:12] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-north/cllr.-ian-ferguson             \n[12:09:13] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-north/cllr.-helen-law                \n           Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-south                                \n[12:09:14] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-south/cllr.-james-calder             \n[12:09:15] Scraping from https://www.fife.gov.uk/kb/docs/articles/abo base.py:42\n           ut-your-council2/politicians-and-committees/your-local-cou           \n           ncillors/councillor/dunfermline-south/cllr.-ross-paterson            \n           404 Client Error: Not Found for url: https://www.fife. handlers.py:36\n           gov.uk/kb/docs/articles/about-your-council2/politician               \n           s-and-committees/your-local-councillors/councillor/dun               \n           fermline-south/cllr.-ross-paterson                                   \n           Committing batch 1 consisting of 40 files                 base.py:265\n[12:09:17] Finished attempting to scrape: FIF                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/FIF-fife/councillors.py\", line 27, in get_single_councillor\n    soup = self.get_page(url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.fife.gov.uk/kb/docs/articles/about-your-council2/politicians-and-committees/your-local-councillors/councillor/dunfermline-south/cllr.-ross-paterson\n","start":"2022-05-09 12:08:49.821354","end":"2022-05-09 12:09:17.416792","duration":27}},{"council_id":"GLG","missing":false,"latest_run":{"status_code":1,"log_text":"[14:19:07] Fetching Scraper for: GLG                              handlers.py:23\n           Begin attempting to scrape: GLG                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[14:19:08] Getting all files in GLG...                               base.py:182\n           ...found 1 files in GLG                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[14:19:09] ...data deleted.                                          base.py:237\n           Scraping from https://www.glasgow.gov.uk/councillorsandcom base.py:42\n           mittees/allMembers.asp?sort=0&page=0&rec=100                         \n[14:19:11] Scraping from https://www.glasgow.gov.uk/councillorsandcom base.py:42\n           mittees/member.asp?id=2391&t=Councillor+Saqib+Ahmed                  \n           Scraping from https://www.glasgow.gov.uk/councillorsandcom base.py:42\n           mittees/member.asp?id=1300&t=Councillor+Susan+Aitken                 \n[14:19:12] Scraping from https://www.glasgow.gov.uk/councillorsandcom base.py:42\n           mittees/member.asp?id=3246&t=Councilor+Imran+Alam                    \n           'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Committing batch 1 consisting of 4 files                  base.py:265\n[14:19:14] Finished attempting to scrape: GLG                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/GLG-glasgow-city/councillors.py\", line 45, in get_single_councillor\n    councillor.email = soup.select_one(\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2022-05-09 14:19:07.686171","end":"2022-05-09 14:19:14.255654","duration":6}},{"council_id":"GRE","missing":false,"latest_run":{"status_code":1,"log_text":"[14:07:15] Fetching Scraper for: GRE                              handlers.py:23\n           Begin attempting to scrape: GRE                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in GRE...                               base.py:182\n           ...found 1 files in GRE                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[14:07:16] ...data deleted.                                          base.py:237\n           Scraping from https://committees.royalgreenwich.gov.uk/Cou base.py:42\n           ncillors/tabid/63/ScreenMode/Alphabetical/Default.aspx               \n[14:07:18] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/336/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:19] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/354/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:20] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/337/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:21] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/75/ScreenM           \n           ode/Alphabetical/Default.aspx                                        \n[14:07:23] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/345/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:24] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/348/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:25] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/314/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:27] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/76/ScreenM           \n           ode/Alphabetical/Default.aspx                                        \n[14:07:28] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/315/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:30] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/355/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:33] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/96/ScreenM           \n           ode/Alphabetical/Default.aspx                                        \n[14:07:34] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/102/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:36] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/346/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:37] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/334/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:38] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/107/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:41] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/110/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:42] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/79/ScreenM           \n           ode/Alphabetical/Default.aspx                                        \n[14:07:44] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/81/ScreenM           \n           ode/Alphabetical/Default.aspx                                        \n[14:07:46] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/349/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:47] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/86/ScreenM           \n           ode/Alphabetical/Default.aspx                                        \n[14:07:49] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/92/ScreenM           \n           ode/Alphabetical/Default.aspx                                        \n[14:07:52] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/105/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:53] Scraping from http://committees.royalgreenwich.gov.uk/Coun base.py:42\n           cillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/341/Screen           \n           Mode/Alphabetical/Default.aspx                                       \n[14:07:54] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 46 files                 base.py:265\n[14:07:56] Finished attempting to scrape: GRE                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1486, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2022-05-09 14:07:15.036089","end":"2022-05-09 14:07:56.579015","duration":41}},{"council_id":"HER","missing":false,"latest_run":{"status_code":1,"log_text":"[13:39:49] Fetching Scraper for: HER                              handlers.py:23\n           Begin attempting to scrape: HER                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in HER...                               base.py:182\n[13:39:50] ...found 1 files in HER                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n           ...data deleted.                                          base.py:237\n           Scraping from https://www5.hertsmere.gov.uk/democracy//mgW base.py:42\n           ebService.asmx/GetCouncillorsByWard                                  \n[13:39:51] 404 Client Error: Not Found for url: https://www5.hert handlers.py:36\n           smere.gov.uk/democracy//mgWebService.asmx/GetCouncillo               \n           rsByWard                                                             \n           Finished attempting to scrape: HER                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www5.hertsmere.gov.uk/democracy//mgWebService.asmx/GetCouncillorsByWard\n","start":"2022-05-09 13:39:49.011584","end":"2022-05-09 13:39:51.349273","duration":2}},{"council_id":"HLD","missing":false,"latest_run":{"status_code":1,"log_text":"[13:52:04] Fetching Scraper for: HLD                              handlers.py:23\n           Begin attempting to scrape: HLD                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:52:05] Getting all files in HLD...                               base.py:182\n           ...found 1 files in HLD                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:52:06] ...data deleted.                                          base.py:237\n           Scraping from https://www.highland.gov.uk/councillors/name base.py:42\n[13:52:08] Scraping from                                              base.py:42\n           https://www.highland.gov.uk/councillors/81/colin_aitken              \n           Scraping from                                              base.py:42\n           https://www.highland.gov.uk/councillors/102/sarah_atkin              \n[13:52:09] Scraping from                                              base.py:42\n           https://www.highland.gov.uk/councillors/110/michael_baird            \n           Scraping from                                              base.py:42\n           https://www.highland.gov.uk/councillors/94/andrew_baldrey            \n[13:52:10] 'NoneType' object has no attribute 'get_text'          handlers.py:36\n           Committing batch 1 consisting of 6 files                  base.py:265\n[13:52:11] Finished attempting to scrape: HLD                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/HLD-highland/councillors.py\", line 47, in get_single_councillor\n    councillor.email = soup.select_one(\"li a[href^=mailto]\").get_text(strip=True)\nAttributeError: 'NoneType' object has no attribute 'get_text'\n","start":"2022-05-09 13:52:04.303122","end":"2022-05-09 13:52:11.755535","duration":7}},{"council_id":"KEC","missing":false,"latest_run":{"status_code":1,"log_text":"[12:26:53] Fetching Scraper for: KEC                              handlers.py:23\n           Begin attempting to scrape: KEC                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in KEC...                               base.py:182\n           Getting all files in KEC/json...                          base.py:182\n[12:26:54] ...found 68 files in KEC/json                             base.py:198\n           Getting all files in KEC/raw...                           base.py:182\n           ...found 68 files in KEC/raw                              base.py:198\n           ...found 137 files in KEC                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[12:26:55] Deleting batch no. 2 consisting of 37 files               base.py:207\n[12:26:56] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.rbkc.gov.uk/committees/Councillors.aspx                  \n[12:26:57] Scraping from https://www.rbkc.gov.uk/committees/Councillo base.py:42\n           rs/tabid/62/ctl/ViewCMIS_Person/mid/384/id/521/ScreenMode/           \n           Alphabetical/Default.aspx                                            \n[12:26:59] Scraping from https://www.rbkc.gov.uk/committees/Councillo base.py:42\n           rs/tabid/62/ctl/ViewCMIS_Person/mid/384/id/421/ScreenMode/           \n           Alphabetical/Default.aspx                                            \n[12:27:00] Scraping from https://www.rbkc.gov.uk/committees/Councillo base.py:42\n           rs/tabid/62/ctl/ViewCMIS_Person/mid/384/id/448/ScreenMode/           \n           Alphabetical/Default.aspx                                            \n[12:27:02] Scraping from https://www.rbkc.gov.uk/committees/Councillo base.py:42\n           rs/tabid/62/ctl/ViewCMIS_Person/mid/384/id/525/ScreenMode/           \n           Alphabetical/Default.aspx                                            \n[12:27:03] Scraping from https://www.rbkc.gov.uk/committees/Councillo base.py:42\n           rs/tabid/62/ctl/ViewCMIS_Person/mid/384/id/479/ScreenMode/           \n           Alphabetical/Default.aspx                                            \n[12:27:05] Scraping from https://www.rbkc.gov.uk/committees/Councillo base.py:42\n           rs/tabid/62/ctl/ViewCMIS_Person/mid/384/id/450/ScreenMode/           \n           Alphabetical/Default.aspx                                            \n[12:27:06] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 12 files                 base.py:265\n[12:27:08] Finished attempting to scrape: KEC                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1486, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2022-05-09 12:26:53.004027","end":"2022-05-09 12:27:08.044401","duration":15}},{"council_id":"LUT","missing":false,"latest_run":{"status_code":1,"log_text":"[12:48:28] Fetching Scraper for: LUT                              handlers.py:23\n           Begin attempting to scrape: LUT                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:48:29] Getting all files in LUT...                               base.py:182\n           Getting all files in LUT/json...                          base.py:182\n[12:48:30] ...found 17 files in LUT/json                             base.py:198\n           Getting all files in LUT/raw...                           base.py:182\n           ...found 17 files in LUT/raw                              base.py:198\n           ...found 35 files in LUT                                  base.py:198\n           Deleting batch no. 1 consisting of 35 files               base.py:207\n[12:48:31] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           http://democracy.luton.gov.uk/cmis5public/Councillors.aspx           \n[12:48:32] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/364/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:33] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/365/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:36] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/292/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:37] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/366/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:39] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/253/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:42] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/367/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:43] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/392/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:44] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/396/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:45] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/368/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:46] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/235/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:49] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/191/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:52] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/369/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:53] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/193/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:55] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/370/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:56] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/371/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:48:58] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/150/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:49:02] Scraping from https://democracy.luton.gov.uk/cmis5public/C base.py:42\n           ouncillors/tabid/63/ctl/ViewCMIS_Person/mid/383/id/210/Scr           \n           eenMode/Alphabetical/Default.aspx                                    \n[12:49:06] 'title'                                                handlers.py:36\n           Committing batch 1 consisting of 34 files                 base.py:265\n[12:49:07] Finished attempting to scrape: LUT                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1486, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2022-05-09 12:48:28.909677","end":"2022-05-09 12:49:07.618074","duration":38}},{"council_id":"MIK","missing":false,"latest_run":{"status_code":1,"log_text":"[13:22:34] Fetching Scraper for: MIK                              handlers.py:23\n           Begin attempting to scrape: MIK                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:22:35] Getting all files in MIK...                               base.py:182\n           Getting all files in MIK/json...                          base.py:182\n           ...found 56 files in MIK/json                             base.py:198\n           Getting all files in MIK/raw...                           base.py:182\n[13:22:36] ...found 56 files in MIK/raw                              base.py:198\n           ...found 113 files in MIK                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[13:22:37] Deleting batch no. 2 consisting of 13 files               base.py:207\n[13:22:38] ...data deleted.                                          base.py:237\n           Scraping from http://milton-keynes.cmis.uk.com/milton-keyn base.py:42\n           es/Councillors.aspx                                                  \n[13:22:40] 'title'                                                handlers.py:36\n           Finished attempting to scrape: MIK                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1486, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2022-05-09 13:22:34.646358","end":"2022-05-09 13:22:40.461710","duration":5}},{"council_id":"NEL","missing":false,"latest_run":{"status_code":1,"log_text":"[12:51:14] Fetching Scraper for: NEL                              handlers.py:23\n           Begin attempting to scrape: NEL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in NEL...                               base.py:182\n[12:51:15] ...found 1 files in NEL                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:51:16] ...data deleted.                                          base.py:237\n           Scraping from https://www.nelincs.gov.uk/your-council/coun base.py:42\n           cillors-mps-and-meps/find-your-councillor/councillors-by-p           \n           arty/                                                                \n[12:51:19] More than one element selected                         handlers.py:36\n           Finished attempting to scrape: NEL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 137, in get_list_container\n    raise ValueError(\"More than one element selected\")\nValueError: More than one element selected\n","start":"2022-05-09 12:51:14.101280","end":"2022-05-09 12:51:19.733899","duration":5}},{"council_id":"NFK","missing":false,"latest_run":{"status_code":1,"log_text":"[11:47:16] Fetching Scraper for: NFK                              handlers.py:23\n           Begin attempting to scrape: NFK                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:47:17] Getting all files in NFK...                               base.py:182\n           ...found 1 files in NFK                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:47:18] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://norfolkcc.cmis.uk.com/norfolkcc/Councillors.aspx             \n[11:47:21] 'NoneType' object has no attribute 'next'              handlers.py:36\n[11:47:22] Finished attempting to scrape: NFK                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 258, in get_single_councillor\n    division = list_page_html.find(text=self.division_text).next.strip()\nAttributeError: 'NoneType' object has no attribute 'next'\n","start":"2022-05-09 11:47:16.760313","end":"2022-05-09 11:47:22.041956","duration":5}},{"council_id":"NNO","missing":false,"latest_run":{"status_code":1,"log_text":"[12:52:44] Fetching Scraper for: NNO                              handlers.py:23\n           Begin attempting to scrape: NNO                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:52:45] Getting all files in NNO...                               base.py:182\n           ...found 1 files in NNO                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:52:46] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.north-norfolk.gov.uk/members/#filter-form                \n[12:52:47] HTTPSConnectionPool(host='modgov.north-norfolk.gov.uk' handlers.py:36\n           , port=443): Max retries exceeded with url:                          \n           /mgMemberIndex.aspx?bcr=1 (Caused by                                 \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: NNO                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='modgov.north-norfolk.gov.uk', port=443): Max retries exceeded with url: /mgMemberIndex.aspx?bcr=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"scrapers/NNO-north-norfolk/councillors.py\", line 15, in get_councillors\n    return super().get_councillors()[1:]\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 667, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 667, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 237, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='modgov.north-norfolk.gov.uk', port=443): Max retries exceeded with url: /mgMemberIndex.aspx?bcr=1 (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2022-05-09 12:52:44.774342","end":"2022-05-09 12:52:47.499331","duration":2}},{"council_id":"PEN","missing":false,"latest_run":{"status_code":1,"log_text":"[13:05:12] Fetching Scraper for: PEN                              handlers.py:23\n           Begin attempting to scrape: PEN                        handlers.py:27\n[13:05:13] Deleting existing data...                                 base.py:230\n           Getting all files in PEN...                               base.py:182\n           Getting all files in PEN/json...                          base.py:182\n[13:05:14] ...found 3 files in PEN/json                              base.py:198\n           Getting all files in PEN/raw...                           base.py:182\n           ...found 3 files in PEN/raw                               base.py:198\n           ...found 7 files in PEN                                   base.py:198\n           Deleting batch no. 1 consisting of 7 files                base.py:207\n[13:05:15] ...data deleted.                                          base.py:237\n           Scraping from https://www.pendle.gov.uk/councillors/name   base.py:42\n[13:05:16] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/76/mohammed_adnan              \n[13:05:17] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/83/faraz_ahmad                 \n[13:05:18] Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/2/nadeem_ahmed                 \n           Scraping from                                              base.py:42\n           https://www.pendle.gov.uk/councillors/91/sajjad_ahmed                \n[13:05:19] 'NoneType' object is not subscriptable                 handlers.py:36\n           Committing batch 1 consisting of 6 files                  base.py:265\n[13:05:20] Finished attempting to scrape: PEN                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/PEN-pendle/councillors.py\", line 48, in get_single_councillor\n    councillor.email = soup.select_one(\"li a[href^=mailto]\")[\"href\"].replace(\nTypeError: 'NoneType' object is not subscriptable\n","start":"2022-05-09 13:05:12.936549","end":"2022-05-09 13:05:20.814915","duration":7}},{"council_id":"SHO","missing":false,"latest_run":{"status_code":1,"log_text":"[14:01:04] Fetching Scraper for: SHO                              handlers.py:23\n           Begin attempting to scrape: SHO                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[14:01:05] Getting all files in SHO...                               base.py:182\n           ...found 1 files in SHO                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[14:01:06] ...data deleted.                                          base.py:237\n           Scraping from https://democracy.sholland.gov.uk/mgWebServi base.py:42\n           ce.asmx/GetCouncillorsByWard                                         \n[14:01:10] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://democracy.sholland.gov.uk/mgError.aspx                       \n[14:01:11] Finished attempting to scrape: SHO                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://democracy.sholland.gov.uk/mgError.aspx\n","start":"2022-05-09 14:01:04.310800","end":"2022-05-09 14:01:11.106083","duration":6}},{"council_id":"SLA","missing":false,"latest_run":{"status_code":1,"log_text":"[12:02:17] Fetching Scraper for: SLA                              handlers.py:23\n           Begin attempting to scrape: SLA                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in SLA...                               base.py:182\n[12:02:18] ...found 1 files in SLA                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:02:19] ...data deleted.                                          base.py:237\n           Scraping from http://democracy.southlakeland.gov.uk/mgWebS base.py:42\n           ervice.asmx/GetCouncillorsByWard                                     \n           HTTPSConnectionPool(host='democracy.southlakeland.gov. handlers.py:36\n           uk', port=443): Max retries exceeded with url:                       \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: SLA                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='democracy.southlakeland.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 667, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 667, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 237, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='democracy.southlakeland.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2022-05-09 12:02:17.192494","end":"2022-05-09 12:02:19.608709","duration":2}},{"council_id":"STG","missing":false,"latest_run":{"status_code":1,"log_text":"[13:07:23] Fetching Scraper for: STG                              handlers.py:23\n           Begin attempting to scrape: STG                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:07:24] Getting all files in STG...                               base.py:182\n           ...found 1 files in STG                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:07:25] ...data deleted.                                          base.py:237\n           Scraping from https://www.stirling.gov.uk/councillors      base.py:42\n           404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.stirling.gov.uk/councillors                              \n[13:07:26] Finished attempting to scrape: STG                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.stirling.gov.uk/councillors\n","start":"2022-05-09 13:07:23.745060","end":"2022-05-09 13:07:26.168096","duration":2}},{"council_id":"STS","missing":false,"latest_run":{"status_code":1,"log_text":"[13:13:29] Fetching Scraper for: STS                              handlers.py:23\n           Begin attempting to scrape: STS                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[13:13:30] Getting all files in STS...                               base.py:182\n           ...found 1 files in STS                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:13:31] ...data deleted.                                          base.py:237\n           Scraping from http://moderngov.staffordshire.gov.uk//mgWeb base.py:42\n           Service.asmx/GetCouncillorsByWard                                    \n[13:15:42] HTTPConnectionPool(host='moderngov.staffordshire.gov.u handlers.py:36\n           k', port=80): Max retries exceeded with url:                         \n           //mgWebService.asmx/GetCouncillorsByWard (Caused by                  \n           NewConnectionError('<urllib3.connection.HTTPConnection               \n           object at 0x7f7fb0ac8d00>: Failed to establish a new                 \n           connection: [Errno 110] Connection timed out'))                      \n[13:15:43] Finished attempting to scrape: STS                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 398, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"/opt/python/urllib3/connection.py\", line 239, in request\n    super(HTTPConnection, self).request(method, url, body=body, headers=headers)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1256, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1302, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1251, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 1011, in _send_output\n    self.send(msg)\n  File \"/var/lang/lib/python3.8/http/client.py\", line 951, in send\n    self.connect()\n  File \"/opt/python/urllib3/connection.py\", line 205, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPConnection object at 0x7f7fb0ac8d00>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPConnectionPool(host='moderngov.staffordshire.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7fb0ac8d00>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPConnectionPool(host='moderngov.staffordshire.gov.uk', port=80): Max retries exceeded with url: //mgWebService.asmx/GetCouncillorsByWard (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f7fb0ac8d00>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2022-05-09 13:13:29.663082","end":"2022-05-09 13:15:43.135748","duration":133}},{"council_id":"TEN","missing":false,"latest_run":{"status_code":1,"log_text":"[13:22:16] Fetching Scraper for: TEN                              handlers.py:23\n           Begin attempting to scrape: TEN                        handlers.py:27\n[13:22:17] Deleting existing data...                                 base.py:230\n           Getting all files in TEN...                               base.py:182\n           ...found 1 files in TEN                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:22:18] ...data deleted.                                          base.py:237\n[13:22:19] Scraping from http://tdcdemocracy.tendringdc.gov.uk/mgWebS base.py:42\n           ervice.asmx/GetCouncillorsByWard                                     \n           HTTPSConnectionPool(host='tdcdemocracy.tendringdc.gov. handlers.py:36\n           uk', port=443): Max retries exceeded with url:                       \n           /mgWebService.asmx/GetCouncillorsByWard (Caused by                   \n           SSLError(SSLCertVerificationError(1, '[SSL:                          \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: TEN                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='tdcdemocracy.tendringdc.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 173, in run\n    wards = self.get_councillors()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 190, in get_councillors\n    req = self.get(self.format_councillor_api_url(), verify=self.verify_requests)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 667, in send\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 667, in <listcomp>\n    history = [resp for resp in gen]\n  File \"/opt/python/requests/sessions.py\", line 237, in resolve_redirects\n    resp = self.send(\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='tdcdemocracy.tendringdc.gov.uk', port=443): Max retries exceeded with url: /mgWebService.asmx/GetCouncillorsByWard (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2022-05-09 13:22:16.837562","end":"2022-05-09 13:22:19.459984","duration":2}},{"council_id":"TES","missing":false,"latest_run":{"status_code":1,"log_text":"[12:50:28] Fetching Scraper for: TES                              handlers.py:23\n           Begin attempting to scrape: TES                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in TES...                               base.py:182\n[12:50:29] ...found 1 files in TES                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[12:50:30] ...data deleted.                                          base.py:237\n           Scraping from http://testvalley.cmis.uk.com/testvalleypubl base.py:42\n           ic/ElectedRepresentatives/tabid/63/ScreenMode/Alphabetical           \n           /Default.aspx#MemberSectionA                                         \n           404 Client Error: Not Found for url: http://testvalley handlers.py:36\n           .cmis.uk.com/testvalleypublic/ElectedRepresentatives/t               \n           abid/63/ScreenMode/Alphabetical/Default.aspx#MemberSec               \n           tionA                                                                \n           Finished attempting to scrape: TES                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: http://testvalley.cmis.uk.com/testvalleypublic/ElectedRepresentatives/tabid/63/ScreenMode/Alphabetical/Default.aspx#MemberSectionA\n","start":"2022-05-09 12:50:28.230586","end":"2022-05-09 12:50:30.518296","duration":2}},{"council_id":"THE","missing":false,"latest_run":{"status_code":1,"log_text":"[11:46:09] Fetching Scraper for: THE                              handlers.py:23\n           Begin attempting to scrape: THE                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in THE...                               base.py:182\n           Getting all files in THE/json...                          base.py:182\n[11:46:10] ...found 38 files in THE/json                             base.py:198\n           Getting all files in THE/raw...                           base.py:182\n           ...found 38 files in THE/raw                              base.py:198\n           ...found 77 files in THE                                  base.py:198\n           Deleting batch no. 1 consisting of 77 files               base.py:207\n[11:46:11] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/listing/councillors                   \n[11:46:12] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/matthew-bedford            \n[11:46:13] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/sara-bedford               \n[11:46:14] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/ruth-clark                 \n[11:46:15] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/david-coltman              \n[11:46:16] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/stephen-cox                \n[11:46:17] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/steve-drury                \n[11:46:18] Scraping from                                              base.py:42\n           https://www.threerivers.gov.uk/councillor/andrea-fraser              \n           list index out of range                                handlers.py:36\n           Committing batch 1 consisting of 12 files                 base.py:265\n[11:46:20] Finished attempting to scrape: THE                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"scrapers/THE-three-rivers/councillors.py\", line 30, in get_single_councillor\n    councillor.email = soup.select(\"a[href^=mailto]\")[0].get_text(strip=True)\nIndexError: list index out of range\n","start":"2022-05-09 11:46:09.055764","end":"2022-05-09 11:46:20.570050","duration":11}},{"council_id":"VGL","missing":false,"latest_run":{"status_code":1,"log_text":"[14:27:20] Fetching Scraper for: VGL                              handlers.py:23\n           Begin attempting to scrape: VGL                        handlers.py:27\n[14:27:21] Deleting existing data...                                 base.py:230\n           Getting all files in VGL...                               base.py:182\n           ...found 1 files in VGL                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[14:27:23] ...data deleted.                                          base.py:237\n           Scraping from https://www.valeofglamorgan.gov.uk/en/our_co base.py:42\n           uncil/Council-Structure/councillors/Councillors.aspx                 \n           HTTPSConnectionPool(host='www.valeofglamorgan.gov.uk', handlers.py:36\n           port=443): Max retries exceeded with url: /en/our_coun               \n           cil/Council-Structure/councillors/Councillors.aspx                   \n           (Caused by SSLError(SSLCertVerificationError(1, '[SSL:               \n           CERTIFICATE_VERIFY_FAILED] certificate verify failed:                \n           unable to get local issuer certificate                               \n           (_ssl.c:1131)')))                                                    \n           Finished attempting to scrape: VGL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 416, in connect\n    self.sock = ssl_wrap_socket(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 449, in ssl_wrap_socket\n    ssl_sock = _ssl_wrap_socket_impl(\n  File \"/opt/python/urllib3/util/ssl_.py\", line 493, in _ssl_wrap_socket_impl\n    return ssl_context.wrap_socket(sock, server_hostname=server_hostname)\n  File \"/var/lang/lib/python3.8/ssl.py\", line 500, in wrap_socket\n    return self.sslsocket_class._create(\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1040, in _create\n    self.do_handshake()\n  File \"/var/lang/lib/python3.8/ssl.py\", line 1309, in do_handshake\n    self._sslobj.do_handshake()\nssl.SSLCertVerificationError: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='www.valeofglamorgan.gov.uk', port=443): Max retries exceeded with url: /en/our_council/Council-Structure/councillors/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 517, in send\n    raise SSLError(e, request=request)\nrequests.exceptions.SSLError: HTTPSConnectionPool(host='www.valeofglamorgan.gov.uk', port=443): Max retries exceeded with url: /en/our_council/Council-Structure/councillors/Councillors.aspx (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1131)')))\n","start":"2022-05-09 14:27:20.939178","end":"2022-05-09 14:27:23.446651","duration":2}},{"council_id":"WAW","missing":false,"latest_run":{"status_code":1,"log_text":"[11:46:53] Fetching Scraper for: WAW                              handlers.py:23\n           Begin attempting to scrape: WAW                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[11:46:54] Getting all files in WAW...                               base.py:182\n           ...found 1 files in WAW                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[11:46:55] ...data deleted.                                          base.py:237\n           Scraping from https://estates4.warwickdc.gov.uk/cmis/Counc base.py:42\n           illorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx                     \n           HTTPSConnectionPool(host='estates4.warwickdc.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url: /cmis/Counci               \n           llorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx                      \n           (Caused by NewConnectionError('<urllib3.connection.HTT               \n           PSConnection object at 0x7f6189df7940>: Failed to                    \n           establish a new connection: [Errno -5] No address                    \n           associated with hostname'))                                          \n           Finished attempting to scrape: WAW                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 72, in create_connection\n    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n  File \"/var/lang/lib/python3.8/socket.py\", line 918, in getaddrinfo\n    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\nsocket.gaierror: [Errno -5] No address associated with hostname\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 358, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f6189df7940>: Failed to establish a new connection: [Errno -5] No address associated with hostname\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='estates4.warwickdc.gov.uk', port=443): Max retries exceeded with url: /cmis/CouncillorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f6189df7940>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='estates4.warwickdc.gov.uk', port=443): Max retries exceeded with url: /cmis/CouncillorsAtoZ/tabid/39/ScreenMode/Ward/Default.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f6189df7940>: Failed to establish a new connection: [Errno -5] No address associated with hostname'))\n","start":"2022-05-09 11:46:53.388082","end":"2022-05-09 11:46:55.688749","duration":2}},{"council_id":"WDU","missing":false,"latest_run":{"status_code":1,"log_text":"[17:54:15] Fetching Scraper for: WDU                              handlers.py:23\n[17:54:20] Begin attempting to scrape: WDU                        handlers.py:27\n[17:54:21] Deleting existing data...                                 base.py:230\n[17:54:22] Getting all files in WDU...                               base.py:182\n[17:54:23] ...found 1 files in WDU                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[17:54:24] ...data deleted.                                          base.py:237\n           Scraping from https://wdccmis.west-dunbarton.gov.uk/cmis5/ base.py:42\n           Councillors.aspx                                                     \n           'title'                                                handlers.py:36\n           Finished attempting to scrape: WDU                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 48, in run\n    councillor = self.get_single_councillor(councillor_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 259, in get_single_councillor\n    party = self.get_party_name(list_page_html)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 247, in get_party_name\n    return list_page_html.find_all(\"img\")[-1][\"title\"].replace(\"(logo)\", \"\").strip()\n  File \"/opt/python/bs4/element.py\", line 1486, in __getitem__\n    return self.attrs[key]\nKeyError: 'title'\n","start":"2022-05-09 17:54:15.473146","end":"2022-05-09 17:54:24.933475","duration":9}},{"council_id":"WLL","missing":false,"latest_run":{"status_code":1,"log_text":"[12:46:08] Fetching Scraper for: WLL                              handlers.py:23\n           Begin attempting to scrape: WLL                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[12:46:09] Getting all files in WLL...                               base.py:182\n           Getting all files in WLL/json...                          base.py:182\n           ...found 58 files in WLL/json                             base.py:198\n           Getting all files in WLL/raw...                           base.py:182\n           ...found 58 files in WLL/raw                              base.py:198\n           ...found 117 files in WLL                                 base.py:198\n           Deleting batch no. 1 consisting of 100 files              base.py:207\n[12:46:11] Deleting batch no. 2 consisting of 17 files               base.py:207\n           ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://cmispublic.walsall.gov.uk/cmis/Councillors.aspx              \n[12:48:22] HTTPSConnectionPool(host='cmispublic.walsall.gov.uk',  handlers.py:36\n           port=443): Max retries exceeded with url:                            \n           /cmis/Councillors.aspx (Caused by NewConnectionError('               \n           <urllib3.connection.HTTPSConnection object at                        \n           0x7f6189f98bb0>: Failed to establish a new connection:               \n           [Errno 110] Connection timed out'))                                  \n           Finished attempting to scrape: WLL                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/opt/python/urllib3/connection.py\", line 174, in _new_conn\n    conn = connection.create_connection(\n  File \"/opt/python/urllib3/util/connection.py\", line 95, in create_connection\n    raise err\n  File \"/opt/python/urllib3/util/connection.py\", line 85, in create_connection\n    sock.connect(sa)\nTimeoutError: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/urllib3/connectionpool.py\", line 703, in urlopen\n    httplib_response = self._make_request(\n  File \"/opt/python/urllib3/connectionpool.py\", line 386, in _make_request\n    self._validate_conn(conn)\n  File \"/opt/python/urllib3/connectionpool.py\", line 1040, in _validate_conn\n    conn.connect()\n  File \"/opt/python/urllib3/connection.py\", line 358, in connect\n    conn = self._new_conn()\n  File \"/opt/python/urllib3/connection.py\", line 186, in _new_conn\n    raise NewConnectionError(\nurllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f6189f98bb0>: Failed to establish a new connection: [Errno 110] Connection timed out\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/python/requests/adapters.py\", line 440, in send\n    resp = conn.urlopen(\n  File \"/opt/python/urllib3/connectionpool.py\", line 785, in urlopen\n    retries = retries.increment(\n  File \"/opt/python/urllib3/util/retry.py\", line 592, in increment\n    raise MaxRetryError(_pool, url, error or ResponseError(cause))\nurllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='cmispublic.walsall.gov.uk', port=443): Max retries exceeded with url: /cmis/Councillors.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f6189f98bb0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 242, in get_councillors\n    req = self.get(self.base_url, extra_headers=self.extra_headers)\n  File \"/var/task/lgsf/scrapers/base.py\", line 48, in get\n    response = self.requests_session.get(url, headers=headers, verify=verify)\n  File \"/opt/python/requests/sessions.py\", line 542, in get\n    return self.request('GET', url, **kwargs)\n  File \"/opt/python/requests/sessions.py\", line 529, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"/opt/python/requests/sessions.py\", line 645, in send\n    r = adapter.send(request, **kwargs)\n  File \"/opt/python/requests/adapters.py\", line 519, in send\n    raise ConnectionError(e, request=request)\nrequests.exceptions.ConnectionError: HTTPSConnectionPool(host='cmispublic.walsall.gov.uk', port=443): Max retries exceeded with url: /cmis/Councillors.aspx (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f6189f98bb0>: Failed to establish a new connection: [Errno 110] Connection timed out'))\n","start":"2022-05-09 12:46:08.566246","end":"2022-05-09 12:48:22.526890","duration":133}},{"council_id":"WRT","missing":false,"latest_run":{"status_code":1,"log_text":"[14:30:38] Fetching Scraper for: WRT                              handlers.py:23\n           Begin attempting to scrape: WRT                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n[14:30:39] Getting all files in WRT...                               base.py:182\n           ...found 1 files in WRT                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[14:30:40] ...data deleted.                                          base.py:237\n           Scraping from                                              base.py:42\n           https://www.warrington.gov.uk/councillors/name                       \n[14:30:41] 404 Client Error: Not Found for url:                   handlers.py:36\n           https://www.warrington.gov.uk/councillors/name                       \n           Finished attempting to scrape: WRT                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.warrington.gov.uk/councillors/name\n","start":"2022-05-09 14:30:38.808654","end":"2022-05-09 14:30:41.889926","duration":3}},{"council_id":"WYE","missing":false,"latest_run":{"status_code":1,"log_text":"[13:16:14] Fetching Scraper for: WYE                              handlers.py:23\n           Begin attempting to scrape: WYE                        handlers.py:27\n           Deleting existing data...                                 base.py:230\n           Getting all files in WYE...                               base.py:182\n[13:16:15] ...found 1 files in WYE                                   base.py:198\n           Deleting batch no. 1 consisting of 1 files                base.py:207\n[13:16:16] ...data deleted.                                          base.py:237\n           Scraping from http://www.wyreforestdc.gov.uk/the-council/c base.py:42\n           ouncillors-committees-and-meetings/your-district-councillo           \n           r.aspx                                                               \n           404 Client Error: Not Found for url: https://www.wyref handlers.py:36\n           orestdc.gov.uk/the-council/councillors-committees-and-               \n           meetings/your-district-councillor.aspx                               \n           Finished attempting to scrape: WYE                        base.py:315\n","errors":"Traceback (most recent call last):\n  File \"/var/task/lgsf/aws_lambda/handlers.py\", line 32, in scraper_worker_handler\n    scraper.run(run_log)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 46, in run\n    for councillor_html in self.get_councillors():\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 141, in get_councillors\n    container = self.get_list_container()\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 134, in get_list_container\n    self.base_url_soup = self.get_page(self.base_url)\n  File \"/var/task/lgsf/councillors/scrapers.py\", line 123, in get_page\n    page = self.get(url).text\n  File \"/var/task/lgsf/scrapers/base.py\", line 49, in get\n    response.raise_for_status()\n  File \"/opt/python/requests/models.py\", line 960, in raise_for_status\n    raise HTTPError(http_error_msg, response=self)\nrequests.exceptions.HTTPError: 404 Client Error: Not Found for url: https://www.wyreforestdc.gov.uk/the-council/councillors-committees-and-meetings/your-district-councillor.aspx\n","start":"2022-05-09 13:16:14.070776","end":"2022-05-09 13:16:16.711660","duration":2}}]
